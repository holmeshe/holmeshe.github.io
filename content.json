{"meta":{"title":"Holmes He","subtitle":"Holmes' Blog","description":"","author":"Holmes He","url":"https://holmeshe.me"},"pages":[{"title":"Archives","date":"2016-08-16T03:00:24.000Z","updated":"2020-12-22T10:48:38.121Z","comments":true,"path":"archives/index.html","permalink":"https://holmeshe.me/archives/index.html","excerpt":"","text":""},{"title":"Categories","date":"2017-08-16T03:00:00.000Z","updated":"2020-12-22T10:48:38.121Z","comments":false,"path":"categories/index.html","permalink":"https://holmeshe.me/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2016-08-11T00:12:45.000Z","updated":"2020-12-22T10:48:38.145Z","comments":true,"path":"tags/index.html","permalink":"https://holmeshe.me/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Understanding The Memcached Source Code-Event Driven III","slug":"understanding-memcached-source-code-IX","date":"2019-06-01T22:30:00.000Z","updated":"2020-12-22T10:48:38.121Z","comments":true,"path":"understanding-memcached-source-code-IX/","link":"","permalink":"https://holmeshe.me/understanding-memcached-source-code-IX/","excerpt":"slab allocator (I, II, III) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I , II , III) for entry expiration; and an event driven model (I , II , III - this article) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. We continue examining the other two operations, i.e., create and delete, in the event driven context. Now it’s a good chance to revisit the core data structure and look at the Properties in discussion","text":"slab allocator (I, II, III) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I , II , III) for entry expiration; and an event driven model (I , II , III - this article) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. We continue examining the other two operations, i.e., create and delete, in the event driven context. Now it’s a good chance to revisit the core data structure and look at the Properties in discussion wbuf - the address for write buffer for simple response output (e.g., STORED). used by out_string wcurr - not very useful as it points to the same address as wbuf. used by conn_write wsize - the total size of write buffer. used by out_string to determine buff overflow wbytes - the length of data populated in write buffer. value is set in out_string; used by conn_write when writing it to the “real” output buffer iov write_and_go - set to conn_new_cmd in the very last step to form a “state loop” ritem - one of the essential properties for data reading. it is set to the address of the data portion of the actual item created by create command rlbytes - one of the essential properties for data reading. it is set to the length of the data in need item - record of the actual item created by create command noreply - determined by the command. we assume it is set to false CreateAs usual, we start with a command sent to a Memcached server. &gt; add test 0 60 11 (\\r\\n)&gt; hello world As mentioned in LRU III, two passes are involved in the command processing, the first pass creates an empty object after reading the first line, and the second populates the object with the concrete value contained in the second line. Such division is for the multiple I/O triggered by line breaks in telnet. In fact, most of the logic involved in this post has been discussed before such as in LRU III and Event Driven II. Hence this post will only resolve the missing parts and linking points. For the first command, &gt; add test 0 60 11 (\\r\\n) The Memcached instance outputs the following lines. This time we omit the output for accepting the new connection 27: going from conn_new_cmd to conn_waiting27: going from conn_waiting to conn_read27: going from conn_read to conn_parse_cmd27: Client using the ascii protocol&lt;27 add test 0 60 1127: going from conn_parse_cmd to conn_nread The logic for command reading and parsing (conn_new_cmd to conn_parse_cmd) are the same as what described in Event Driven II. The difference is that process_update_commandis invoked after the command parsing. Though the method has been examined in LRU III, it is worth reminding that the last step is to update the session context for the next state (conn_nread) which handles the actual data reading. static void process_update_command(conn *c, token_t *tokens, const size_t ntokens, int comm, bool handle_cas) &#123;... // LRU III c-&gt;item = it; c-&gt;ritem = ITEM_data(it); c-&gt;rlbytes = it-&gt;nbytes; c-&gt;cmd = comm; conn_set_state(c, conn_nread);&#125; process_update_command@memcached.c Next we look at the second command ...&gt; hello world and its associated output &gt; NOT FOUND test&gt;27 STORED27: going from conn_nread to conn_write27: going from conn_write to conn_new_cmd... The key code fragment for the state switching above is conn_nread...static void drive_machine(conn *c) &#123;... case conn_nread: if (c-&gt;rlbytes == 0) &#123; // scr: ---------------------------&gt; 5) complete_nread(c); break; &#125;...// scr: error handling /* first check if we have leftovers in the conn_read buffer */ if (c-&gt;rbytes &gt; 0) &#123; // scr: -----------------------------&gt; 1) int tocopy = c-&gt;rbytes &gt; c-&gt;rlbytes ? c-&gt;rlbytes : c-&gt;rbytes; if (c-&gt;ritem != c-&gt;rcurr) &#123; memmove(c-&gt;ritem, c-&gt;rcurr, tocopy); &#125; c-&gt;ritem += tocopy; c-&gt;rlbytes -= tocopy; c-&gt;rcurr += tocopy; c-&gt;rbytes -= tocopy; if (c-&gt;rlbytes == 0) &#123; break; &#125; &#125; /* now try reading from the socket */ res = read(c-&gt;sfd, c-&gt;ritem, c-&gt;rlbytes); // scr: --------&gt; 2) if (res &gt; 0) &#123;...// scr: stat if (c-&gt;rcurr == c-&gt;ritem) &#123; c-&gt;rcurr += res; &#125; c-&gt;ritem += res; c-&gt;rlbytes -= res; break; &#125; if (res == 0) &#123; /* end of stream */ // scr: --------------&gt; 3) conn_set_state(c, conn_closing); break; &#125; // scr: ------------------------------------------------&gt; 4) if (res == -1 &amp;&amp; (errno == EAGAIN || errno == EWOULDBLOCK)) &#123; if (!update_event(c, EV_READ | EV_PERSIST)) &#123;...// scr: error handling &#125; stop = true; break; &#125;...// scr: error handling break;... process_update_command@memcached.c 1) Check if there are some leftover data (from the command read phase). If so, read directly. More specific, say, if you can enter the command above fast enough (maybe copy paste it directly to telnet), the data portion will be coalesced in with the command by read. 2) Read the data to the memory pointed by ritem. 3) If the connection is closed (FIN) in the middle of the read, close the session. 4) If the data is separated into multiple reads, then set the drive machine to listen to more data and suspend. 5) Normal termination - read finished, call complete_nread which is covered in LRU III. The missing part in LRU III is out_string. Combined with conn_write, it functions as a simpler version of process_get_command for “simple response”, and the actual data writing is handled by conn_mwrite (note the fall through... in the switch case) block which, as discussed, changes the state back to conn_new_cmd. Next we discuss the process in detail. out_stringstatic void out_string(conn *c, const char *str) &#123; size_t len; ...//scr: not applicable if (settings.verbose &gt; 1) fprintf(stderr, \"&gt;%d %s\\n\", c-&gt;sfd, str); /* Nuke a partial output... */ c-&gt;msgcurr = 0; // scr: ---------------------------&gt; 1) c-&gt;msgused = 0; c-&gt;iovused = 0; add_msghdr(c); len = strlen(str); // scr: ------------------------&gt; 2) if ((len + 2) &gt; c-&gt;wsize) &#123; /* ought to be always enough. just fail for simplicity */ str = \"SERVER_ERROR output line too long\"; len = strlen(str); &#125; memcpy(c-&gt;wbuf, str, len); // scr: ----------------&gt; 3) memcpy(c-&gt;wbuf + len, \"\\r\\n\", 2); c-&gt;wbytes = len + 2; c-&gt;wcurr = c-&gt;wbuf; conn_set_state(c, conn_write); // scr: ------------&gt; 4) c-&gt;write_and_go = conn_new_cmd; // scr: -----------&gt; 5) return;&#125; memcached.c:out_string 1) Initialize the iov. the mechanism and add_msghdr has been discussed in the last post 2) Calculate string length, and be paranoid for survival. 3) Populate wbuf with the output string and point wcurr to wbuf. 4) Indicate the next state conn_write. 5) Set the last state to conn_new_cmd and form the “loop”. conn_write... case conn_write: /* * We want to write out a simple response. If we haven't already, * assemble it into a msgbuf list (this will be a single-entry * list for TCP or a two-entry list for UDP). */ if (c-&gt;iovused == 0 || (IS_UDP(c-&gt;transport) &amp;&amp; c-&gt;iovused == 1)) &#123; if (add_iov(c, c-&gt;wcurr, c-&gt;wbytes) != 0) &#123; // scr: ---&gt; 1) if (settings.verbose &gt; 0) fprintf(stderr, \"Couldn't build response\\n\"); conn_set_state(c, conn_closing); break; &#125; &#125; /* fall through... */ case conn_mwrite:...// scr: discussed switch (transmit(c)) &#123; case TRANSMIT_COMPLETE: if (c-&gt;state == conn_mwrite) &#123;...// scr: discussed &#125; else if (c-&gt;state == conn_write) &#123; if (c-&gt;write_and_free) &#123;...// scr: not applicable &#125; conn_set_state(c, c-&gt;write_and_go); // scr: -------&gt; 2) &#125; else &#123;...// scr: discussed &#125; break;... memcached.c:4507 1) Add the content of “write buffer” to the actual iov. 2) Pick up the write_and_go (i.e., conn_new_cmd) and set it to the next state. Next, we send the Deletecommand to delete the entry we just added. &gt; delete test And the output this time is 28: going from conn_read to conn_parse_cmd&lt;28 delete test&gt; FOUND KEY test&gt;28 DELETED28: going from conn_parse_cmd to conn_write28: going from conn_write to conn_new_cmd28: going from conn_new_cmd to conn_waiting28: going from conn_waiting to conn_read Like add the entry point of this command (after parsed) is process_delete_command which has been fully covered in LRU III. Furthermore, out_string is called within process_delete_command to trigger the state switches from conn_write, and back to conn_new_cmd. To go","categories":[{"name":"Memcached Source Code","slug":"Memcached-Source-Code","permalink":"https://holmeshe.me/categories/Memcached-Source-Code/"}],"tags":[{"name":"socket","slug":"socket","permalink":"https://holmeshe.me/tags/socket/"},{"name":"event driven","slug":"event-driven","permalink":"https://holmeshe.me/tags/event-driven/"},{"name":"state machine","slug":"state-machine","permalink":"https://holmeshe.me/tags/state-machine/"},{"name":"memecached","slug":"memecached","permalink":"https://holmeshe.me/tags/memecached/"}]},{"title":"理解 Memcached 源码 - LRU I","slug":"cn/understanding-memcached-source-code-IV","date":"2019-05-17T22:28:00.000Z","updated":"2020-12-22T10:48:38.109Z","comments":true,"path":"cn/understanding-memcached-source-code-IV/","link":"","permalink":"https://holmeshe.me/cn/understanding-memcached-source-code-IV/","excerpt":"多半情况下，LRU 会和 哈希表一起使用，然后我们把这个组合称为 LRU 缓存在 LRU缓存 中，哈希表提供了快速随机访问对象的能力；而LRU（算法）则用于淘汰很久没用 (least recently used) 的对象，来避免缓存无限增加。我们先大致看下 LRU 组成。","text":"多半情况下，LRU 会和 哈希表一起使用，然后我们把这个组合称为 LRU 缓存在 LRU缓存 中，哈希表提供了快速随机访问对象的能力；而LRU（算法）则用于淘汰很久没用 (least recently used) 的对象，来避免缓存无限增加。我们先大致看下 LRU 组成。 链表从技术上来说，LRU 算法是在链表上完成的 - 当一个表项被使用（访问或更新），LRU 会先做移除，然后把它重新插入到表头。这样，越接近表尾的对象就是 越久没使用 (least recently used)，淘汰起来比较简单。 哈希表链表是不支持快速的随机访问的，所以需要和 哈希表 一起使用。之前我们之前已经读过，板 子系统的空闲列表是通过链表把 板 里的 块 (chuck) 串起来形成的。在 LRU缓存 里也差不多，而这次用链表串起来的是表项。大致看起来是这样： 从另外一个维度看起来可能会更直观一点： 核心数据结构 - itemtypedef struct _stritem &#123; /* Protected by LRU locks */ struct _stritem *next; struct _stritem *prev; /* Rest are protected by an item lock */ struct _stritem *h_next; /* hash chain next */ rel_time_t time; /* least recent access */ rel_time_t exptime; /* expire time */ int nbytes; /* size of data */ unsigned short refcount; uint8_t nsuffix; /* length of flags-and-length string */ uint8_t it_flags; /* ITEM_* above */ uint8_t slabs_clsid;/* which slab class we're in */ uint8_t nkey; /* key length, w/terminating null and padding */ /* this odd type prevents type-punning issues when we do * the little shuffle to save space when not using CAS. */ union &#123;... // scr: cas char end; // scr: flexible array member indicating the item header \"end\" &#125; data[]; /* if it_flags &amp; ITEM_CAS we have 8 bytes CAS */ /* then null-terminated key */ /* then \" flags length\\r\\n\" (no terminating null) */ /* then data with terminating \\r\\n (no terminating null; it's binary!) */&#125; item; do_item_unlink@item.c 使用到的字段next, prev - LRU 链表 指针, 在 do_item_alloc (LRU III) 初始化, 被 item_link_q, item_unlink_q 使用。 h_next - hash 冲突链表 的指针, 在 do_item_alloc (LRU III) 初始化, 被 assoc_insert, assoc_delete, 多个模块 (LRU II) 使用。 time - 最后访问时间, 在 do_item_link 中设置, 被 lru_pull_tail (LRU III) 使用。 exptime - 超时时间（由请求参数指定）, 在 do_item_alloc (LRU III) 初始化, 被 lru_pull_tail (LRU III) 使用。 nbytes - 数据大小（由请求参数指定），在 do_item_alloc (LRU III) 初始化。 refcount - 引用计数, 在 do_slabs_alloc (Slab III) 初始化, 被 do_item_link 使用。 nsuffix - 在 do_item_alloc (LRU III) 用 item_make_header 初始化。 it_flags - 在 do_item_alloc (LRU III) 初始化, 被 do_item_link, do_item_unlink 使用。 slabs_clsid - 当前对象存在的具体的 LRU 链表 , 被 do_item_alloc (LRU III) 初始化, 在 item_link_q, item_unlink_q 使用。 nkey - 键大小, 在 do_item_alloc (LRU III) 中计算, 被 assoc_delete 使用。 块的内存布局我们在 do_slabs_free 提到过 块。这次我们直接通过数据结构来看下它的构造。 下面我们来读直接操作 LRU 的相关代码。 do_item_linkint do_item_link(item *it, const uint32_t hv) &#123; // scr: -------------------&gt; 1)... it-&gt;it_flags |= ITEM_LINKED; // scr: -------------------&gt; 2) it-&gt;time = current_time;... // scr: stat /* Allocate a new CAS ID on link. */... // scr: cas assoc_insert(it, hv); // scr: -------------------&gt; 3) item_link_q(it); // scr: -------------------&gt; 4) refcount_incr(&amp;it-&gt;refcount); // scr: -------------------&gt; 5)... // scr: stat return 1;&#125; do_item_link@item.c 1) 正常理解， hv 应该就是哈希值 “hashed value” 的缩写。 2) 设置 it-&gt;it_flags 的 ITEM_LINKED 标志, 然后将当前时间赋值给 it-&gt;time。 The field it_flags is used in do_slabs_free and do_slabs_alloc 3) 将 对象 添加到哈系表。 4) 将 对象 添加到链表。 5) 递增 reference count。 这个字段的初始值是 1，do_slabs_alloc 要注意 引用计数 代表了有几个子模块同时在使用该资源。这个字段是决定是否回收该资源的关键参考 （在这里，对象 同时被 板 和 LRU 在使用）。我在 另一篇文章 里详细讨论了C++里相似的机制。 item_link_q - 添加至链表item_link_q 是主力函数 do_item_link_q 的线程安全的简单包装。static void item_link_q(item *it) &#123; pthread_mutex_lock(&amp;lru_locks[it-&gt;slabs_clsid]); do_item_link_q(it); pthread_mutex_unlock(&amp;lru_locks[it-&gt;slabs_clsid]);&#125; item_link_q@item.c static void do_item_link_q(item *it) &#123; /* item is the new head */ item **head, **tail; assert((it-&gt;it_flags &amp; ITEM_SLABBED) == 0); head = &amp;heads[it-&gt;slabs_clsid]; // scr: -------------------&gt; 1) tail = &amp;tails[it-&gt;slabs_clsid]; assert(it != *head); assert((*head &amp;&amp; *tail) || (*head == 0 &amp;&amp; *tail == 0)); it-&gt;prev = 0; // scr: -------------------&gt; 2) it-&gt;next = *head; if (it-&gt;next) it-&gt;next-&gt;prev = it; *head = it; if (*tail == 0) *tail = it; sizes[it-&gt;slabs_clsid]++; // scr: -------------------&gt; 3) return;&#125; do_item_link_q@item.c 1) 从对应的 LRU 链表 （由slabs_clsid指定）获取 head 和 tail。注意 slabs_clsid 是用队列类型加过盐的，所以每个 板组 可能会包含复数个列表。 2) 标准动作，”添加表头项”。 3) 增加全局的数组 大小。 static item *heads[LARGEST_ID];static item *tails[LARGEST_ID];...static unsigned int sizes[LARGEST_ID];item.c:59 assoc_insert - 添加至哈希表int assoc_insert(item *it, const uint32_t hv) &#123; // scr: again, hv -&gt; hash value unsigned int oldbucket;... // scr: expanding related operations &#125; else &#123; it-&gt;h_next = primary_hashtable[hv &amp; hashmask(hashpower)]; // scr: 1) primary_hashtable[hv &amp; hashmask(hashpower)] = it; // scr: 2) &#125;... // scr: expanding related operations&#125; assoc_insert@assoc.c 1) 冲突处理。没冲突？将 h_next 直接设置为 null。 2) 将 对象 添加到 primary_hashtable 的桶。 ...static item** primary_hashtable = 0;...assoc.c:42 扩容的逻辑先留个坑，下篇 再来讲。 do_item_unlinkvoid do_item_unlink(item *it, const uint32_t hv) &#123; MEMCACHED_ITEM_UNLINK(ITEM_key(it), it-&gt;nkey, it-&gt;nbytes); if ((it-&gt;it_flags &amp; ITEM_LINKED) != 0) &#123; it-&gt;it_flags &amp;= ~ITEM_LINKED; // scr: -------------------&gt; 1)... // scr: stat assoc_delete(ITEM_key(it), it-&gt;nkey, hv); // scr: ---------------&gt; 2) item_unlink_q(it); // scr: -------------------&gt; 3) do_item_remove(it); // scr: -------------------&gt; *) &#125;&#125; do_item_unlink@item.c 1) 清除 it-&gt;it_flags 的 ITEM_LINKED 位。 2) 从哈希表移除 对象。 3) 从链表移除 对象。 *) 实际释放 对象 的逻辑后面会讲。 item_unlink_q - 从链表移除同上，item_unlink_q 只是一个对于实调函数 do_item_unlink_q 线程安全的简单封装。 static void item_link_q(item *it) &#123; pthread_mutex_lock(&amp;lru_locks[it-&gt;slabs_clsid]); do_item_link_q(it); pthread_mutex_unlock(&amp;lru_locks[it-&gt;slabs_clsid]);&#125;item_unlink_q@item.c static void do_item_unlink_q(item *it) &#123; item **head, **tail; head = &amp;heads[it-&gt;slabs_clsid]; // scr: -------------------&gt; 1) tail = &amp;tails[it-&gt;slabs_clsid]; if (*head == it) &#123; // scr: -------------------&gt; 2) assert(it-&gt;prev == 0); *head = it-&gt;next; &#125; if (*tail == it) &#123; assert(it-&gt;next == 0); *tail = it-&gt;prev; &#125; assert(it-&gt;next != it); assert(it-&gt;prev != it); if (it-&gt;next) it-&gt;next-&gt;prev = it-&gt;prev; if (it-&gt;prev) it-&gt;prev-&gt;next = it-&gt;next; sizes[it-&gt;slabs_clsid]--; // scr: -------------------&gt; 3) return;&#125; do_item_unlink_q@item.c 1) 同样, 获取 由 slabs_clsid 指定的 LUR 链表 的 head 和 tail。 2) 标准的 “移除链表项” 操作。 3) 减少全局的数组 大小。 static item *heads[LARGEST_ID];static item *tails[LARGEST_ID];...static unsigned int sizes[LARGEST_ID];item.c:59 assoc_delete - 从哈希表移除static item** _hashitem_before (const char *key, const size_t nkey, const uint32_t hv) &#123; item **pos; unsigned int oldbucket;... // scr: expanding related operations &#125; else &#123; pos = &amp;primary_hashtable[hv &amp; hashmask(hashpower)]; // scr: -----&gt; 1) &#125; while (*pos &amp;&amp; ((nkey != (*pos)-&gt;nkey) || memcmp(key, ITEM_key(*pos), nkey))) &#123; pos = &amp;(*pos)-&gt;h_next; // scr: ----------------------------------&gt; 2) &#125; return pos;&#125;...void assoc_delete(const char *key, const size_t nkey, const uint32_t hv) &#123; item **before = _hashitem_before(key, nkey, hv); if (*before) &#123; item *nxt;... nxt = (*before)-&gt;h_next; // scr: --------------------------------&gt; 3) (*before)-&gt;h_next = 0; /* probably pointless, but whatever. */ *before = nxt; // scr: ------------------------------------------&gt; 4) return; &#125; /* Note: we never actually get here. the callers don't delete things they can't find. */ assert(*before != 0);&#125; assoc_delete@assoc.c 1) 通过 hv 获取桶。 2) 遍历冲突链表并比较 key。注意这里的返回值是 指定元素上一个元素的 next 字段的地址。而当没有冲突时，这个地址就是桶本身。 3) 将找到的下一个元素赋值给 nxt。 4) 更新 指定元素上一个元素的 next 字段。 打包带走试下这个","categories":[{"name":"Memcached 源码分析","slug":"Memcached-源码分析","permalink":"https://holmeshe.me/categories/Memcached-源码分析/"}],"tags":[{"name":"memcached","slug":"memcached","permalink":"https://holmeshe.me/tags/memcached/"},{"name":"LRU","slug":"LRU","permalink":"https://holmeshe.me/tags/LRU/"},{"name":"源码","slug":"源码","permalink":"https://holmeshe.me/tags/源码/"},{"name":"源码分析","slug":"源码分析","permalink":"https://holmeshe.me/tags/源码分析/"},{"name":"缓存","slug":"缓存","permalink":"https://holmeshe.me/tags/缓存/"}]},{"title":"理解 Memcached 源码 - Slab III","slug":"cn/understanding-memcached-source-code-III","date":"2019-03-24T02:30:00.000Z","updated":"2020-12-22T10:48:38.109Z","comments":true,"path":"cn/understanding-memcached-source-code-III/","link":"","permalink":"https://holmeshe.me/cn/understanding-memcached-source-code-III/","excerpt":"上次我们看完了内存分配，以及形成待分配列表（free list，即slots）的过程。本篇我们继续查看如何使用建立好的数据结构来分配/回收块内存，并将它们用于存储对象。 板配给首先，我们来看 do_slabs_alloc","text":"上次我们看完了内存分配，以及形成待分配列表（free list，即slots）的过程。本篇我们继续查看如何使用建立好的数据结构来分配/回收块内存，并将它们用于存储对象。 板配给首先，我们来看 do_slabs_alloc 这个函数对应讨论过的do_slabs_free. 这里do_slabs_alloc的“公有”接口是slabs_alloc。slabs_alloc除了提供了对外接口外，还对核心数据结构加了线程锁以保证此函数（在Memcached被配置为多线程时，multithreaded）线程安全。 void *slabs_alloc(size_t size, unsigned int id, unsigned int *total_chunks, unsigned int flags) &#123; void *ret; pthread_mutex_lock(&amp;slabs_lock); ret = do_slabs_alloc(size, id, total_chunks, flags); pthread_mutex_unlock(&amp;slabs_lock); return ret;&#125;slabs_alloc@slabs.c ... case 't': settings.num_threads = atoi(optarg); if (settings.num_threads &lt;= 0) &#123; fprintf(stderr, \"Number of threads must be greater than 0\\n\"); return 1; &#125; /* There're other problems when you get above 64 threads. * In the future we should portably detect # of cores for the * default. */ if (settings.num_threads &gt; 64) &#123; fprintf(stderr, \"WARNING: Setting a high number of worker\" \"threads is not recommended.\\n\" \" Set this value to the number of cores in\" \" your machine or less.\\n\"); &#125; break;...main@memcached.c:5572 static void *do_slabs_alloc(const size_t size, unsigned int id, unsigned int *total_chunks, unsigned int flags) &#123; slabclass_t *p; void *ret = NULL; item *it = NULL;... p = &amp;slabclass[id]; // scr: ----------------------------------------&gt; 1)... if (total_chunks != NULL) &#123; *total_chunks = p-&gt;slabs * p-&gt;perslab; // scr: -----------------&gt; 2) &#125; /* fail unless we have space at the end of a recently allocated page, we have something on our freelist, or we could allocate a new page */ if (p-&gt;sl_curr == 0 &amp;&amp; flags != SLABS_ALLOC_NO_NEWPAGE) &#123; // scr: --&gt; *) do_slabs_newslab(id); // scr: ----------------------------------&gt; 3) &#125; if (p-&gt;sl_curr != 0) &#123; /* return off our freelist */ it = (item *)p-&gt;slots; // scr: ---------------------------------&gt; 4) p-&gt;slots = it-&gt;next; if (it-&gt;next) it-&gt;next-&gt;prev = 0; /* Kill flag and initialize refcount here for lock safety in slab * mover's freeness detection. */ it-&gt;it_flags &amp;= ~ITEM_SLABBED; // scr: -------------------------&gt; 5) it-&gt;refcount = 1; p-&gt;sl_curr--; ret = (void *)it; // scr: --------------------------------------&gt; 6) &#125; else &#123; ret = NULL; &#125;... return ret;&#125; do_slabs_alloc@slabs.c 1）id代表 板组。之前提到过，不同大小的对象会用不同的 板组 来存储。换句话说，id 的值由对象大小决定。这个过程后面会讨论。 2）total_chunks 是出参，用于存储当前 板组 的空闲内存块（memory chunk），或者说是待分配列表里还有多少空位。if (total_chunks != NULL) 则说明这个是可选参数。 *）和字面意思一样，SLABS_ALLOC_NO_NEWPAGE（flags）即使在没有空闲内存块时也不会额外分配新的 板 来满足后续分配需要。这个选项并不属于对象分配的通常路径，所以暂时忽略。 3）没有空闲内存块时分配新 板。这里很容易看到p-&gt;sl_curr 表示空闲内存块的数量。这个变量的值会在每次调用这个函数时递减（看第5步）。 另一方面, 这个字段在do_slabs_free里自增。 注意 new slab 在这篇也提到过。 4）从待分配列表（free list，即slots）表头干掉一个元素（f），并将其赋值给it。 在do_slabs_free, 内存块也是从表头加入的。 5) 清除对应内存块(f)的ITEM_SLABBED标志，将引用次数设置为1，并且内存块的数量p-&gt;sl_curr 减少1。 同样，这个标志在do_slabs_free中被设置。 6) 返回(f). 下面我们来看如何通过对象大小来决定id，对应的函数是 slabs_clsidunsigned int slabs_clsid(const size_t size) &#123; int res = POWER_SMALLEST; if (size == 0) return 0; while (size &gt; slabclass[res].size) if (res++ == power_largest) /* won't fit in the biggest slab */ return 0; return res;&#125; do_slabs_alloc@slabs.c slabs_clsid 主要由一个 while 循环组成，这个循环会渐次找到最小的 板组 来刚好达到申请对象的大小要求。这个函数是在 do_item_alloc 中先于 slabs_alloc 被调用。 我们会在后面的文章中讨论 do_item_alloc。 item *do_item_alloc(char *key, const size_t nkey, const unsigned int flags, const rel_time_t exptime, const int nbytes, const uint32_t cur_hv) &#123;... unsigned int id = slabs_clsid(ntotal); if (id == 0) return 0;... it = slabs_alloc(ntotal, id, &amp;total_chunks, 0);...do_item_alloc@items.c","categories":[{"name":"Memcached 源码分析","slug":"Memcached-源码分析","permalink":"https://holmeshe.me/categories/Memcached-源码分析/"}],"tags":[{"name":"memcached","slug":"memcached","permalink":"https://holmeshe.me/tags/memcached/"},{"name":"slab allocator","slug":"slab-allocator","permalink":"https://holmeshe.me/tags/slab-allocator/"},{"name":"cache","slug":"cache","permalink":"https://holmeshe.me/tags/cache/"},{"name":"源码","slug":"源码","permalink":"https://holmeshe.me/tags/源码/"},{"name":"源码分析","slug":"源码分析","permalink":"https://holmeshe.me/tags/源码分析/"}]},{"title":"理解 Memcached 源码- Slab II","slug":"cn/understanding-memcached-source-code-II","date":"2019-03-23T22:23:00.000Z","updated":"2020-12-22T10:48:38.109Z","comments":true,"path":"cn/understanding-memcached-source-code-II/","link":"","permalink":"https://holmeshe.me/cn/understanding-memcached-source-code-II/","excerpt":"这次我们继续看用于 板 的内存是如何分配的。 首先我们继续看 slabs_init 的两个实参。第一个是 settings.maxbytes - 控制这个 Memcached 实例可以使用的总内存大小。在传入 slabs_init 之前，这个参数被赋值为全局变量 mem_limit。","text":"这次我们继续看用于 板 的内存是如何分配的。 首先我们继续看 slabs_init 的两个实参。第一个是 settings.maxbytes - 控制这个 Memcached 实例可以使用的总内存大小。在传入 slabs_init 之前，这个参数被赋值为全局变量 mem_limit。 void slabs_init(const size_t limit, const double factor, const bool prealloc, const uint32_t *slab_sizes) &#123;... mem_limit = limit; // scr: here...slabs_init@memcached.c ... settings.maxbytes = 64 * 1024 * 1024; /* default is 64MB */... case 'm': settings.maxbytes = ((size_t)atoi(optarg)) * 1024 * 1024; break;...memcached.c:210,5493 static size_t mem_limit = 0;memcached.c:43 另外一个怎是 preallocate。它决定了是否为（各个）板组 预分配 内存。这个参数的值由 L 命令行参数来决定。 ... bool preallocate = false;... case 'L' : if (enable_large_pages() == 0) &#123; preallocate = true; &#125; else &#123; fprintf(stderr, \"Cannot enable large pages on this system\\n\" \"(There is no Linux support as of this version)\\n\"); return 1; &#125; break;...main@memcached.c:5350,5597 下面我们来看 slabs 的内存分配函数。 新建板do_slabs_newslab具体来说，这个函数用于给 板组 分配大小为1M的内存块。而 板组 由参数 id 指定。 static int do_slabs_newslab(const unsigned int id) &#123; slabclass_t *p = &amp;slabclass[id]; // scr: ----------------------------&gt; 1) slabclass_t *g = &amp;slabclass[SLAB_GLOBAL_PAGE_POOL]; // scr: ---------&gt; *) int len = settings.slab_reassign ? settings.item_size_max // scr: ---&gt; 2) : p-&gt;size * p-&gt;perslab; char *ptr; if ((mem_limit &amp;&amp; mem_malloced + len &gt; mem_limit &amp;&amp; p-&gt;slabs &gt; 0 // -&gt; 3) &amp;&amp; g-&gt;slabs == 0)) &#123; mem_limit_reached = true; MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id); return 0; &#125; if ((grow_slab_list(id) == 0) || // scr: ----------------------------&gt; 4) (((ptr = get_page_from_global_pool()) == NULL) &amp;&amp; // scr: -------&gt; *) ((ptr = memory_allocate((size_t)len)) == 0))) &#123; // scr: ---------&gt; 5) MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id); return 0; &#125; memset(ptr, 0, (size_t)len); split_slab_page_into_freelist(ptr, id); // scr: ---------------------&gt; 6) p-&gt;slab_list[p-&gt;slabs++] = ptr; // scr: -----------------------------&gt; 7) MEMCACHED_SLABS_SLABCLASS_ALLOCATE(id); return 1;&#125; do_slabs_newslab@slabs.c 1）slabclass[id] 是 板组 的数据结构。上篇讨论了这个数组的初始化。 2）settings.slab_reassign 决定是否启用 再平衡 策略。如果启用，未使用的 板 不会被立即释放，而是分配给其他 板组 使用，这就产生了一个问题，即所有 板组 都需要使用统一大小的 板。所以这个设置同时也决定了是否使用 同种板 （大小为 settings.item_size_max，或者上述的1M），还是 异种板 （p-&gt;size * p-&gt;perslab）。除了用命令行参数 \"slab_reassign\" 以外，\"modern\" 也会设置这个值，而本文也会用1M作为 板 的大小。 ... settings.slab_reassign = false;... case SLAB_REASSIGN: settings.slab_reassign = true; break;...main@memcached.c:238,5694 case MODERN: /* Modernized defaults. Need to add equivalent no_* flags * before making truly default. */ settings.slab_reassign = true; settings.slab_automove = 1;... break;main@memcached.c:5820 N.b. *, rebalancing mechanism will be discussed later when we have a better understanding of the LRU module. 3）检查内存使用是否超出上线。 4）grow_slab_list 检查是否增长 slabclass_t.slab_list，如果需要，则增长之。 static int grow_slab_list (const unsigned int id) &#123; slabclass_t *p = &amp;slabclass[id]; if (p-&gt;slabs == p-&gt;list_size) &#123; size_t new_size = (p-&gt;list_size != 0) ? p-&gt;list_size * 2 : 16; void *new_list = realloc(p-&gt;slab_list, new_size * sizeof(void *)); if (new_list == 0) return 0; p-&gt;list_size = new_size; p-&gt;slab_list = new_list; &#125; return 1;&#125;grow_slab_list@slabs.c 5）memory_allocate 是真正分配 板 内存的函数。如上述，这里的 len 是1M。 static void *memory_allocate(size_t size) &#123; void *ret; if (mem_base == NULL) &#123; /* We are not using a preallocated large memory chunk */ ret = malloc(size); &#125; else &#123; // scr: when preallocate is set to true...memory_allocate@slabs.c 6）split_slab_page_into_freelist 初始化 （或者是 free）刚刚分配的 板 内存用作对象存储。这个函数会在下一节讨论。 7) 将刚刚分配的 板 加入到 slabclass_t.slab_list. 下图总结了这个过程（我们想象 do_slabs_newslab(n) 被调用了两次） 接下来我们来看在第6）步中一块 板 是如何被初始化的。 split_slab_page_into_freeliststatic void split_slab_page_into_freelist(char *ptr, const unsigned int id) &#123; slabclass_t *p = &amp;slabclass[id]; int x; for (x = 0; x &lt; p-&gt;perslab; x++) &#123; do_slabs_free(ptr, 0, id); ptr += p-&gt;size; &#125;&#125; split_slab_page_into_freelist@slabs.c 这个函数会遍历 板 里的所有 块（slabclass_t.size），然后调用 do_slabs_free 来初始化每个 块 的元数据。换一个说法，就是 “拆分 slab到待分配列表”-“split a slab into item free list”。你也许已经猜到了，这个 待分配列表 会被直接用于 对象分配，这个过程后面会详细讨论。 do_slabs_freestatic void do_slabs_free(void *ptr, const size_t size, unsigned int id) &#123; slabclass_t *p; item *it;... p = &amp;slabclass[id]; it = (item *)ptr; it-&gt;it_flags = ITEM_SLABBED; // scr: ---------------&gt; 1) it-&gt;slabs_clsid = 0; it-&gt;prev = 0; // scr: ------------------------------&gt; 2) it-&gt;next = p-&gt;slots; if (it-&gt;next) it-&gt;next-&gt;prev = it; p-&gt;slots = it; p-&gt;sl_curr++; // scr: ------------------------------&gt; 3) p-&gt;requested -= size; return;&#125; do_slabs_free@slabs.c 技术上讲，这个函数处理的 元数据 元数据存在于每个 块的开始。 typedef struct _stritem &#123; /* Protected by LRU locks */ struct _stritem *next; struct _stritem *prev;... uint8_t it_flags; /* ITEM_* above */ uint8_t slabs_clsid;/* which slab class we're in */...&#125; item;main@memcached.c:5820 1）初始化一些域。这里 item 是另一个核心数据结构，后续会讨论。 2）将 item 加入到上述的 待分配列表 ，并且更新链表表头，slabclass_t.slots。 3）更新可分配项目数量，slabclass_t.sl_curr；并且更新 slabclass_t.requested 负责统计。注意这里并没有真正的释放对象，所以传入的 size 是0。 板预分配下面我们来看 do_slabs_newslab 怎么使用。其中一个地方是之前看到过的 slabs_init（preallocate 设置为 true）， void slabs_init(const size_t limit, const double factor, const bool prealloc, const uint32_t *slab_sizes) &#123;... if (prealloc) &#123; slabs_preallocate(power_largest); &#125;&#125;slabs_init@slabs.c static void slabs_preallocate (const unsigned int maxslabs) &#123; int i; unsigned int prealloc = 0; /* pre-allocate a 1MB slab in every size class so people don't get confused by non-intuitive \"SERVER_ERROR out of memory\" messages. this is the most common question on the mailing list. if you really don't want this, you can rebuild without these three lines. */ for (i = POWER_SMALLEST /* scr: 1 */; i &lt; MAX_NUMBER_OF_SLAB_CLASSES; i++) &#123; if (++prealloc &gt; maxslabs) return; if (do_slabs_newslab(i) == 0) &#123; fprintf(stderr, \"Error while preallocating slab memory!\\n\" \"If using -L or other prealloc options, max memory must be \" \"at least %d megabytes.\\n\", power_largest); exit(1); &#125; &#125;&#125; slabs_preallocate@slabs.c 这个方法从POWER_SMALLEST（1）开始遍历所有的 slabclass，然后给每个 板组 预分配一个 板。（下标为0th 的 板组 是一个特殊的组，存储空闲的 板 用于上面提到的 再平衡 策略）。 #define POWER_SMALLEST 1#define POWER_LARGEST 256 /* actual cap is 255 */#define SLAB_GLOBAL_PAGE_POOL 0 /* magic slab class for storing pages for reassignment */memcached.h:88 引用和上文一样。","categories":[{"name":"Memcached 源码分析","slug":"Memcached-源码分析","permalink":"https://holmeshe.me/categories/Memcached-源码分析/"}],"tags":[{"name":"memcached","slug":"memcached","permalink":"https://holmeshe.me/tags/memcached/"},{"name":"slab allocator","slug":"slab-allocator","permalink":"https://holmeshe.me/tags/slab-allocator/"},{"name":"cache","slug":"cache","permalink":"https://holmeshe.me/tags/cache/"},{"name":"source code analysis","slug":"source-code-analysis","permalink":"https://holmeshe.me/tags/source-code-analysis/"},{"name":"源码","slug":"源码","permalink":"https://holmeshe.me/tags/源码/"},{"name":"源码分析","slug":"源码分析","permalink":"https://holmeshe.me/tags/源码分析/"}]},{"title":"理解 Memcached 源码- Slab I","slug":"cn/understanding-memcached-source-code-I","date":"2019-03-22T23:30:00.000Z","updated":"2020-12-22T10:48:38.109Z","comments":true,"path":"cn/understanding-memcached-source-code-I/","link":"","permalink":"https://holmeshe.me/cn/understanding-memcached-source-code-I/","excerpt":"Slab分配器是这个缓存系统的核心，并在很大程度上决定了核心资源 - 内存 - 的利用效率。其它的三个部分，用来淘汰（超时）对象的LRU算法；和基于libevent的事件驱动；以及用于分布数据的一致性哈希，可以看作是围绕Slab来开发的。 在其他系统，比如内核，都能看到 Slab 分配器 的身影。无论它出现在哪里，都是为了对抗同一个性能问题，内存碎片。而本文就主要讨论 Slab 分配器 在memcached 中的实现（废话）。 memcached version: 1.4.28 首先我们来回答一些问题。","text":"Slab分配器是这个缓存系统的核心，并在很大程度上决定了核心资源 - 内存 - 的利用效率。其它的三个部分，用来淘汰（超时）对象的LRU算法；和基于libevent的事件驱动；以及用于分布数据的一致性哈希，可以看作是围绕Slab来开发的。 在其他系统，比如内核，都能看到 Slab 分配器 的身影。无论它出现在哪里，都是为了对抗同一个性能问题，内存碎片。而本文就主要讨论 Slab 分配器 在memcached 中的实现（废话）。 memcached version: 1.4.28 首先我们来回答一些问题。 前言啥是SlabSlab翻译过来就是（一块）板，具体来说，它是是被预先分配好的，大小为1M的内存块。这些 板 可以被进一步分割成一些相同大小的 块 (chunk)，对象就存写在每一个 块 上面。所有的 板 会根据所存储对象的大小分成 板组（slab class）。 刚刚提到的内存碎片是啥具体来说，板分配器 解决的其实是 内在碎片 （internal memory fragmentation）。这种碎片存在于分配的内存单元的内部。拿内核来说，内存的分配单元叫页（page），所有的内存分配的请求本质上都是在页里面拿走一块，同时产生的碎片也就自然产生于每页的内部了。 和内在碎片不一样，外在碎片（external memory fragmentation）则存在于分配的内存单元之间。解决外在碎片的一般做法则是用buddy，就不在本文范围内了。 我们再看下制造内存碎片过程， 1）malloc一堆小对象， 2）随机free一些上述小对象。 于是本来是整片的内存就会出现很多空洞，这些空洞，或者说碎片，因为太小而且分散，大概率永远无法被后续的malloc利用。 内存碎片引起的问题继续往后说。这些碎片由于不能被 malloc 使用，基本也就和 内存泄漏 差不多了。引发的具体问题也差不多 - 定期重启。 怎么办板分配器 并不消除内存碎片，而是将它们收敛起来，并锁定在某个固定的内存区域。具体来说，1）将大小相近的对象分组；2）同一组的的对象只会用对应的板组（slab class）来分配内存。 接下来看代码。 reminder: memcached version is 1.4.28 核心数据结构： typedef struct &#123; unsigned int size; /* sizes of items */ unsigned int perslab; /* how many items per slab */ void *slots; /* list of item ptrs */ unsigned int sl_curr; /* total free items in list */ unsigned int slabs; /* how many slabs were allocated for this class */ void **slab_list; /* array of slab pointers */ unsigned int list_size; /* size of prev array */ size_t requested; /* The number of requested bytes */&#125; slabclass_t;static slabclass_t slabclass[MAX_NUMBER_OF_SLAB_CLASSES]; slabclass_t@slabs.c 模块初始化本节我们来看slabs_init，和 slabclass[MAX_NUMBER_OF_SLAB_CLASSES] 的初始化。 这个函数主要给 slabclass_t.size ，和 slabclass_t.perslab 赋值。第一个域表示 Slab 组 所对应的对象大小，第二个域则表示一个 Slab 上可以放多少个该类的对象。最后，slabs_init 是在系统初始化的过程被调用（如以下代码）， ... assoc_init(settings.hashpower_init); conn_init(); slabs_init(settings.maxbytes, settings.factor, preallocate, use_slab_sizes ? slab_sizes : NULL);...main@memcached.c:5977 在这个阶段，slab_sizes 和 settings.factor 共同决定了后续逻辑的走向，并且确定各个 板组 所存储的对象大小， uint32_t slab_sizes[MAX_NUMBER_OF_SLAB_CLASSES];main@memcached.c:5372 settings.factor = 1.25;settings_init@memcached.c:217 如果 slab_sizes 不是 NULL, 用此数组的里面的值直接初始化各 板组 所对应的对象大小（支线a）； 反之，则用base size×n×settings.factor 来初始化上述的目标。这里 n 是 slabclass 的下标（支线b）。 除了写死的默认值，上述两个变量也能用 命令行参数赋值 。 ... case 'f': settings.factor = atof(optarg); if (settings.factor &lt;= 1.0) &#123; fprintf(stderr, \"Factor must be greater than 1\\n\"); return 1; &#125; break;... case 'o': /* It's sub-opts time! */... case SLAB_SIZES: if (_parse_slab_sizes(subopts_value, slab_sizes)) &#123; use_slab_sizes = true; &#125; else &#123; return 1; &#125; break;...main@memcached.c:5558, 5810 本函数的另外两个参数 settings.maxbytes 和 preallocate ，会在 后续 讨论。这里我们假设 preallocate 为 false，并忽略其对应的逻辑。 下面我们来看 slabs_init 本身。 void slabs_init(const size_t limit, const double factor, const bool prealloc, const uint32_t *slab_sizes) &#123; int i = POWER_SMALLEST /* scr: 1 */ - 1; unsigned int size = sizeof(item) + settings.chunk_size; // scr: ---------&gt; b 1)... memset(slabclass, 0, sizeof(slabclass)); while (++i &lt; MAX_NUMBER_OF_SLAB_CLASSES-1) &#123; if (slab_sizes != NULL) &#123; // scr: -----------------------------------&gt; a 1) if (slab_sizes[i-1] == 0) break; size = slab_sizes[i-1]; &#125; else if (size &gt;= settings.item_size_max / factor) &#123; break; &#125; /* Make sure items are always n-byte aligned */ if (size % CHUNK_ALIGN_BYTES) // scr: ---------------------------------&gt; 2) size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES); slabclass[i].size = size; slabclass[i].perslab = settings.item_size_max / slabclass[i].size; // -&gt; 3) if (slab_sizes == NULL) size *= factor; // scr: -----------------------------------------&gt; b 4) if (settings.verbose &gt; 1) &#123; fprintf(stderr, \"slab class %3d: chunk size %9u perslab %7u\\n\", i, slabclass[i].size, slabclass[i].perslab); &#125; &#125; // scr: -------------------------------------------------------------------&gt; 5) power_largest = i; slabclass[power_largest].size = settings.item_size_max; slabclass[power_largest].perslab = 1;...&#125; slabs_init@slabs.c 支线 a1) 使用 slab_sizes 里面的值； 2) 将 size 用 CHUNK_ALIGN_BYTES 对其，并赋值给 slabclass[i].size； 3) 计算 slabclass[i].perslab; 5) 用 settings.item_size_max 初始化最后一个 板组。 这里要注意 settings.item_size_max 是 slab 本身的大小，也即是 memcached 能存的最大对象。类似的，settings.item_size_max 也可以在 运行时 确定 settings.item_size_max = 1024 * 1024;settings_init@memcached.c:226 case 'I': buf = strdup(optarg); unit = buf[strlen(buf)-1]; if (unit == 'k' || unit == 'm' || unit == 'K' || unit == 'M') &#123; buf[strlen(buf)-1] = '\\0'; size_max = atoi(buf); if (unit == 'k' || unit == 'K') size_max *= 1024; if (unit == 'm' || unit == 'M') size_max *= 1024 * 1024; settings.item_size_max = size_max; &#125; else &#123; settings.item_size_max = atoi(buf); &#125; free(buf); if (settings.item_size_max &lt; 1024) &#123; fprintf(stderr, \"Item max size cannot be less than 1024 bytes.\\n\"); return 1; &#125; if (settings.item_size_max &gt; 1024 * 1024 * 128) &#123; fprintf(stderr, \"Cannot set item size limit higher than 128 mb.\\n\"); return 1; &#125; if (settings.item_size_max &gt; 1024 * 1024) &#123; fprintf(stderr, \"WARNING: Setting item max size above 1MB is not\" \" recommended!\\n\" \" Raising this limit increases the minimum memory requirements\\n\" \" and will decrease your memory efficiency.\\n\" ); &#125; break;main@memcached.c:5626 支线 b1) 用 settings.chunk_size 加上给每个对象附着的元数据（meta data）来计算基础大小（对象 item 会在后面讨论）； 2) 将 size 用 CHUNK_ALIGN_BYTES 对其，并赋值给 slabclass[i].size（同支线a）； 3) 计算 slabclass[i].perslab（同支线a）； 4) 用 factor(settings.factor) 计算下一个 板组 的大小； 5) 用 settings.item_size_max 初始化最后一个 板组。 引用memcached wiki 第2回 memcachedのメモリストレージを理解する Memcached源码分析之存储机制Slabs（7） Understanding Malloc Ch8 - Slab Allocator The Slab Allocator:An Object-Caching Kernel Memory Allocator","categories":[{"name":"Memcached 源码分析","slug":"Memcached-源码分析","permalink":"https://holmeshe.me/categories/Memcached-源码分析/"}],"tags":[{"name":"memcached","slug":"memcached","permalink":"https://holmeshe.me/tags/memcached/"},{"name":"slab allocator","slug":"slab-allocator","permalink":"https://holmeshe.me/tags/slab-allocator/"},{"name":"cache","slug":"cache","permalink":"https://holmeshe.me/tags/cache/"},{"name":"源码","slug":"源码","permalink":"https://holmeshe.me/tags/源码/"},{"name":"源码分析","slug":"源码分析","permalink":"https://holmeshe.me/tags/源码分析/"}]},{"title":"Understanding The Memcached Source Code-Event Driven II","slug":"understanding-memcached-source-code-VIII","date":"2019-01-27T20:00:00.000Z","updated":"2020-12-22T10:48:38.121Z","comments":true,"path":"understanding-memcached-source-code-VIII/","link":"","permalink":"https://holmeshe.me/understanding-memcached-source-code-VIII/","excerpt":"slab allocator (I, II, III) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I , II , III) for entry expiration; and an event driven model (I , II - this article , III) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. In classic multithreading, blocking I/O operations constrain the maximum number of requests a server can handle. Hence asynchronous event driven model is used to eliminate the throughput bottleneck. As such, the synchronous and potentially slow process is divided into logic segments that are free of I/O, and are executed asynchronously. When it comes to asynchronization, extra space is required to store contexts. This is because the logic segments, that could be associated with different sessions, are executed in an interleaved way. For instance, in the case when asynchronization is implemented (emulated) using synchronous multithreading, the “extra space” is in the form of thread stack. Whilst contexts are maintained in user land in event driven. conn is the representative of those contexts in Memcached.","text":"slab allocator (I, II, III) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I , II , III) for entry expiration; and an event driven model (I , II - this article , III) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. In classic multithreading, blocking I/O operations constrain the maximum number of requests a server can handle. Hence asynchronous event driven model is used to eliminate the throughput bottleneck. As such, the synchronous and potentially slow process is divided into logic segments that are free of I/O, and are executed asynchronously. When it comes to asynchronization, extra space is required to store contexts. This is because the logic segments, that could be associated with different sessions, are executed in an interleaved way. For instance, in the case when asynchronization is implemented (emulated) using synchronous multithreading, the “extra space” is in the form of thread stack. Whilst contexts are maintained in user land in event driven. conn is the representative of those contexts in Memcached. Core data structure - conntypedef struct conn conn;struct conn &#123; int sfd;...// scr: not applicable enum conn_states state;...// scr: not applicable struct event event; short ev_flags; short which; /** which events were just triggered */ char *rbuf; /** buffer to read commands into */ char *rcurr; /** but if we parsed some already, this is where we stopped */ int rsize; /** total allocated size of rbuf */ int rbytes; /** how much data, starting from rcur, do we have unparsed */ char *wbuf; char *wcurr; int wsize; int wbytes; /** which state to go into after finishing current write */ enum conn_states write_and_go; void *write_and_free; /** free this memory after finishing writing */ char *ritem; /** when we read in an item's value, it goes here */ int rlbytes; /* data for the nread state */ /** * item is used to hold an item structure created after reading the command * line of set/add/replace commands, but before we finished reading the actual * data. The data is read into ITEM_data(item) to avoid extra copying. */ void *item; /* for commands set/add/replace */ /* data for the swallow state */ int sbytes; /* how many bytes to swallow */ /* data for the mwrite state */ struct iovec *iov; int iovsize; /* number of elements allocated in iov[] */ int iovused; /* number of elements used in iov[] */ struct msghdr *msglist; int msgsize; /* number of elements allocated in msglist[] */ int msgused; /* number of elements used in msglist[] */ int msgcurr; /* element in msglist[] being transmitted now */ int msgbytes; /* number of bytes in current msg */ item **ilist; /* list of items to write out */ int isize; item **icurr; int ileft;...// scr: not applicable enum protocol protocol; /* which protocol this connection speaks */...// scr: not applicable socklen_t request_addr_size; unsigned char *hdrbuf; /* udp packet headers */ int hdrsize; /* number of headers' worth of space is allocated */ bool noreply; /* True if the reply should not be sent. */...// scr: not applicable short cmd; /* current command being processed */...// scr: not applicable int keylen; conn *next; /* Used for generating a list of conn structures */ LIBEVENT_THREAD *thread; /* Pointer to the thread object serving this connection */&#125;; Properties in discussionfd - the file descriptor a event is rooted. used by last post state - the main focus of this post rbuf - read buffer address. used by try_read_network rcurr - address of unprocessed data. used by try_read_network rsize - current size of the read buffer. used by try_read_network rbytes - size of data to be processed (it is also used as an indicator for leftover data in various places). initialised by try_read_network, updated by process_get_command, used by try_read_command last_cmd_time - updated when start processing a command. used by try_read_network ilist - the item list that is associated with the context; icurr and ileft indicate the current entry and number of entries left. used by process_get_command, conn_release_items iov - the actual storage for pointers of output data, which is used by msglist; iovsize, and iovused are its allocated size and used size respectively. initialised by process_command, used by add_iov, ensure_iov_space Here the data structures (struct msghdr and struct iovec) is required by sendmsg. The relevant text about the API is pasted bellow. The msg_iov and msg_iovlen fields of message specify zero or more buffers containing the data to be sent. msg_iov points to an array of iovec structures; msg_iovlen shall be set to the dimension of this array. In each iovec structure, the iov_base field specifies a storage area and the iov_len field gives its size in bytes. Some of these sizes can be zero. The data from each storage area indicated by msg_iov is sent in turn.… msglist - the list that stores the struct msghdr themselves; msgsize and msgused are its allocated size and used size respectively; msgbytes indicates totall size of the output data size; msgcurr points to the index that has been processed (written). Yet nothing is bette than a chart to demonstrate the data structures and the layout in memory. State switchAn event triggers cascading changes of states which in turn invokes various procedures, before drive machine relinquishes control and waits for a new event arrival. In last post, we have seen this process on dispatch thread, in which 1) conn_listening is triggered by a new connection; 2) dispatch_conn_new is invoked, which transfer the new accepted fd, as well as succeeding events to one of the worker threads; 3) dispatch thread gives up CPU and waits for new “new connection” events. In this post, we are going to see more complex state switches that effectively link together the procedures we discussed in LRU III, The state of a given session is represented by conn.state of its associated context. and this time we are going to adopt a similar approach as LRU III, i.e., sending telnet commands to a Memcached instance, to navigate the outermost layer of the Memcached application. We will also switch ON the convenient verbose with -vvv to better observe the internal state transitions. ...case 'v': settings.verbose++; break;...memcached.c:5518 ReadFirstly (as usual) we telnet to the Memcached instance, and add some items ...// add some items~telnet localhost 11211Trying 127.0.0.1...Connected to localhost.Escape character is '^]'.... Telnet input ...&lt;36 new auto-negotiating client connection... Server verbose Here 36 is the accepted fd. As mentioned, the following operations will be on this fd. Next we send the exact same read command to the Memcached instance as in LRU III &gt; get test Telnet input ...36: going from conn_new_cmd to conn_waiting36: going from conn_waiting to conn_read36: going from conn_read to conn_parse_cmd36: Client using the ascii protocol&lt;36 get test&gt; FOUND KEY test&gt;36 sending key test&gt;36 END36: going from conn_parse_cmd to conn_mwrite36: going from conn_mwrite to conn_new_cmd36: going from conn_new_cmd to conn_waiting36: going from conn_waiting to conn_read... Server verbose As mentioned in last post, the initial state of worker threads is conn_new_cmdso we get started from here. ...static void drive_machine(conn *c) &#123; int nreqs = settings.reqs_per_event; // scr: --------&gt; 1)... case conn_new_cmd: /* Only process nreqs at a time to avoid starving other connections */ --nreqs; // scr: ----------------------------&gt; 1) if (nreqs &gt;= 0) &#123; reset_cmd_handler(c); // scr: -----------&gt; 2) &#125; else &#123; pthread_mutex_lock(&amp;c-&gt;thread-&gt;stats.mutex); c-&gt;thread-&gt;stats.conn_yields++; pthread_mutex_unlock(&amp;c-&gt;thread-&gt;stats.mutex); if (c-&gt;rbytes &gt; 0) &#123;...// scr: error handling &#125; stop = true; // scr: --------------------&gt; 3) &#125; break;...static void reset_cmd_handler(conn *c) &#123; c-&gt;cmd = -1; c-&gt;substate = bin_no_state; if(c-&gt;item != NULL) &#123; item_remove(c-&gt;item); c-&gt;item = NULL; &#125; conn_shrink(c); if (c-&gt;rbytes &gt; 0) &#123; conn_set_state(c, conn_parse_cmd); // scr: -----&gt; 2a) &#125; else &#123; conn_set_state(c, conn_waiting); // scr: -------&gt; 2b) &#125;&#125; memcached.c:4361 & reset_cmd_handler 1) nreqs (settings.reqs_per_event) is the maximum requests one event loop iteration should handle. Note that the threshold is needed because new connections will not be handled if one event loop iteration takes too long to complete. Note also that the connection being interrupted will be fired again and get the chance to enter the drive machine with a “read” event since the descriptor is set with EV_PERSIST in the last post. ...settings.reqs_per_event = 20;...case 'R': settings.reqs_per_event = atoi(optarg); if (settings.reqs_per_event == 0) &#123; fprintf(stderr, \"Number of requests per event must be greater than 0\\n\"); return 1; &#125; break;...memcached.c:5545, 6112 2) Initializes the relevant properties in the context for a new command. 2a) If there are leftover data, then switch to conn_parse_cmd directly. 2b) If it is a fresh session, then switch to conn_waiting. 3) Yield the current iteration when the threshold is reached. conn_waiting... case conn_waiting: if (!update_event(c, EV_READ | EV_PERSIST)) &#123; if (settings.verbose &gt; 0) fprintf(stderr, \"Couldn't update event\\n\"); conn_set_state(c, conn_closing); break; &#125; conn_set_state(c, conn_read); stop = true; break;... memcached.c:4322 Simply reset the descriptor with the original flags (i.e., EV_READ, EV_PERSIST), update the state of the context to the next hop (conn_read), and relinquish the CPU. conn_read...case conn_read: res = IS_UDP(c-&gt;transport) ? try_read_udp(c) : try_read_network(c); //1) switch (res) &#123; case READ_NO_DATA_RECEIVED: conn_set_state(c, conn_waiting); break; case READ_DATA_RECEIVED: conn_set_state(c, conn_parse_cmd); // scr: ---------------------&gt; 2) break; case READ_ERROR: conn_set_state(c, conn_closing); break; case READ_MEMORY_ERROR: /* Failed to allocate more memory */ /* State already set by try_read_network */ break; &#125; break;... memcached.c:4334 1) Read from the file descriptor and save the data to the context. 2) Switch to the next state, conn_parse_cmd. try_read_networkstatic enum try_read_result try_read_network(conn *c) &#123; enum try_read_result gotdata = READ_NO_DATA_RECEIVED; int res; int num_allocs = 0; assert(c != NULL); if (c-&gt;rcurr != c-&gt;rbuf) &#123; // scr: -------------------------&gt; 1) if (c-&gt;rbytes != 0) /* otherwise there's nothing to copy */ memmove(c-&gt;rbuf, c-&gt;rcurr, c-&gt;rbytes); c-&gt;rcurr = c-&gt;rbuf; &#125; while (1) &#123; if (c-&gt;rbytes &gt;= c-&gt;rsize) &#123; // scr: -------------------&gt; 2) if (num_allocs == 4) &#123; return gotdata; &#125; ++num_allocs; char *new_rbuf = realloc(c-&gt;rbuf, c-&gt;rsize * 2); if (!new_rbuf) &#123;...// scr: error handling &#125; c-&gt;rcurr = c-&gt;rbuf = new_rbuf; c-&gt;rsize *= 2; &#125; int avail = c-&gt;rsize - c-&gt;rbytes; // scr: --------------&gt; 3) res = read(c-&gt;sfd, c-&gt;rbuf + c-&gt;rbytes, avail); if (res &gt; 0) &#123;...// scr: stat gotdata = READ_DATA_RECEIVED; c-&gt;rbytes += res; if (res == avail) &#123; // scr: -----------------------&gt; 3a) continue; &#125; else &#123; break; // scr: --------------------------------&gt; 3b) &#125; &#125; if (res == 0) &#123; return READ_ERROR; &#125; if (res == -1) &#123; if (errno == EAGAIN || errno == EWOULDBLOCK) &#123; //src:3b) break; &#125; return READ_ERROR; &#125; &#125; return gotdata;&#125; memcached.c:try_read_network Here the while (1) is used to handle logic flow for buffer expanding instead of loop. 1) Move rcurr to the beginning of the read buffer. 2) If the data size exceeds the read buffer size, try expanding the buffer (for at most 4 times). 3) Calculate the available buffer space for reading from the socket, and update rbytes accordingly. 3a) Goto 2) if the buffer is full. 3b) Return READ_DATA_RECEIVED, which switches the state to conn_parse_cmd in the state machine pass through. conn_parse_cmd...case conn_parse_cmd : if (try_read_command(c) == 0) &#123; /* wee need more data! */ conn_set_state(c, conn_waiting); &#125; break;... memcached.c:try_read_network try_read_commandstatic int try_read_command(conn *c) &#123; assert(c != NULL); assert(c-&gt;rcurr &lt;= (c-&gt;rbuf + c-&gt;rsize)); assert(c-&gt;rbytes &gt; 0); if (c-&gt;protocol == negotiating_prot || c-&gt;transport == udp_transport) &#123; if ((unsigned char)c-&gt;rbuf[0] == (unsigned char)PROTOCOL_BINARY_REQ) &#123; c-&gt;protocol = binary_prot; &#125; else &#123; c-&gt;protocol = ascii_prot; // scr: -------------------------&gt; 1) &#125; if (settings.verbose &gt; 1) &#123; // scr: ---------------------------&gt; ~) fprintf(stderr, \"%d: Client using the %s protocol\\n\", c-&gt;sfd, prot_text(c-&gt;protocol)); &#125; &#125; if (c-&gt;protocol == binary_prot) &#123;...// scr: not applicable &#125; else &#123; char *el, *cont; if (c-&gt;rbytes == 0) return 0; el = memchr(c-&gt;rcurr, '\\n', c-&gt;rbytes); // scr: ---------------&gt; 2) if (!el) &#123;...// scr: not applicable &#125; cont = el + 1; if ((el - c-&gt;rcurr) &gt; 1 &amp;&amp; *(el - 1) == '\\r') &#123; el--; &#125; *el = '\\0'; // scr: -------------------------------------------&gt; 2) assert(cont &lt;= (c-&gt;rcurr + c-&gt;rbytes)); c-&gt;last_cmd_time = current_time; // scr: ----------------------&gt; 3) process_command(c, c-&gt;rcurr); // scr: -------------------------&gt; 4) c-&gt;rbytes -= (cont - c-&gt;rcurr); // scr: -----------------------&gt; 5) c-&gt;rcurr = cont; // scr: --------------------------------------&gt; 6) assert(c-&gt;rcurr &lt;= (c-&gt;rbuf + c-&gt;rsize)); &#125; return 1;&#125; memcached.c:try_read_command 1) Determine the protocol type, in this case is ascii_prot. ~) Verbose message we saw in the beginning. 2) Trim all the &#39;\\n&#39; and &#39;\\r&#39; in the end, store the position of the command last character to el, and store the command end to cont. 3) Update last_cmd_time. 4) Call process_command which locates the “get” command and call process_get_command. In process_command, a) tokenize_command is a string parsing method that stores command (i.e., “get”) in tokens[COMMAND_TOKEN] and key (i.e., test) in tokens[KEY_TOKEN]; b) initialization of msgcurr, msgused, iovused; c) initialization other fields in add_msghdr; and d) process_get_command is the next step. ...c-&gt;msgcurr = 0; // scr: ---------------------------------------------&gt; b)c-&gt;msgused = 0;c-&gt;iovused = 0;if (add_msghdr(c) != 0) &#123; out_of_memory(c, \"SERVER_ERROR out of memory preparing response\"); return;&#125;ntokens = tokenize_command(command, tokens, MAX_TOKENS); // scr: ----&gt; a)if (ntokens &gt;= 3 &amp;&amp; ((strcmp(tokens[COMMAND_TOKEN].value, \"get\") == 0) || (strcmp(tokens[COMMAND_TOKEN].value, \"bget\") == 0))) &#123; process_get_command(c, tokens, ntokens, false); // scr: ---------&gt; c)&#125; else if ......memcached.c:process_command 5) Update rbytes with the length of the command that has been processed (cont - c-&gt;rcurr). 6) Move the rcurr to the unprocessed data located at end of the command portion. add_msghdrBefore the logic reaches process_get_command, an entry should be initialised in msglist for the current command. static int add_msghdr(conn *c)&#123; struct msghdr *msg; assert(c != NULL); if (c-&gt;msgsize == c-&gt;msgused) &#123; // scr: --------------------&gt; 1) msg = realloc(c-&gt;msglist, c-&gt;msgsize * 2 * sizeof(struct msghdr)); if (! msg) &#123; STATS_LOCK(); stats.malloc_fails++; STATS_UNLOCK(); return -1; &#125; c-&gt;msglist = msg; c-&gt;msgsize *= 2; &#125; msg = c-&gt;msglist + c-&gt;msgused; // scr: ---------------------&gt; 2) /* this wipes msg_iovlen, msg_control, msg_controllen, and msg_flags, the last 3 of which aren't defined on solaris: */ memset(msg, 0, sizeof(struct msghdr)); // scr: -------------&gt; 3) msg-&gt;msg_iov = &amp;c-&gt;iov[c-&gt;iovused]; // scr: ----------------&gt; 3) if (IS_UDP(c-&gt;transport) &amp;&amp; c-&gt;request_addr_size &gt; 0) &#123;...// scr: UDP related &#125; c-&gt;msgbytes = 0; // scr: -----------------------------------&gt; 4) c-&gt;msgused++; // scr: --------------------------------------&gt; 5) if (IS_UDP(c-&gt;transport)) &#123;...// scr: UDP related &#125; return 0;&#125; add_msghdr@memcached.c 1) Expand the msglist when required. 2) Point to the next empty entry in msglist with msg. 3) Initialise the entry pointed by msg. Here the critical operation is msg-&gt;msg_iov = &amp;c-&gt;iov[c-&gt;iovused]; which links the msglist to the specific entry in iov. (Figure - msglist &amp; iov) 4) Initialise msgbytes to 0. 5) Update msgused accordingly. process_get_commandWe have seen this method in beginning of LRU III. This time, we will complete its pass through with the context of event driven. static inline void process_get_command(conn *c, token_t *tokens, size_t ntokens, bool return_cas) &#123; char *key; size_t nkey; int i = 0; item *it; token_t *key_token = &amp;tokens[KEY_TOKEN]; char *suffix; assert(c != NULL); do &#123; while(key_token-&gt;length != 0) &#123; // scr: -----------------&gt; 1) key = key_token-&gt;value; nkey = key_token-&gt;length; if(nkey &gt; KEY_MAX_LENGTH) &#123; out_string(c, \"CLIENT_ERROR bad command line format\"); while (i-- &gt; 0) &#123; item_remove(*(c-&gt;ilist + i)); &#125; return; &#125; it = item_get(key, nkey, c); // scr: ----------------&gt; 2) if (settings.detail_enabled) &#123; stats_prefix_record_get(key, nkey, NULL != it); &#125; if (it) &#123; if (i &gt;= c-&gt;isize) &#123; // scr: --------------------&gt; 3) item **new_list = realloc(c-&gt;ilist, sizeof(item *) * c-&gt;isize * 2); if (new_list) &#123; c-&gt;isize *= 2; c-&gt;ilist = new_list; &#125; else &#123;...// scr: stat item_remove(it); break; &#125; &#125; if (return_cas) &#123;...// scr: cas &#125; else &#123; MEMCACHED_COMMAND_GET(c-&gt;sfd, ITEM_key(it), it-&gt;nkey, it-&gt;nbytes, ITEM_get_cas(it)); if (add_iov(c, \"VALUE \", 6) != 0 || // scr: ---&gt; 4) add_iov(c, ITEM_key(it), it-&gt;nkey) != 0 || add_iov(c, ITEM_suffix(it), it-&gt;nsuffix + it-&gt;nbytes) != 0) &#123; item_remove(it); break; &#125; &#125;...// scr: verbose &amp; stat item_update(it); // scr: ------------------------&gt; 5) *(c-&gt;ilist + i) = it; // scr: -------------------&gt; 6) i++; &#125; else &#123;...// scr: stat &#125; key_token++; // scr: --------------------------------&gt; 1) &#125; /* * If the command string hasn't been fully processed, get the next set * of tokens. */ if(key_token-&gt;value != NULL) &#123; // scr: ------------------&gt; 1) ntokens = tokenize_command(key_token-&gt;value, tokens, MAX_TOKENS); key_token = tokens; &#125; &#125; while(key_token-&gt;value != NULL); c-&gt;icurr = c-&gt;ilist; // scr: --------------------------------&gt; 6) c-&gt;ileft = i; scr: ------------------------------------------&gt; 6)...// scr: cas &amp; verbose if (key_token-&gt;value != NULL || add_iov(c, \"END\\r\\n\", 5) != 0 || (IS_UDP(c-&gt;transport) &amp;&amp; build_udp_headers(c) != 0)) &#123; out_of_memory(c, \"SERVER_ERROR out of memory writing get response\"); &#125; else &#123; // scr: ----------------------------------------------&gt; 7) conn_set_state(c, conn_mwrite); c-&gt;msgcurr = 0; &#125;&#125; process_get_command@memcached.c 1) Iterate through key token array. Here we got one key token ‘test’. 2) Call item_get for the item pointer. 3) Increase the ilist size if it is full, and . Here ilist stores the item being processed. In the end of the current command processing, this list is used to batch release the items reference counts. 4) add_iov prepares the output of this session. 5) Call item_update to manipulate the LRU lists. 6) Link the item currently being processed to ilist, and update the associated fields. 7) Move on to the next state conn_mwrite. add_iovstatic int add_iov(conn *c, const void *buf, int len) &#123; struct msghdr *m; int leftover; bool limit_to_mtu; assert(c != NULL); do &#123; m = &amp;c-&gt;msglist[c-&gt;msgused - 1]; // scr: -------------------&gt; 1) /* * Limit UDP packets, and the first payloads of TCP replies, to * UDP_MAX_PAYLOAD_SIZE bytes. */ limit_to_mtu = IS_UDP(c-&gt;transport) || (1 == c-&gt;msgused); /* We may need to start a new msghdr if this one is full. */ if (m-&gt;msg_iovlen == IOV_MAX || (limit_to_mtu &amp;&amp; c-&gt;msgbytes &gt;= UDP_MAX_PAYLOAD_SIZE)) &#123; add_msghdr(c); m = &amp;c-&gt;msglist[c-&gt;msgused - 1]; // scr: ---------------&gt; 7) &#125; if (ensure_iov_space(c) != 0) // scr: ----------------------&gt; 2) return -1; /* If the fragment is too big to fit in the datagram, split it up */ if (limit_to_mtu &amp;&amp; len + c-&gt;msgbytes &gt; UDP_MAX_PAYLOAD_SIZE) &#123; leftover = len + c-&gt;msgbytes - UDP_MAX_PAYLOAD_SIZE; //scr*) len -= leftover; &#125; else &#123; leftover = 0; &#125; m = &amp;c-&gt;msglist[c-&gt;msgused - 1]; // scr: ------------------&gt; 1) m-&gt;msg_iov[m-&gt;msg_iovlen].iov_base = (void *)buf; // scr: -&gt; 3) m-&gt;msg_iov[m-&gt;msg_iovlen].iov_len = len; c-&gt;msgbytes += len; // scr: -------------------------------&gt; 4) c-&gt;iovused++; // scr: -------------------------------------&gt; 5) m-&gt;msg_iovlen++; // scr: ----------------------------------&gt; 6) buf = ((char *)buf) + len; len = leftover; &#125; while (leftover &gt; 0); return 0;&#125; add_iov@memcached.c This method initialised an entry on iov list and add it to the last in-use item in msglist (Figure - msglist &amp; iov). 1) Get the tail of the in use portion of msglist. 2) Expend iov if necessary. 3) Initialize the iov_base and iov_len fields within the iov entry. Note that the msg_iov has been linked to the position of specific entry in iov, hence operations on msg_iov change the content of iov as well. 4) Update msgbytes with the total item size. 5, 6) Update iovused and msg_iovlen accordingly. 7) Handle MTU with the assistance of do while loop. conn_mwrite... case conn_mwrite: if (IS_UDP(c-&gt;transport) &amp;&amp; c-&gt;msgcurr == 0 &amp;&amp; build_udp_headers(c) != 0) &#123;...// scr: UDP related &#125; switch (transmit(c)) &#123;...// scr: state processing &#125; break;... memcached.c:4521 Before explaining the logic process of conn_mwrite state, we look at the essential within first, which is transmitstatic enum transmit_result transmit(conn *c) &#123; assert(c != NULL); if (c-&gt;msgcurr &lt; c-&gt;msgused &amp;&amp; c-&gt;msglist[c-&gt;msgcurr].msg_iovlen == 0) &#123; // scr: ---------&gt; 1) /* Finished writing the current msg; advance to the next. */ c-&gt;msgcurr++; &#125; if (c-&gt;msgcurr &lt; c-&gt;msgused) &#123; // scr: ----------------------------&gt; 2) ssize_t res; struct msghdr *m = &amp;c-&gt;msglist[c-&gt;msgcurr]; res = sendmsg(c-&gt;sfd, m, 0); if (res &gt; 0) &#123;...// scr: state /* We've written some of the data. Remove the completed iovec entries from the list of pending writes. */ while (m-&gt;msg_iovlen &gt; 0 &amp;&amp; res &gt;= m-&gt;msg_iov-&gt;iov_len) &#123; res -= m-&gt;msg_iov-&gt;iov_len; m-&gt;msg_iovlen--; m-&gt;msg_iov++; &#125; /* Might have written just part of the last iovec entry; adjust it so the next write will do the rest. */ if (res &gt; 0) &#123; m-&gt;msg_iov-&gt;iov_base = (caddr_t)m-&gt;msg_iov-&gt;iov_base + res; m-&gt;msg_iov-&gt;iov_len -= res; &#125; return TRANSMIT_INCOMPLETE; &#125; if (res == -1 &amp;&amp; (errno == EAGAIN || errno == EWOULDBLOCK)) &#123; // 3) if (!update_event(c, EV_WRITE | EV_PERSIST)) &#123; if (settings.verbose &gt; 0) fprintf(stderr, \"Couldn't update event\\n\"); conn_set_state(c, conn_closing); return TRANSMIT_HARD_ERROR; &#125; return TRANSMIT_SOFT_ERROR; &#125; /* if res == 0 or res == -1 and error is not EAGAIN or EWOULDBLOCK, we have a real error, on which we close the connection */ if (settings.verbose &gt; 0) perror(\"Failed to write, and not due to blocking\"); if (IS_UDP(c-&gt;transport))...// scr: UDP related else conn_set_state(c, conn_closing); return TRANSMIT_HARD_ERROR; // scr: --------------------------&gt; 4) &#125; else &#123; return TRANSMIT_COMPLETE; // scr: ----------------------------&gt; 5) &#125;&#125; transmit@memcached.c As the essential method of state conn_mwrite processing, transmit goes through the msglist (starting from 0, the initial value) and tries its best to send out all the pending data accumulated in the current session. This is done within itself or in subsequent passes through the event loop. Only when blocking operation is indicated by EAGAIN or EWOULDBLOCK, the state machine stops the current event loop iteration, and the same session will be resumed when the buffer space becomes available again. 1) If the msg_iovlen is 0, the writing of msgcurr slot has finished, hence move to the next slot. 2) Call sendmsg and move msg_iov, iov_base and iov_len according to the data length (res) that has been written successfully. This leads to case b) of the state processing. 3) As mentioned, EAGAIN or EWOULDBLOCK returned by sendmsg leads to case c) of state processing. 4) Errors other than the above two lead to case c) of state processing. 5) c-&gt;msgcurr &gt;= c-&gt;msgused means write of all data of the session finished, which leads to b) of the state processing. Back to state processing... case conn_mwrite: if (IS_UDP(c-&gt;transport) &amp;&amp; c-&gt;msgcurr == 0 &amp;&amp; build_udp_headers(c) != 0) &#123;...// scr: UDP related &#125; switch (transmit(c)) &#123; case TRANSMIT_COMPLETE: if (c-&gt;state == conn_mwrite) &#123; // scr: ------------&gt; a) conn_release_items(c); /* XXX: I don't know why this wasn't the general case */ if(c-&gt;protocol == binary_prot) &#123; conn_set_state(c, c-&gt;write_and_go); &#125; else &#123; conn_set_state(c, conn_new_cmd); &#125; &#125; else if (c-&gt;state == conn_write) &#123;...// scr: not applicable &#125; else &#123;...// scr: not applicable &#125; break; case TRANSMIT_INCOMPLETE: // scr: ---------------------&gt; b) case TRANSMIT_HARD_ERROR: break; /* Continue in state machine. */ case TRANSMIT_SOFT_ERROR: // scr: ---------------------&gt; c) stop = true; break; &#125; break;... memcached.c:4521 According to the result of transmit, the logic flows to the following 3 branches, a) If the result is TRANSMIT_COMPLETE, 1) finalise the current command processing with conn_release_items; 2) switch the state to conn_new_cmd which 3) eventually falls to conn_waiting and, as discussed, finishes the current event loop. b) If the result is TRANSMIT_INCOMPLETE and TRANSMIT_HARD_ERROR, the state machine keeps the same state, and the subsequent passes through the event loop continues consuming more data in msglist. Unlike read operation, TRANSMIT_INCOMPLETE does not lead to immediate event loop finish because write operation does not block until buffer is full. c) TRANSMIT_SOFT_ERROR means the buffer is full, hence finish the current event loop iteration straight away. Finish readstatic void conn_release_items(conn *c) &#123; assert(c != NULL); if (c-&gt;item) &#123;...// scr: not applicable &#125; while (c-&gt;ileft &gt; 0) &#123; item *it = *(c-&gt;icurr); assert((it-&gt;it_flags &amp; ITEM_SLABBED) == 0); item_remove(it); // scr: ---------------------&gt; 1) c-&gt;icurr++; c-&gt;ileft--; &#125;...// scr: cas c-&gt;icurr = c-&gt;ilist;...// scr: cas&#125; conn_release_items@memcached.c Not sure if you noticed or not, there is a subtle bug in the LRU III, the reference count of the item in read operation is not returned to 0 as in other operations. This is because 1) all items ownership are batched released here at the end the (read) command processing.","categories":[{"name":"Memcached Source Code","slug":"Memcached-Source-Code","permalink":"https://holmeshe.me/categories/Memcached-Source-Code/"}],"tags":[{"name":"socket","slug":"socket","permalink":"https://holmeshe.me/tags/socket/"},{"name":"memcached","slug":"memcached","permalink":"https://holmeshe.me/tags/memcached/"},{"name":"event driven","slug":"event-driven","permalink":"https://holmeshe.me/tags/event-driven/"},{"name":"state machine","slug":"state-machine","permalink":"https://holmeshe.me/tags/state-machine/"},{"name":"multithreading","slug":"multithreading","permalink":"https://holmeshe.me/tags/multithreading/"}]},{"title":"Understanding The Memcached Source Code - Event Driven I","slug":"understanding-memcached-source-code-VII","date":"2019-01-17T20:00:00.000Z","updated":"2020-12-22T10:48:38.121Z","comments":true,"path":"understanding-memcached-source-code-VII/","link":"","permalink":"https://holmeshe.me/understanding-memcached-source-code-VII/","excerpt":"slab allocator (I, II, III) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I , II , III) for entry expiration; and an event driven model (I - this article , II , III) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. In classic multithreading, large amounts of slow and blocking operations, mostly, I/O, can easily drain out available thread resources, which severely constrains the maximum number of requests a server can handle per unit time. More specific, threads are scheduled out and put into sleep in the middle of procedures that contain blocking I/O, despite piling up requests packets queuing within the network stack. In such situation, server side will show low throughput, low CPU saturation and high latency.","text":"slab allocator (I, II, III) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I , II , III) for entry expiration; and an event driven model (I - this article , II , III) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. In classic multithreading, large amounts of slow and blocking operations, mostly, I/O, can easily drain out available thread resources, which severely constrains the maximum number of requests a server can handle per unit time. More specific, threads are scheduled out and put into sleep in the middle of procedures that contain blocking I/O, despite piling up requests packets queuing within the network stack. In such situation, server side will show low throughput, low CPU saturation and high latency. Here is a post around server side performance. Feel free to check it out. An introduction to event drivenThis is where asynchronous event driven model comes in, which drops the idea that context of a session must be coupled with a thread. In such model, session contexts are contained within and managed by a drive machine and a thread is fully unleashed with unblocking I/O operations. More specific, 1) when I/O occurs in the middle of a procedure, a thread does not block but instantly switch to the processing of another request; and 2) when the I/O completes, the context will be picked up by the drive machine to resume the interrupted session. As such, a potentially slow procedure is effectively divided into multiple manageable pieces, and the cutting points are marked by I/O operations. This results in more performant single threaded architecture in comparison to those employ thousands of threads. In my understanding, event driven model, in its essential, is yet another instance of “divide and conquer” and “trade space for time” in a not very obvious way. On the other hand, multithreading can be still used in event driven model purely for the purpose of parallelism. Thus, in practice, the number of threads employed does not exceed that of CPU cores. I will discuss the Memcached multithreading soon in Thread model. The drive machineFrom a developer’s point of view, there are numerous ways to program an asynchronous even driven server. Memcached adopts an approach called state machine, in which logic flow is divided into non-linear fragments identified with states, which is normally controled by a huge switch case. The brightside of this approach is that the mentioned breakdown of slow procedure is sincerely reflected by the logic fragments. But it makes the code style a bit different from what most developers are already used to. Following is how the event driven state machine actually looks like. static void drive_machine(conn *c) &#123; bool stop = false; int sfd; socklen_t addrlen; struct sockaddr_storage addr; int nreqs = settings.reqs_per_event; int res; const char *str;... assert(c != NULL); while (!stop) &#123; switch(c-&gt;state) &#123; case conn_listening: addrlen = sizeof(addr);... sfd = accept(c-&gt;sfd, (struct sockaddr *)&amp;addr, &amp;addrlen);... if (sfd == -1) &#123;... &#125;... if (settings.maxconns_fast &amp;&amp; stats_state.curr_conns + stats_state.reserved_fds &gt;= settings.maxconns - 1) &#123;... &#125; else &#123; dispatch_conn_new(sfd, conn_new_cmd, EV_READ | EV_PERSIST, DATA_BUFFER_SIZE, c-&gt;transport); &#125; stop = true; break; case conn_waiting:... stop = true; break; case conn_read:... break; case conn_parse_cmd :... break; case conn_new_cmd:... break; case conn_nread:... break; case conn_swallow:... break; case conn_write:... case conn_mwrite:... break; case conn_closing: if (IS_UDP(c-&gt;transport)) conn_cleanup(c); else conn_close(c); stop = true; break; case conn_closed: /* This only happens if dormando is an idiot. */ abort(); break; case conn_watch: /* We handed off our connection to the logger thread. */ stop = true; break; case conn_max_state: assert(false); break; &#125; &#125; return;&#125; drive_machine@memcached.c The omitted switch blocks will be discussed in detail in following posts, so no worries. Thread modelThe thread model of Memcached is quite standard. There is a dispatcher thread, and there are preconfigured number of worker threads. Each thread runs an independent drive machine described above. The dispatcher thread, of which the responsible is to distribute requests among worker threads, only executes code under conn_listening. The actual requests are completed by worker threads running on the rest of the states. Next we go through the bootstrap portion of main function which establishes the various building blocks of event driven as well as the multithreading mechanism. And we will also see locations of the discussed sub-system ***_init methods in relation to the whole initialization process. First thing first, all the system initialization relevant procedures are executed in the discussed dispatcher thread. The call stack of this process is |-main |-hash_init (LRU II) |-assoc_init (LRU II) |-conn_init |-slabs_init (Slab I) |-memcached_thread_init |-setup_thread |-create_worker |-server_sockets |-new_socket |-conn_new System initializationint main (int argc, char **argv) &#123;...// scr: -----------------------------------------------------&gt; *)...// scr: initialize `settings` using default values and command line arguements...// scr: sanity check if (hash_init(hash_type) != 0) &#123; // scr: ---------------&gt; LRU II fprintf(stderr, \"Failed to initialize hash_algorithm!\\n\"); exit(EX_USAGE); &#125;...// scr: initialize `settings` &amp; sanity check if (maxcore != 0) &#123; // scr: --------------------------------&gt; 1) struct rlimit rlim_new; /* * First try raising to infinity; if that fails, try bringing * the soft limit to the hard. */ if (getrlimit(RLIMIT_CORE, &amp;rlim) == 0) &#123; rlim_new.rlim_cur = rlim_new.rlim_max = RLIM_INFINITY; if (setrlimit(RLIMIT_CORE, &amp;rlim_new)!= 0) &#123; /* failed. try raising just to the old max */ rlim_new.rlim_cur = rlim_new.rlim_max = rlim.rlim_max; (void)setrlimit(RLIMIT_CORE, &amp;rlim_new); &#125; &#125; /* * getrlimit again to see what we ended up with. Only fail if * the soft limit ends up 0, because then no core files will be * created at all. */ if ((getrlimit(RLIMIT_CORE, &amp;rlim) != 0) || rlim.rlim_cur == 0) &#123; fprintf(stderr, \"failed to ensure corefile creation\\n\"); exit(EX_OSERR); &#125; &#125; if (getrlimit(RLIMIT_NOFILE, &amp;rlim) != 0) &#123; // scr: --------&gt; 1) fprintf(stderr, \"failed to getrlimit number of files\\n\"); exit(EX_OSERR); &#125; else &#123; rlim.rlim_cur = settings.maxconns; rlim.rlim_max = settings.maxconns; if (setrlimit(RLIMIT_NOFILE, &amp;rlim) != 0) &#123; fprintf(stderr, \"failed to set rlimit for open files. Try starting as root or requesting smaller maxconns value.\\n\"); exit(EX_OSERR); &#125; &#125;...// scr: -----------------------------------------------------&gt; *) main_base = event_init(); // scr: --------------------------&gt; 2)...// scr: stat assoc_init(settings.hashpower_init); // scr: -----------&gt; LRU II conn_init(); // scr: ---------------------------------------&gt; 3) slabs_init(settings.maxbytes, settings.factor, preallocate, use_slab_sizes ? slab_sizes : NULL); // scr: ---&gt; Slab I... memcached_thread_init(settings.num_threads, main_base); //scr:4)...// scr: maintainer threads initialization...// scr: unix socket /* create the listening socket, bind it, and init */ if (settings.socketpath == NULL) &#123;...// scr: not applicable if (portnumber_filename != NULL) &#123;...// scr: not applicable &#125; errno = 0; // scr: -------------------------------------&gt; 5) if (settings.port &amp;&amp; server_sockets(settings.port, tcp_transport, portnumber_file)) &#123; vperror(\"failed to listen on TCP port %d\", settings.port); exit(EX_OSERR); &#125; errno = 0; // scr: -------------------------------------&gt; 5) if (settings.udpport &amp;&amp; server_sockets(settings.udpport, udp_transport, portnumber_file)) &#123; vperror(\"failed to listen on UDP port %d\", settings.udpport); exit(EX_OSERR); &#125; if (portnumber_file) &#123;...// scr: not applicable &#125; usleep(1000);... if (event_base_loop(main_base, 0) != 0) &#123; // scr: ----------&gt; 6) retval = EXIT_FAILURE; &#125;...// scr: finalization&#125; main@memcached.c The two relevant steps are 4) and 5) which will be discussed in the following sections. 1) Raise the limit for core dump file size as well as the number of file descriptors. 2) Call event_init to initialize the libevent framework. The value returned is called an event base. 3) For all potential connections, call conn_init to allocate space to store their respective contexts (located using file descriptor in global variable conns). The role of context in event driven model has already been discussed in introduction. static void conn_init(void) &#123;... if (getrlimit(RLIMIT_NOFILE, &amp;rl) == 0) &#123; max_fds = rl.rlim_max; &#125; else &#123; fprintf(stderr, \"Failed to query maximum file descriptor; \" \"falling back to maxconns\\n\"); &#125;... if ((conns = calloc(max_fds, sizeof(conn *))) == NULL) &#123; fprintf(stderr, \"Failed to allocate connection structures\\n\"); /* This is unrecoverable so bail out early. */ exit(1); &#125;&#125;conn_init@memcached.c 4) Preallocate threads and their associated resources using memcached_thread_init. 5) Setup the socket and first event listener - conn_listening. 6) Call event_base_loop to start the event loop. *) Other miscellaneous system operations, such as setting the signal handler for SIGINT and SIGTERM; setbuf stderr to NULL; dropping the root privileges of the process; and daemonizing. If those names do not ring a bell, $\\lt$$\\lt$Advanced UNIX Programming$\\gt$$\\gt$ is your friend. Threads initializationThe core data structure of multithreading mechanism is typedef struct &#123; pthread_t thread_id; /* unique ID of this thread */ struct event_base *base; /* libevent handle this thread uses */ struct event notify_event; /* listen event for notify pipe */ int notify_receive_fd; /* receiving end of notify pipe */ int notify_send_fd; /* sending end of notify pipe */...// scr: stat struct conn_queue *new_conn_queue; /* queue of new connections to handle */...// scr: cas &amp; log&#125; LIBEVENT_THREAD; LIBEVENT_THREAD@memcached.h memcached_thread_initvoid memcached_thread_init(int nthreads, struct event_base *main_base) &#123;...// scr: initialize all sorts of mutexes and condition variables threads = calloc(nthreads, sizeof(LIBEVENT_THREAD)); // scr: 1) if (! threads) &#123; perror(\"Can't allocate thread descriptors\"); exit(1); &#125; dispatcher_thread.base = main_base; // scr: ----------------&gt; 2) dispatcher_thread.thread_id = pthread_self(); // scr: ------&gt; 3) for (i = 0; i &lt; nthreads; i++) &#123; int fds[2]; if (pipe(fds)) &#123; // scr: -------------------------------&gt; 4) perror(\"Can't create notify pipe\"); exit(1); &#125; threads[i].notify_receive_fd = fds[0]; // scr: ---------&gt; 4) threads[i].notify_send_fd = fds[1]; // scr: ------------&gt; 4) setup_thread(&amp;threads[i]); // scr: ---------------------&gt; 5) /* Reserve three fds for the libevent base, and two for the pipe */ stats_state.reserved_fds += 5; &#125; /* Create threads after we've done all the libevent setup. */ for (i = 0; i &lt; nthreads; i++) &#123; create_worker(worker_libevent, &amp;threads[i]); // scr: ---&gt; 6) &#125; /* Wait for all the threads to set themselves up before returning. */ pthread_mutex_lock(&amp;init_lock); wait_for_thread_registration(nthreads); pthread_mutex_unlock(&amp;init_lock);&#125; conn_init@memcached.c 1) Allocate memory for an array of LIBEVENT_THREAD. The number of thread is num_threads Each element represents one thread. As described above, better the num_threads does not exceed the number of cores. ...settings.num_threads = 4; /* N workers */... case 't': settings.num_threads = atoi(optarg); if (settings.num_threads &lt;= 0) &#123; fprintf(stderr, \"Number of threads must be greater than 0\\n\"); return 1; &#125; /* There're other problems when you get above 64 threads. * In the future we should portably detect # of cores for the * default. */ if (settings.num_threads &gt; 64) &#123; fprintf(stderr, \"WARNING: Setting a high number of worker\" \"threads is not recommended.\\n\" \" Set this value to the number of cores in\" \" your machine or less.\\n\"); &#125; break;...num_threads@memcached.c 2) Set the event base for the dispatcher_thread which represents the main thread itself. Note that dispatcher_thread is a global variable so the reference is accessible to all the worker threads. ...static LIBEVENT_DISPATCHER_THREAD dispatcher_thread;...dispatcher_thread@thread.c 3) Set the thread id for dispatcher_thread. 4) Initialize the pipe fds for each of the worker thread. Here the notify_send_fd is used for communication between dispatcher thread and worker threads - whenever the dispatcher thread writes to notify_send_fd, an event is generated on the other side, notify_receive_fd, which is listened by worker threads. Again, $\\lt$$\\lt$Advanced UNIX Programming$\\gt$$\\gt$ gives more information about pipe. 5) The full method name is supposed to be setup_libevent_for_each_thread. Will examine this method in the next section. 6) Call pthread_create to create the actual worker threads. Will examine this method in create_worker. setup_threadstatic void setup_thread(LIBEVENT_THREAD *me) &#123; me-&gt;base = event_init(); // scr: ---------------------------&gt; 1) if (! me-&gt;base) &#123; fprintf(stderr, \"Can't allocate event base\\n\"); exit(1); &#125; /* Listen for notifications from other threads */ event_set(&amp;me-&gt;notify_event, me-&gt;notify_receive_fd, // scr: &gt; 2) EV_READ | EV_PERSIST, thread_libevent_process, me); event_base_set(me-&gt;base, &amp;me-&gt;notify_event); // scr: -------&gt; 2) if (event_add(&amp;me-&gt;notify_event, 0) == -1) &#123; // scr: -------&gt; 2) fprintf(stderr, \"Can't monitor libevent notify pipe\\n\"); exit(1); &#125; me-&gt;new_conn_queue = malloc(sizeof(struct conn_queue)); //scr:3) if (me-&gt;new_conn_queue == NULL) &#123; perror(\"Failed to allocate memory for connection queue\"); exit(EXIT_FAILURE); &#125; cq_init(me-&gt;new_conn_queue);...// scr: stat &amp; cas&#125; setup_thread@thread.c 1) Call event_init to initialize the libevent instance for the worker thread. As discussed in thread model, each worker thread runs its own drive machine. 2) Set the thread_libevent_process as the callback of events emitted from the discussed notify_receive_fd. The major function of thread_libevent_process is to link the actual drive machine to events, which we will see very soon in inter-thread communication. 3) Allocate and initialize the connection queue of the worker thread. create_workerstatic void create_worker(void *(*func)(void *), void *arg) &#123; pthread_attr_t attr; int ret; pthread_attr_init(&amp;attr); if ((ret = pthread_create(&amp;((LIBEVENT_THREAD*)arg)-&gt;thread_id, &amp;attr, func, arg)) != 0) &#123; fprintf(stderr, \"Can't create thread: %s\\n\", strerror(ret)); exit(1); &#125;&#125; create_worker@thread.c As mentioned, this method calls pthread_create to create the actual worker threads. The callback passed through is worker_libevent which essentially starts the event loop using event_base_loop, this time, on worker threads rather than dispatch thread. static void *worker_libevent(void *arg) &#123; LIBEVENT_THREAD *me = arg;... event_base_loop(me-&gt;base, 0); return NULL;&#125;worker_libevent@thread.c Socket initializationThe methods involved in socket initialization reconcile the initialization of both TCP and UDP while the following discussion covers only the TCP logic branch. And we consider portnumber_file is not set so as to focus on the critical path. ...const char *portnumber_filename = getenv(\"MEMCACHED_PORT_FILENAME\");...if (portnumber_filename != NULL) &#123; len = strlen(portnumber_filename)+4+1; temp_portnumber_filename = malloc(len); snprintf(temp_portnumber_filename, len, \"%s.lck\", portnumber_filename); portnumber_file = fopen(temp_portnumber_filename, \"a\"); if (portnumber_file == NULL) &#123; fprintf(stderr, \"Failed to open \\\"%s\\\": %s\\n\", temp_portnumber_filename, strerror(errno)); &#125;&#125;memcached.c:6029 Unlike worker threads that listen to internal (pipe) fds, dispatch thread is responsible for events generated from external socket fds (by network requests). The method that initializes sockets is server_sockets. If network interface is not indicated by inter, server_sockets is equivalent to ...settings.inter = NULL;...memcached.c:209 static int server_sockets(int port, enum network_transport transport, FILE *portnumber_file) &#123; if (settings.inter == NULL) &#123; return server_socket(settings.inter, port, transport, portnumber_file); &#125; else &#123;... &#125;&#125;server_sockets@memcached.c server_socketstatic int server_socket(const char *interface, int port, enum network_transport transport, FILE *portnumber_file) &#123; int sfd; struct linger ling = &#123;0, 0&#125;; struct addrinfo *ai; struct addrinfo *next; struct addrinfo hints = &#123; .ai_flags = AI_PASSIVE, .ai_family = AF_UNSPEC &#125;; char port_buf[NI_MAXSERV]; int error; int success = 0; int flags =1; hints.ai_socktype = IS_UDP(transport) ? SOCK_DGRAM : SOCK_STREAM; if (port == -1) &#123; port = 0; &#125; snprintf(port_buf, sizeof(port_buf), \"%d\", port); error= getaddrinfo(interface, port_buf, &amp;hints, &amp;ai); // scr: 1) if (error != 0) &#123;...// scr: error handling &#125; for (next= ai; next; next= next-&gt;ai_next) &#123; // scr: --------&gt; 2) conn *listen_conn_add; if ((sfd = new_socket(next)) == -1) &#123; // scr: ----------&gt; 3) /* getaddrinfo can return \"junk\" addresses, * we make sure at least one works before erroring. */ if (errno == EMFILE) &#123; /* ...unless we're out of fds */ perror(\"server_socket\"); exit(EX_OSERR); &#125; continue; &#125;// scr: --------------------------------------------------------&gt; 4) setsockopt(sfd, SOL_SOCKET, SO_REUSEADDR, (void *)&amp;flags, sizeof(flags)); if (IS_UDP(transport)) &#123;...// scr: not applicable &#125; else &#123; error = setsockopt(sfd, SOL_SOCKET, SO_KEEPALIVE, (void *)&amp;flags, sizeof(flags)); if (error != 0) perror(\"setsockopt\"); error = setsockopt(sfd, SOL_SOCKET, SO_LINGER, (void *)&amp;ling, sizeof(ling)); if (error != 0) perror(\"setsockopt\"); error = setsockopt(sfd, IPPROTO_TCP, TCP_NODELAY, (void *)&amp;flags, sizeof(flags)); if (error != 0) perror(\"setsockopt\"); &#125;// scr: --------------------------------------------------------&gt; 5) if (bind(sfd, next-&gt;ai_addr, next-&gt;ai_addrlen) == -1) &#123;...// scr: error handling &#125; else &#123; success++; if (!IS_UDP(transport) &amp;&amp; listen(sfd, settings.backlog) == -1) &#123;...// scr: error handling &#125; if (portnumber_file != NULL &amp;&amp;...// scr: not applicable &#125; &#125; if (IS_UDP(transport)) &#123;...// scr: not applicable &#125; else &#123; // scr: ---------------------------------------&gt; 6) if (!(listen_conn_add = conn_new(sfd, conn_listening, EV_READ | EV_PERSIST, 1, transport, main_base))) &#123; fprintf(stderr, \"failed to create listening connection\\n\"); exit(EXIT_FAILURE); &#125; listen_conn_add-&gt;next = listen_conn; // scr: -------&gt; 7) listen_conn = listen_conn_add; &#125; &#125; freeaddrinfo(ai); /* Return zero iff we detected no errors in starting up connections */ return success == 0;&#125; server_socket@memcached.c 1) Get all the available network interfaces. 2) Iterate all the network interfaces and setup the sockets with the following steps. 3) new_socket encapsulates the operation of socket creation as well as that of setting it to non-block. static int new_socket(struct addrinfo *ai) &#123; int sfd; int flags; if ((sfd = socket(ai-&gt;ai_family, ai-&gt;ai_socktype, ai-&gt;ai_protocol)) == -1) &#123; return -1; &#125; if ((flags = fcntl(sfd, F_GETFL, 0)) &lt; 0 || fcntl(sfd, F_SETFL, flags | O_NONBLOCK) &lt; 0) &#123; perror(\"setting O_NONBLOCK\"); close(sfd); return -1; &#125; return sfd;&#125;new_socket@memcached.c 4) Tweak the newly created socket fd using setsockopt, in which SO_REUSEADDR allows binding to a port in TIME_WAIT. This is useful for instantly rebooting a server on a “not fresh” TCP port; SO_KEEPALIVE sends heartbeats to detect an absent client, and to release the resource for network connection in both kernel and user space; learn more SO_LINGER enables fast close of a connection on RST; learn more TCP_NODELAY disables nagle to improve latency. learn more 5) bind and start listening to the fd. 6) conn_new initializes the context for the fd and adds it to libevent with initial state set to conn_listening and callback as event_handler. Here event_handler is another transient method leading to the drive machine on dispatcher thread. Likewise, this method will be discussed soon in inter-thread communication. conn *conn_new(const int sfd, enum conn_states init_state, const int event_flags, const int read_buffer_size, enum network_transport transport, struct event_base *base) &#123; conn *c;...// scr: initialize context conn c-&gt;state = init_state;...// scr: initialize context conn event_set(&amp;c-&gt;event, sfd, event_flags, event_handler, (void *)c); event_base_set(base, &amp;c-&gt;event); c-&gt;ev_flags = event_flags; if (event_add(&amp;c-&gt;event, 0) == -1) &#123; perror(\"event_add\"); return NULL; &#125;...// scr: stat return c;&#125;memcached.c:120 7) Add the context to the head of a global list listen_conn. ...static conn *listen_conn = NULL;...memcached.c:120 Next we briefly go through the process that handles a new connection to wrap up the Inter-thread communication event_handlerFirstly, after a TCP connection completes, the fd monitored by dispatcher thread notifies libevent, which invokes the mentioned event_handler. Next, the logic flow enters the code snippet we got in the beginning - the drive machine, with context state initialized as conn_listening in socket initialization. void event_handler(const int fd, const short which, void *arg) &#123; conn *c; c = (conn *)arg; assert(c != NULL); c-&gt;which = which; /* sanity */... drive_machine(c); /* wait for next event */ return;&#125;event_handler@memcached.c static void drive_machine(conn *c) &#123;... while (!stop) &#123; switch(c-&gt;state) &#123; case conn_listening: addrlen = sizeof(addr);... // scr: --------------------------------------&gt; 1) sfd = accept(c-&gt;sfd, (struct sockaddr *)&amp;addr, &amp;addrlen);... if (sfd == -1) &#123;...// scr: error handling &#125;... if (settings.maxconns_fast &amp;&amp; stats_state.curr_conns + stats_state.reserved_fds &gt;= settings.maxconns - 1) &#123;...// scr: error handling &#125; else &#123; // scr: ----------------------------------------&gt; 2) dispatch_conn_new(sfd, conn_new_cmd, EV_READ | EV_PERSIST, DATA_BUFFER_SIZE, c-&gt;transport); &#125; stop = true; break;... break; &#125; &#125; return;&#125; drive_machine@memcached.c At this stage, the drive machine 1) accepts the connection and derives another fd that can be read from. It 2) then calls dispatch_conn_new with the new fd and other relevant information including the next state, conn_new_cmd. dispatch_conn_newvoid dispatch_conn_new(int sfd, enum conn_states init_state, int event_flags, int read_buffer_size, enum network_transport transport) &#123; CQ_ITEM *item = cqi_new(); char buf[1]; if (item == NULL) &#123;...// scr: error handling &#125; int tid = (last_thread + 1) % settings.num_threads; // scr: &gt; 1) LIBEVENT_THREAD *thread = threads + tid; // scr: -----------&gt; 1) last_thread = tid; // scr: ---------------------------------&gt; 1) item-&gt;sfd = sfd; // scr: -----------------------------------&gt; 2) item-&gt;init_state = init_state; item-&gt;event_flags = event_flags; item-&gt;read_buffer_size = read_buffer_size; item-&gt;transport = transport; cq_push(thread-&gt;new_conn_queue, item); // scr: -------------&gt; 3) MEMCACHED_CONN_DISPATCH(sfd, thread-&gt;thread_id); buf[0] = 'c'; if (write(thread-&gt;notify_send_fd, buf, 1) != 1) &#123; // scr: --&gt; 4) perror(\"Writing to thread notify pipe\"); &#125;&#125; dispatch_conn_new@thread.c 1) Round robin the threads established in threads initialization. 2) Initializes a CQ_ITEM instance. Here CQ_ITEM is an intermediate object passed to worker threads through connection queue, so worker threads can create new context based on it. typedef struct conn_queue_item CQ_ITEM;struct conn_queue_item &#123; int sfd; enum conn_states init_state; int event_flags; int read_buffer_size; enum network_transport transport; CQ_ITEM *next;&#125;;CQ_ITEM@thread.c 3) Push CQ_ITEM to the connection queue. 4) Write to notify_send_fd with the command &#39;c&#39;. As discussed before, 4) generates an event on the other side of the pipe (on the chosen worker thread), which invokes thread_libevent_processstatic void thread_libevent_process(int fd, short which, void *arg) &#123; LIBEVENT_THREAD *me = arg; CQ_ITEM *item; char buf[1]; unsigned int timeout_fd; if (read(fd, buf, 1) != 1) &#123; // scr: -----------------------&gt; 1) if (settings.verbose &gt; 0) fprintf(stderr, \"Can't read from libevent pipe\\n\"); return; &#125; switch (buf[0]) &#123; case 'c': item = cq_pop(me-&gt;new_conn_queue); // scr: -------------&gt; 2) if (NULL != item) &#123; conn *c = conn_new(item-&gt;sfd, item-&gt;init_state, item-&gt;event_flags, item-&gt;read_buffer_size, item-&gt;transport, me-&gt;base); // scr: --------------&gt; 3) if (c == NULL) &#123;...// scr: error handling &#125; else &#123; c-&gt;thread = me; // scr: ------------------------&gt; 4) &#125; cqi_free(item); &#125; break; /* we were told to pause and report in */...// scr: not applicable break; &#125;&#125; thread_libevent_process@thread.c 1) Read the command (i.e., &#39;c&#39;) from the pipe. 2) Read the CQ_ITEM from the connection queue. 3) Call conn_new. In server_socket we know that conn_new establishes the context, this time, for the new connection, and adds the accepted fd to libevent. Here on worker thread, the callback is set to event_handler, which essentially connects the drive machine to the upcoming events on the same connection. 4) Set the thread information to the context. ReferenceWhy Threads Are A Bad Idea W. Richard Stevens. 1992. Advanced Programming in the UNIX Environment. Addison Wesley Longman Publishing Co., Inc., Redwood City, CA, USA. Resetting a TCP connection and SO_LINGER Single-process event-driven","categories":[{"name":"Memcached Source Code","slug":"Memcached-Source-Code","permalink":"https://holmeshe.me/categories/Memcached-Source-Code/"}],"tags":[{"name":"socket","slug":"socket","permalink":"https://holmeshe.me/tags/socket/"},{"name":"memcached","slug":"memcached","permalink":"https://holmeshe.me/tags/memcached/"},{"name":"event driven","slug":"event-driven","permalink":"https://holmeshe.me/tags/event-driven/"},{"name":"state machine","slug":"state-machine","permalink":"https://holmeshe.me/tags/state-machine/"},{"name":"multithreading","slug":"multithreading","permalink":"https://holmeshe.me/tags/multithreading/"}]},{"title":"Understanding The Memcached Source Code - LRU III","slug":"understanding-memcached-source-code-VI","date":"2018-12-23T22:00:00.000Z","updated":"2020-12-22T10:48:38.121Z","comments":true,"path":"understanding-memcached-source-code-VI/","link":"","permalink":"https://holmeshe.me/understanding-memcached-source-code-VI/","excerpt":"slab allocator (I, II, III) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I, II, III - this article) for entry expiration; and an event driven model (I , II , III) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. In previous posts, we have discussed different facets of an item, i.e., slab, hash map and LRU list as well as their associated (CRUD) methods, which build up the internal procedures and perform client requests after the corresponding commands are parsed by the drive machine. This time we will go through those procedures by issuing telnet commands to a Memcached instance and see how the discussed modules work together on various item operations. We will also see the whole picture of LRU lists that maintain the property of ‘least recently used’ in accordance to those operations.","text":"slab allocator (I, II, III) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I, II, III - this article) for entry expiration; and an event driven model (I , II , III) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. In previous posts, we have discussed different facets of an item, i.e., slab, hash map and LRU list as well as their associated (CRUD) methods, which build up the internal procedures and perform client requests after the corresponding commands are parsed by the drive machine. This time we will go through those procedures by issuing telnet commands to a Memcached instance and see how the discussed modules work together on various item operations. We will also see the whole picture of LRU lists that maintain the property of ‘least recently used’ in accordance to those operations. On top of standard LRU algorithm, the Memcached (1.4.28) emploies 3 lists instead of just 1, i.e., hot, warm and cold, a.k.a., Segmented LRU for each slab class. This heuristic is implemented to reduce the lock contention between item bumping (an action that moves recently accessed item to the list head) and item read. Moreover, unlike a casual implementation (such as the one I coded), Memcached does not bump item right on read action. Rather, the bumping is delayed to other operations when the resource is in short, which could reflect Memcached‘s read-first design decision. In normal use cases, let’s say, a social media, the volume of read requests are more than that of other operations combined by orders of magnitude, hence it’s a critical point that worth extensive optimizations, I suppose. We start this post by issuing an item read command to a Memcached instance. ~telnet localhost 11211Trying 127.0.0.1...Connected to localhost.Escape character is &apos;^]&apos;....// add some items&gt; get test ReadThe the normal execution of this procedure, ref1 |~Drive machine &amp; command parser |-process_get_command++ |-item_get |-assoc_find (LRU II) |-item_update |-item_unlink_q (LRU I) |-item_link_q (LRU I) process_get_commandstatic inline void process_get_command(conn *c, token_t *tokens, size_t ntokens, bool return_cas) &#123; char *key; size_t nkey; int i = 0; item *it; token_t *key_token = &amp;tokens[KEY_TOKEN]; char *suffix; assert(c != NULL); do &#123; while(key_token-&gt;length != 0) &#123; // scr: -----------------&gt; *) key = key_token-&gt;value; nkey = key_token-&gt;length; if(nkey &gt; KEY_MAX_LENGTH) &#123; out_string(c, \"CLIENT_ERROR bad command line format\"); while (i-- &gt; 0) &#123; item_remove(*(c-&gt;ilist + i)); &#125; return; &#125; it = item_get(key, nkey, c); // scr: ----------------&gt; 1) if (settings.detail_enabled) &#123; stats_prefix_record_get(key, nkey, NULL != it); &#125; if (it) &#123; if (i &gt;= c-&gt;isize) &#123; // scr: --------------------&gt; *) item **new_list = realloc(c-&gt;ilist, sizeof(item *) * c-&gt;isize * 2); if (new_list) &#123; c-&gt;isize *= 2; c-&gt;ilist = new_list; &#125; else &#123;... // scr: stat item_remove(it); break; &#125; &#125; if (return_cas) &#123;... // scr: cas &#125; else &#123; MEMCACHED_COMMAND_GET(c-&gt;sfd, ITEM_key(it), it-&gt;nkey, it-&gt;nbytes, ITEM_get_cas(it)); if (add_iov(c, \"VALUE \", 6) != 0 || // scr: ---&gt; *) add_iov(c, ITEM_key(it), it-&gt;nkey) != 0 || add_iov(c, ITEM_suffix(it), it-&gt;nsuffix + it-&gt;nbytes) != 0) &#123; item_remove(it); break; &#125; &#125;... // scr: verbose &amp; stat item_update(it); // scr: ------------------------&gt; 2) *(c-&gt;ilist + i) = it; i++; &#125; else &#123;... // scr: stat &#125; key_token++; // scr: --------------------------------&gt; *) &#125; /* * If the command string hasn't been fully processed, get the next set * of tokens. */ if(key_token-&gt;value != NULL) &#123; // scr: ------------------&gt; *) ntokens = tokenize_command(key_token-&gt;value, tokens, MAX_TOKENS); key_token = tokens; &#125; &#125; while(key_token-&gt;value != NULL); c-&gt;icurr = c-&gt;ilist; c-&gt;ileft = i;... // scr: cas &amp; verbose if (key_token-&gt;value != NULL || add_iov(c, \"END\\r\\n\", 5) != 0 || (IS_UDP(c-&gt;transport) &amp;&amp; build_udp_headers(c) != 0)) &#123; out_of_memory(c, \"SERVER_ERROR out of memory writing get response\"); &#125; else &#123; // scr: ----------------------------------------------&gt; *) conn_set_state(c, conn_mwrite); c-&gt;msgcurr = 0; &#125;&#125; process_get_command@memcached.c The only relevant step here are 1) item_get and 2) item_update. Steps marked as *) are mostly command parsing and I/O which will be discussed in later posts when we examine event driven mechanism. do_item_getLike other methods discussed before, item_get is a thread-safe wrapper of do_item_get. item *item_get(const char *key, const size_t nkey, conn *c) &#123; item *it; uint32_t hv; hv = hash(key, nkey); item_lock(hv); it = do_item_get(key, nkey, hv, c); item_unlock(hv); return it;&#125;item_get@thread.c item *do_item_get(const char *key, const size_t nkey, const uint32_t hv, conn *c) &#123; item *it = assoc_find(key, nkey, hv); // scr: -----------------&gt; 1) if (it != NULL) &#123; refcount_incr(&amp;it-&gt;refcount); // scr: ---------------------&gt; 2)...// scr: comments &#125; int was_found = 0;...// scr: verbose if (it != NULL) &#123; was_found = 1; if (item_is_flushed(it)) &#123;...// scr: item flush// scr: -----------------------------------------------------------&gt; 3) &#125; else if (it-&gt;exptime != 0 &amp;&amp; it-&gt;exptime &lt;= current_time) &#123; do_item_unlink(it, hv); do_item_remove(it); it = NULL;...// scr: stat &amp; verbose was_found = 3; &#125; else &#123; // scr: ------------------------------------------&gt; 4) it-&gt;it_flags |= ITEM_FETCHED|ITEM_ACTIVE; DEBUG_REFCNT(it, '+'); &#125; &#125;...// scr: verbose return it;&#125; do_item_get@items.c 1) Use the discussed assoc_find to locate the item using the hash key. 2) Increase the discussed reference count. 3) If the item has expired, remove it. Note that do_item_unlink decreases the reference count held by the last step, and do_item_remove actually removes the item. These two methods will be discussed soon in item delete. 4) Simply mark the item as ITEM_ACTIVE rather than perform item bumping which is offloaded to other operations associated procedures. This is part of the heuristic discussed in the beginning. do_item_updateitem_update is a thread-safe wrapper of do_item_update.void item_update(item *item) &#123; uint32_t hv; hv = hash(ITEM_key(item), item-&gt;nkey); item_lock(hv); do_item_update(item); item_unlock(hv);&#125; item_update@thread.c void do_item_update(item *it) &#123; MEMCACHED_ITEM_UPDATE(ITEM_key(it), it-&gt;nkey, it-&gt;nbytes); if (it-&gt;time &lt; current_time - ITEM_UPDATE_INTERVAL) &#123; assert((it-&gt;it_flags &amp; ITEM_SLABBED) == 0); if ((it-&gt;it_flags &amp; ITEM_LINKED) != 0) &#123; it-&gt;time = current_time; // scr: ----------------------&gt; 1) if (!settings.lru_maintainer_thread) &#123; item_unlink_q(it); item_link_q(it); &#125; &#125; &#125;&#125; do_item_update@items.c 1) The only line effective in this method is to set the access time for the item in (passively) an interval of 60 seconds. lru_maintainer_thread is set to true by command line argument modern so the operations inside if (!settings.lru_maintainer_thread) is not applicable. #define ITEM_UPDATE_INTERVAL 60memcached.h:73 case MODERN:... start_lru_maintainer = true; break;memcached.c:5828 Next, &gt; delete test DeleteCall stack in normal execution, ref1 |~Drive machine &amp; command parser |-process_delete_command++ |-do_item_get |-do_item_unlink |-assoc_delete (LRU II) |-item_unlink_q (LRU I)-- |-do_item_remove-- |-do_item_remove |-do_item_free |-do_slabs_free (Slab II) process_delete_commandstatic void process_delete_command(conn *c, token_t *tokens, const size_t ntokens) &#123; char *key; size_t nkey; item *it;...// scr: sanity check key = tokens[KEY_TOKEN].value; nkey = tokens[KEY_TOKEN].length;...// scr: sanity check &amp; stat it = item_get(key, nkey, c); // scr: -------------------------&gt; 1) if (it) &#123; MEMCACHED_COMMAND_DELETE(c-&gt;sfd, ITEM_key(it), it-&gt;nkey);...// scr: stat item_unlink(it); // scr: ---------------------------------&gt; 2) item_remove(it); /* release our reference */ out_string(c, \"DELETED\"); &#125; else &#123;...// scr: stat out_string(c, \"NOT_FOUND\"); &#125;&#125; process_delete_command@memcached.c 1) Get the item using item_get discussed in last section. Note that the reference count is increased in item_get. 2) Delete it. do_item_unlinkitem_unlink is a thread safe wrapper of do_item_unlink.void item_unlink(item *item) &#123; uint32_t hv; hv = hash(ITEM_key(item), item-&gt;nkey); item_lock(hv); do_item_unlink(item, hv); item_unlock(hv);&#125; item_unlink@thread.c void do_item_unlink(item *it, const uint32_t hv) &#123;... if ((it-&gt;it_flags &amp; ITEM_LINKED) != 0) &#123; it-&gt;it_flags &amp;= ~ITEM_LINKED;...// scr: stat assoc_delete(ITEM_key(it), it-&gt;nkey, hv); // scr: --------&gt; 1) item_unlink_q(it); // scr: -------------------------------&gt; 2) do_item_remove(it); // scr: ------------------------------&gt; 3) &#125;&#125; do_item_unlink@items.c 1) As discussed in last post, assoc_delete removes the item from the hash map; and 2) item_unlink_q removes the item from the LRU list that the item belongs to. 3) This time do_item_remove simply decreases the reference count. The item will be removed when do_item_remove is called the second time from do_item_removeitem_remove is a thread safe wrapper of do_item_remove.void item_remove(item *item) &#123; uint32_t hv; hv = hash(ITEM_key(item), item-&gt;nkey); item_lock(hv); do_item_remove(item); item_unlock(hv);&#125; item_remove@thread.c void do_item_remove(item *it) &#123;...// scr: sanity check if (refcount_decr(&amp;it-&gt;refcount) == 0) &#123; // scr: --------&gt; 1) item_free(it); &#125;&#125;...void item_free(item *it) &#123; size_t ntotal = ITEM_ntotal(it); unsigned int clsid;...// scr: sanity check /* so slab size changer can tell later if item is already free or not */ clsid = ITEM_clsid(it); // scr: -------------------------&gt; 2) DEBUG_REFCNT(it, 'F'); slabs_free(it, ntotal, clsid); // scr: ------------------&gt; 3)&#125; do_item_remove@items.c 1) Decrease the reference count, if it reaches 0, goto 2) and free the item. 2) Use ITEM_clsid to get the slab class the item belongs. This macro removes the list type from slabs_clsid. #define ITEM_clsid(item) ((item)-&gt;slabs_clsid &amp; ~(3&lt;&lt;6))memcached.h:116 3) Call slabs_free to release the memory to slab subsystem. CreateThe Item creating procedure is divided into several logic fragments by the mentioned drive machine, 1) creating an empty item object with the key and other meta data sent through; 2) read the value (from the socket) and fill the item object with it; and 3) link the item object. The workflow controller - drive machine will be discussed in the next post. Now we send an add command to the Memcached instance. &gt; add test 0 60 11 (\\r\\n)&gt; hello world Creating an empty item objectAfter the first line of the above command &gt; add test 0 60 11 (\\r\\n) the procedure described in this section starts, the call stack of the hot path is, ref1 |~Drive machine &amp; command parser |-process_update_command |-do_item_alloc |-slabs_clsid (Slab III)++ |-do_slabs_alloc (Slab III) |-lru_pull_tail (on hot list) |-do_item_update_nolock (same to do_item_update) |-do_item_remove |-item_link_q (LRU I) |-do_item_remove |-lru_pull_tail (on warm list) |-same as hot list |-lru_pull_tail (on cold list) |-do_item_unlink_nolock (same to do_item_unlink LRU I) We start from process_update_commandstatic void process_update_command(conn *c, token_t *tokens, const size_t ntokens, int comm, bool handle_cas) &#123; char *key; size_t nkey; unsigned int flags; int32_t exptime_int = 0; time_t exptime; int vlen; uint64_t req_cas_id=0; item *it;...// scr: irrelevant code &amp; sanity checks key = tokens[KEY_TOKEN].value; // scr: ----------------------&gt; 1) nkey = tokens[KEY_TOKEN].length; if (! (safe_strtoul(tokens[2].value, (uint32_t *)&amp;flags) &amp;&amp; safe_strtol(tokens[3].value, &amp;exptime_int) &amp;&amp; safe_strtol(tokens[4].value, (int32_t *)&amp;vlen))) &#123; out_string(c, \"CLIENT_ERROR bad command line format\"); return; &#125; /* Ubuntu 8.04 breaks when I pass exptime to safe_strtol */ exptime = exptime_int;...// scr: cas &amp; sanity checks vlen += 2; // scr: ------------------------------------------&gt; 2)...// scr: stat it = item_alloc(key, nkey, flags, realtime(exptime), vlen); // 3)...// scr: cas &amp; error handling c-&gt;item = it; // scr: ---------------------------------------&gt; 4) c-&gt;ritem = ITEM_data(it); c-&gt;rlbytes = it-&gt;nbytes; c-&gt;cmd = comm; conn_set_state(c, conn_nread);&#125; process_update_command@memcached.c 1) Set the key (i.e., test), as well as the meta data (i.e., flags, 0; exptime, 60;vlen,11`), to local variables. 2) Increase vlen by 2, to populate the \\n\\r in addition to the key string. 3) Call item_alloc to allocate the memory (from slab) for the item. 4) After item_alloc is called, set the properties of conn. Here ritem points to the data portion of an item chunk; and rlbytes is set to vlen. These two fields will be used to populate the data portion with the content, i.e., hello world, in the next post. do_item_allocUnlike other methods we have discussed, item_alloc is a wrapper of do_item_alloc without adding any locks. I would assume this wrapper is added simply for consistent code style. item *item_alloc(char *key, size_t nkey, int flags, rel_time_t exptime, int nbytes) &#123; item *it; /* do_item_alloc handles its own locks */ it = do_item_alloc(key, nkey, flags, exptime, nbytes, 0); return it;&#125;main@memcached.c:5849 item *do_item_alloc(char *key, const size_t nkey, const unsigned int flags, const rel_time_t exptime, const int nbytes, const uint32_t cur_hv) &#123; int i; uint8_t nsuffix; item *it = NULL; char suffix[40]; unsigned int total_chunks; // scr: -----------------------------&gt; 1) size_t ntotal = item_make_header(nkey + 1, flags, nbytes, suffix, &amp;nsuffix);... // scr: cas unsigned int id = slabs_clsid(ntotal); // scr: -----------------------&gt; 2) if (id == 0) return 0; /* If no memory is available, attempt a direct LRU juggle/eviction */ /* This is a race in order to simplify lru_pull_tail; in cases where * locked items are on the tail, you want them to fall out and cause * occasional OOM's, rather than internally work around them. * This also gives one fewer code path for slab alloc/free */ for (i = 0; i &lt; 5; i++) &#123; /* Try to reclaim memory first */... // scr: legacy, no lru_maintainer_thread it = slabs_alloc(ntotal, id, &amp;total_chunks, 0); // scr: ----------&gt; 3)... // scr: no-expire setting if (it == NULL) &#123; if (settings.lru_maintainer_thread) &#123; // scr: ----------------&gt; 4) lru_pull_tail(id, HOT_LRU, total_chunks, false, cur_hv); lru_pull_tail(id, WARM_LRU, total_chunks, false, cur_hv); lru_pull_tail(id, COLD_LRU, total_chunks, true, cur_hv); &#125; else &#123;... // scr: legacy, no lru_maintainer_thread &#125; &#125; else &#123; break; &#125; &#125;... // scr: stat &amp; sanity check /* Refcount is seeded to 1 by slabs_alloc() */ it-&gt;next = it-&gt;prev = it-&gt;h_next = 0; // scr: ------------------------&gt; 5) /* Items are initially loaded into the HOT_LRU. This is '0' but I want at * least a note here. Compiler (hopefully?) optimizes this out. */ if (settings.lru_maintainer_thread) &#123;... // scr: no expire setting &#125; else &#123; id |= HOT_LRU; // scr: ---------------------------------------&gt; 6) &#125; &#125; else &#123;... // scr: legacy, no lru_maintainer_thread &#125; it-&gt;slabs_clsid = id; // scr: ----------------------------------------&gt; 7) DEBUG_REFCNT(it, '*'); it-&gt;it_flags = settings.use_cas ? ITEM_CAS : 0; it-&gt;nkey = nkey; it-&gt;nbytes = nbytes; memcpy(ITEM_key(it), key, nkey); it-&gt;exptime = exptime; memcpy(ITEM_suffix(it), suffix, (size_t)nsuffix); it-&gt;nsuffix = nsuffix; return it;&#125; do_item_alloc@items.c 1) item_make_header initializes suffix portion of the item chunk using the meta data (flags) and key. static size_t item_make_header(const uint8_t nkey, const unsigned int flags, const int nbytes, char *suffix, uint8_t *nsuffix) &#123; /* suffix is defined at 40 chars elsewhere.. */ *nsuffix = (uint8_t) snprintf(suffix, 40, \" %u %d\\r\\n\", flags, nbytes - 2); return sizeof(item) + nkey + *nsuffix + nbytes;&#125;item_make_header@items.c 2), 3) are discussed in detail in Slab III. To recap, slabs_clsid select the most optimal slab class and slab_alloc allocates one item chunk from slab sub-system. 4) If slab_alloc fails, try to release some memory using lru_pull_tail and retry the allocation for at most 5 times. lru_pull_tail is the focus of the next section. 5) Initialize the pointers of LRU list and hash collision list. 6) Set the list type (HOT_LRU) to the slabs_clsid, which indicates that this item belongs to the “HOT” LRU list of its respective slab class. 7) Initialize other fields of item. lru_pull_tailstatic int lru_pull_tail(const int orig_id, const int cur_lru, const unsigned int total_chunks, const bool do_evict, const uint32_t cur_hv) &#123; item *it = NULL; int id = orig_id; // scr: ---------------------------------------&gt; p) int removed = 0; if (id == 0) return 0; int tries = 5; item *search; item *next_it; void *hold_lock = NULL; unsigned int move_to_lru = 0; uint64_t limit; id |= cur_lru; // scr: ------------------------------------------&gt; p) pthread_mutex_lock(&amp;lru_locks[id]); search = tails[id]; // scr: -------------------------------------&gt; p) /* We walk up *only* for locked items, and if bottom is expired. */ for (; tries &gt; 0 &amp;&amp; search != NULL; tries--, search=next_it) &#123;//s: p) /* we might relink search mid-loop, so search-&gt;prev isn't reliable */ next_it = search-&gt;prev; // scr: -----------------------------&gt; p)...// scr: irrelevant code here uint32_t hv = hash(ITEM_key(search), search-&gt;nkey); /* Attempt to hash item lock the \"search\" item. If locked, no * other callers can incr the refcount. Also skip ourselves. */ if (hv == cur_hv || (hold_lock = item_trylock(hv)) == NULL) continue; /* Now see if the item is refcount locked */ if (refcount_incr(&amp;search-&gt;refcount) != 2) &#123; // scr: --------&gt; s) /* Note pathological case with ref'ed items in tail. * Can still unlink the item, but it won't be reusable yet */ itemstats[id].lrutail_reflocked++; /* In case of refcount leaks, enable for quick workaround. */ /* WARNING: This can cause terrible corruption */ if (settings.tail_repair_time &amp;&amp; search-&gt;time + settings.tail_repair_time &lt; current_time) &#123; itemstats[id].tailrepairs++; search-&gt;refcount = 1; /* This will call item_remove -&gt; item_free since refcnt is 1 */ do_item_unlink_nolock(search, hv); item_trylock_unlock(hold_lock); continue; &#125; &#125; /* Expired or flushed */ // scr: ---------------------------&gt; e1) if ((search-&gt;exptime != 0 &amp;&amp; search-&gt;exptime &lt; current_time) || item_is_flushed(search)) &#123;...// scr: stat /* refcnt 2 -&gt; 1 */ do_item_unlink_nolock(search, hv); /* refcnt 1 -&gt; 0 -&gt; item_free */ do_item_remove(search); item_trylock_unlock(hold_lock); removed++; /* If all we're finding are expired, can keep going */ continue; &#125; /* If we're HOT_LRU or WARM_LRU and over size limit, send to COLD_LRU. * If we're COLD_LRU, send to WARM_LRU unless we need to evict */ switch (cur_lru) &#123; case HOT_LRU: // scr: -----------------------------------&gt; 1) limit = total_chunks * settings.hot_lru_pct / 100; case WARM_LRU: limit = total_chunks * settings.warm_lru_pct / 100; // 1) if (sizes[id] &gt; limit) &#123; // scr: --------------------&gt; 2)...// scr: stat move_to_lru = COLD_LRU; do_item_unlink_q(search); it = search; removed++; break; &#125; else if ((search-&gt;it_flags &amp; ITEM_ACTIVE) != 0) &#123; //e3)...// scr: stat search-&gt;it_flags &amp;= ~ITEM_ACTIVE; do_item_update_nolock(search); do_item_remove(search); item_trylock_unlock(hold_lock); &#125; else &#123; // scr: ------------------------------------&gt; 3) /* Don't want to move to COLD, not active, bail out */ it = search; &#125; break; case COLD_LRU: it = search; /* No matter what, we're stopping */ if (do_evict) &#123; if (settings.evict_to_free == 0) &#123;...// scr: not applied here &#125;...// scr: stat LOGGER_LOG(NULL, LOG_EVICTIONS, LOGGER_EVICTION, search); do_item_unlink_nolock(search, hv); // scr: ------&gt; 4) removed++; if (settings.slab_automove == 2) &#123;...// scr: not applied here &#125; &#125; else if...// scr: not applied here &#125; break; &#125; if (it != NULL) break; &#125; pthread_mutex_unlock(&amp;lru_locks[id]); if (it != NULL) &#123; // scr: --------------------------------------&gt; e2) if (move_to_lru) &#123; it-&gt;slabs_clsid = ITEM_clsid(it); it-&gt;slabs_clsid |= move_to_lru; item_link_q(it); &#125; do_item_remove(it); item_trylock_unlock(hold_lock); &#125; return removed;&#125; lru_pull_tail@items.c Method start &amp; end p) This method starts by selecting the tail element of the designated LRU list using the slab class id and the list type, assuming that the element can be a release candidate. And it iterates over (at most 5 entries) the list in reverse order to find a entry in case that elements near the tail are recently accessed. s) For each item selected, increase its reference count. In normal situation, the original value of reference count should be 1 (as you will see in the last step of the create operation). Hence a != 2 value after the increment indicates an exception that needs to be corrected. Note that the reference count is now 2 so it is required to decrease at least one time (back to 1) when the processing of the current item is done (e1, e2 or e3 is reached). e1) Remove the item directly when an expiration is detected. Here the do_item_unlink_nolock is exactly the same as the discussed do_item_unlink (I think the code is duplicated to emphasize that this method is not thread-safe), and it follows the same “unlink and remove” routine as in item delete. e2) When a candidate is found, we might need to relocate it to another list (when move_to_lru is set in the switch case) by calling item_link_q. And we do need to call do_item_remove to reduce the reference count back to 1. The decision is made by the steps discussed bellow. e3) If an item is recently accessed, reset the ITEM_ACTIVE flag; bump it to the head of the list; decrease its reference count and iterate to the next one (maximum 5 times). Remember that the flag ITEM_ACTIVE is set by item_get, and here is the place where the item gets bumped. Hot &amp; warm 1) The only difference of HOT_LRU and WARM_LRU is the threshold (limit) which are indicated by their respective configurations hot_lru_pct and warm_lru_pct. settings.hot_lru_pct = 32;...case HOT_LRU_PCT: if (subopts_value == NULL) &#123; fprintf(stderr, \"Missing hot_lru_pct argument\\n\"); return 1; &#125;; settings.hot_lru_pct = atoi(subopts_value); if (settings.hot_lru_pct &lt; 1 || settings.hot_lru_pct &gt;= 80) &#123; fprintf(stderr, \"hot_lru_pct must be &gt; 1 and &lt; 80\\n\"); return 1; &#125; break;...hot_lru_pct@memcached.c ...settings.warm_lru_pct = 32;...case WARM_LRU_PCT: if (subopts_value == NULL) &#123; fprintf(stderr, \"Missing warm_lru_pct argument\\n\"); return 1; &#125;; settings.warm_lru_pct = atoi(subopts_value); if (settings.warm_lru_pct &lt; 1 || settings.warm_lru_pct &gt;= 80) &#123; fprintf(stderr, \"warm_lru_pct must be &gt; 1 and &lt; 80\\n\"); return 1; &#125; break;...warm_lru_pct@memcached.c 2) If the threshold of HOT_LRU or WARM_LRU is reached, remove the item from the current list using do_item_unlink_q, and goto e2). Therefore, e2) is responsible to relink it to the COLD_LRU and decrease the reference count. 3) If the current item is not “active”, and the threshold is not reached, finish this method without any item relocation, nor release. Cold 4) If there are any item in the list, evict it directly with the discussed do_item_unlink_nolock to free up its resource, and goto e1). Note that the default values of evict_to_free and slab_automove are set to values that disable their respective if blocks. ...settings.evict_to_free = 1;...memcached.c:215 ...case MODERN: /* Modernized defaults. Need to add equivalent no_* flags * before making truly default. */... settings.slab_automove = 1;...memcached.c:5824 Now we input the second line of the command &gt; hello world and trigger the following procedures. Populate the item with contentAs mentioned in process_update_command, the content we input is populated to conn.item using conn.ritem and conn.rlbytes. This step is handled by drive machine which will be discussed in detail in the next post. ...res = read(c-&gt;sfd, c-&gt;ritem, c-&gt;rlbytes);...memcached.c:4421 Now we consider conn.item is filled with all relevant information, hence the next and final step is to Link the itemthe call stack of this step is ref2 |~Drive machine &amp; command parser |-complete_nread |-complete_nread_ascii |-do_store_item |=do_item_link (LRU I)-- |-do_item_remove complete_nread checks the protocol in use and moves directly tostatic void complete_nread(conn *c) &#123; assert(c != NULL); assert(c-&gt;protocol == ascii_prot || c-&gt;protocol == binary_prot); if (c-&gt;protocol == ascii_prot) &#123; complete_nread_ascii(c); &#125; else if (c-&gt;protocol == binary_prot) &#123; complete_nread_binary(c); &#125;&#125; complete_nread@memcached.c complete_nread_asciistatic void complete_nread_ascii(conn *c) &#123; assert(c != NULL); item *it = c-&gt;item; int comm = c-&gt;cmd; enum store_item_type ret;...// scr: stat if (strncmp(ITEM_data(it) + it-&gt;nbytes - 2, \"\\r\\n\", 2) != 0) &#123; out_string(c, \"CLIENT_ERROR bad data chunk\"); // scr: -----&gt; 1) &#125; else &#123; ret = store_item(it, comm, c); switch (ret) &#123; case STORED: out_string(c, \"STORED\"); break; case EXISTS: out_string(c, \"EXISTS\"); break; case NOT_FOUND: out_string(c, \"NOT_FOUND\"); break; case NOT_STORED: out_string(c, \"NOT_STORED\"); break; default: out_string(c, \"SERVER_ERROR Unhandled storage type.\"); &#125; &#125; // scr: -------------------------------------------------------&gt; 2) item_remove(c-&gt;item); /* release the c-&gt;item reference */ c-&gt;item = 0;&#125; do_store_item@items.c 1) Call store_item to link the item to the LRU list and hash map. 2) The reference count is set to 1 by do_slab_alloc in do_item_alloc and increased by do_item_link in do_store_item. So reduce it to the normal value, 1, with item_remove. The methods of those have all been discussed in detail. do_store_itemenum store_item_type do_store_item(item *it, int comm, conn *c, const uint32_t hv) &#123; char *key = ITEM_key(it); item *old_it = do_item_get(key, it-&gt;nkey, hv, c); // scr: ------------&gt; 1) enum store_item_type stored = NOT_STORED; item *new_it = NULL; uint32_t flags; if (old_it != NULL &amp;&amp; comm == NREAD_ADD) &#123;... // scr: update logic &#125; else if (!old_it &amp;&amp; (comm == NREAD_REPLACE || comm == NREAD_APPEND || comm == NREAD_PREPEND)) &#123; /* replace only replaces an existing value; don't store */ &#125; else if (comm == NREAD_CAS) &#123;... // scr: cas &#125; else &#123;... if (comm == NREAD_APPEND || comm == NREAD_PREPEND) &#123;... // scr: comm is NREAD_ADD (1) &#125; if (stored == NOT_STORED &amp;&amp; failed_alloc == 0) &#123; if (old_it != NULL) item_replace(old_it, it, hv); else do_item_link(it, hv); // scr: ----------------------------&gt; 2)... stored = STORED; &#125; &#125;... // scr: irrelevant code// scr: cas LOGGER_LOG(c-&gt;thread-&gt;l, LOG_MUTATIONS, LOGGER_ITEM_STORE, NULL, stored, comm, ITEM_key(it), it-&gt;nkey); return stored;&#125; do_store_item@items.c 1) The newly created item exists only in the slab subsystem, hence do_item_get returns null as there is no such record in hash map yet. 2) So in the context of item creation, do_store_item is essentially the same as do_item_link. ReferenceReplacing the cache replacement algorithm in memcached Examples of Memcached telnet commands Memcached telnet command summary","categories":[{"name":"Memcached Source Code","slug":"Memcached-Source-Code","permalink":"https://holmeshe.me/categories/Memcached-Source-Code/"}],"tags":[{"name":"memcached","slug":"memcached","permalink":"https://holmeshe.me/tags/memcached/"},{"name":"cache","slug":"cache","permalink":"https://holmeshe.me/tags/cache/"},{"name":"LRU","slug":"LRU","permalink":"https://holmeshe.me/tags/LRU/"}]},{"title":"Understanding The Memcached Source Code - LRU II","slug":"understanding-memcached-source-code-V","date":"2018-12-17T22:00:00.000Z","updated":"2020-12-22T10:48:38.121Z","comments":true,"path":"understanding-memcached-source-code-V/","link":"","permalink":"https://holmeshe.me/understanding-memcached-source-code-V/","excerpt":"slab allocator (I, II, III) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I , II - this article , III) for entry expiration; and an event driven model (I , II , III) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. More often than not, the LRU algorithm is combined with a hash map, and is referred to as a LRU cache. In a LRU-cache, the hash map enables fast accessing of cached objects; and LRU avoids the cache to grow infinitely by marking expired, or so called, least recently used objects. This time we examine the memcached‘s implementation of hash map.","text":"slab allocator (I, II, III) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I , II - this article , III) for entry expiration; and an event driven model (I , II , III) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. More often than not, the LRU algorithm is combined with a hash map, and is referred to as a LRU cache. In a LRU-cache, the hash map enables fast accessing of cached objects; and LRU avoids the cache to grow infinitely by marking expired, or so called, least recently used objects. This time we examine the memcached‘s implementation of hash map. Overview (textbook overlapped, skip)Hash map is basically a fixed-sized array that indexes values with integers hashed from keys. In hash map an array entry is referred to as a bucket. If the hash value exceeds the number of buckets (i.e., array size), it rolls over using ‘mod’ (%). Collision occurs when more than two keys result in the same hash value or different hash values roll over to the same bucket, then a *linked list is formed on the bucket in collision. Collision slows down lookups speed for the sequential access of linked list, hence it is required to increase the bucket number, and to rehash entries using the new bucket number before the performance goes too bad. This process will be discussed soon. Module initializationThe first relevant method is hash_initwhich simply determines the hash algorithm type. int hash_init(enum hashfunc_type type) &#123; switch(type) &#123; case JENKINS_HASH: hash = jenkins_hash; settings.hash_algorithm = \"jenkins\"; break; case MURMUR3_HASH: hash = MurmurHash3_x86_32; settings.hash_algorithm = \"murmur3\"; break; default: return -1; &#125; return 0;&#125; hash_init@hash.c This method is called from here as one of the init steps before the logic enters the main event loop. ... if (hash_init(hash_type) != 0) &#123; fprintf(stderr, \"Failed to initialize hash_algorithm!\\n\"); exit(EX_USAGE); &#125;...main@memcached.c:5849 The parameter hash_type is set to MURMUR3_HASH by the mentioned command-line argument modern. ... case MODERN: /* Modernized defaults. Need to add equivalent no_* flags * before making truly default. */... hash_type = MURMUR3_HASH;... break;...main@memcached.c:5849 The second method assoc_initallocates the fixed sized array mentioned in the beginning. void assoc_init(const int hashtable_init) &#123; if (hashtable_init) &#123; hashpower = hashtable_init; &#125; primary_hashtable = calloc(hashsize(hashpower), sizeof(void *)); if (! primary_hashtable) &#123; fprintf(stderr, \"Failed to init hashtable.\\n\"); exit(EXIT_FAILURE); &#125;...// scr: stat&#125; hash_init@hash.c This method is called in a similar location as hash_init as part of the system bootstrap process. ... assoc_init(settings.hashpower_init);...main@memcached.c:5976 And the actual initial size is determined by the command-line argument hashpower. ...case HASHPOWER_INIT: if (subopts_value == NULL) &#123; fprintf(stderr, \"Missing numeric argument for hashpower\\n\"); return 1; &#125; settings.hashpower_init = atoi(subopts_value); if (settings.hashpower_init &lt; 12) &#123; fprintf(stderr, \"Initial hashtable multiplier of %d is too low\\n\", settings.hashpower_init); return 1; &#125; else if (settings.hashpower_init &gt; 64) &#123; fprintf(stderr, \"Initial hashtable multiplier of %d is too high\\n\" \"Choose a value based on \\\"STAT hash_power_level\\\" from a running instance\\n\", settings.hashpower_init); return 1; &#125; break;...main@memcached.c:5677 As said before, the array can be replaced with a newly allocated larger one if the performance drops due to excessive collision. Next we discuss the process of Scale up &amp; entry migrationIn memcached, the threshold is 1.5, meaning, if the items number exceeds 1.5 * bucket number, the mentioned expanding process starts. ... if (! expanding &amp;&amp; hash_items &gt; (hashsize(hashpower) * 3) / 2) &#123; assoc_start_expand(); &#125;... assoc_insert@assoc.c:173 The assoc_start_expand simply set a flag (i.e., do_run_maintenance_thread), and send a signal to awake a maintenance thread that does the actual job. static void assoc_start_expand(void) &#123; if (started_expanding) return; started_expanding = true; pthread_cond_signal(&amp;maintenance_cond);&#125; assoc_insert@assoc.c:173 Maintenance thread main loopstatic void *assoc_maintenance_thread(void *arg) &#123; mutex_lock(&amp;maintenance_lock); while (do_run_maintenance_thread/* scr: the flag*/) &#123; int ii = 0; /* There is only one expansion thread, so no need to global lock. */ for (ii = 0; ii &lt; hash_bulk_move &amp;&amp; expanding; ++ii) &#123; // scr: ----&gt; 2) item *it, *next; int bucket; void *item_lock = NULL; /* bucket = hv &amp; hashmask(hashpower) =&gt;the bucket of hash table * is the lowest N bits of the hv, and the bucket of item_locks is * also the lowest M bits of hv, and N is greater than M. * So we can process expanding with only one item_lock. cool! */ if ((item_lock = item_trylock(expand_bucket))) &#123; // scr: --------&gt; 3) for (it = old_hashtable[expand_bucket]; NULL != it; it = next) &#123; next = it-&gt;h_next; // scr: ----------------------------------&gt; 4) bucket = hash(ITEM_key(it), it-&gt;nkey) &amp; hashmask(hashpower); it-&gt;h_next = primary_hashtable[bucket]; primary_hashtable[bucket] = it; &#125; old_hashtable[expand_bucket] = NULL; // scr: ----------------&gt; 4.1) expand_bucket++; // scr: --------------------------------------&gt; 5) if (expand_bucket == hashsize(hashpower - 1)) &#123; // scr: -------&gt; 6) expanding = false; free(old_hashtable);... // scr: ---------------------------------------------------&gt; stat &amp; log &#125; &#125; else &#123; usleep(10*1000); // scr: ------------------------------------&gt; 3.1) &#125; if (item_lock) &#123; // scr: --------------------------------------&gt; 3.2) item_trylock_unlock(item_lock); item_lock = NULL; &#125; &#125; if (!expanding) &#123; /* We are done expanding.. just wait for next invocation */ started_expanding = false; pthread_cond_wait(&amp;maintenance_cond, &amp;maintenance_lock); // scr: &gt; 0) /* assoc_expand() swaps out the hash table entirely, so we need * all threads to not hold any references related to the hash * table while this happens. * This is instead of a more complex, possibly slower algorithm to * allow dynamic hash table expansion without causing significant * wait times. */ pause_threads(PAUSE_ALL_THREADS); assoc_expand(); // scr: -----------------------------------------&gt; 1) pause_threads(RESUME_ALL_THREADS); &#125; &#125; return NULL;&#125; assoc_maintenance_thread@assoc.c 0) This is where the thread waits up from sleep and start working, and goes to sleep when there is nothing to be done. 1) assoc_expand allocates the resource for the new hash map which is meant to replace the old one initialized from here. /* grows the hashtable to the next power of 2. */static void assoc_expand(void) &#123; old_hashtable = primary_hashtable; primary_hashtable = calloc(hashsize(hashpower + 1), sizeof(void *)); if (primary_hashtable) &#123;... // scr: log hashpower++; expanding = true; expand_bucket = 0;... // scr: stat &#125; else &#123; primary_hashtable = old_hashtable; /* Bad news, but we can keep running. */ &#125;&#125;assoc_expand@assoc.c 2) Only migrate a certain number of items in one batch. hash_bulk_move avoids the thread hanging around too long when stop_assoc_maintenance_thread is called. In contrast to the discussed assoc_start_expand, stop_assoc_maintenance_thread reset the flag do_run_maintenance_thread and send the signal to wake up the thread to exit. #define DEFAULT_HASH_BULK_MOVE 1int hash_bulk_move = DEFAULT_HASH_BULK_MOVE;assoc.c:207... char *env = getenv(\"MEMCACHED_HASH_BULK_MOVE\"); if (env != NULL) &#123; hash_bulk_move = atoi(env); if (hash_bulk_move == 0) &#123; hash_bulk_move = DEFAULT_HASH_BULK_MOVE; &#125; &#125;...start_assoc_maintenance_thread@assoc.c:281 void stop_assoc_maintenance_thread() &#123; mutex_lock(&amp;maintenance_lock); do_run_maintenance_thread = 0; pthread_cond_signal(&amp;maintenance_cond); mutex_unlock(&amp;maintenance_lock); /* Wait for the maintenance thread to stop */ pthread_join(maintenance_tid, NULL);&#125;stop_assoc_maintenance_thread@assoc.c 3) (The “item lock” actually works on the whole bucket hence I will call it bucket lock instead) Use low priority item_trylock (i.e., pthread_mutex_trylock) to access the bucket lock; 3.1) sleep for 10 sec when the the item is not available; and 3.2) release the lock using item_trylock_unlock when the migration (of this bucket) completes. void *item_trylock(uint32_t hv) &#123; pthread_mutex_t *lock = &amp;item_locks[hv &amp; hashmask(item_lock_hashpower)]; if (pthread_mutex_trylock(lock) == 0) &#123; return lock; &#125; return NULL;&#125;item_trylock@thread.c void item_trylock_unlock(void *lock) &#123; mutex_unlock((pthread_mutex_t *) lock);&#125;item_trylock@thread.c 4) Rehash all the items in the bucket, and migrate them to the new hash map. 5) Move on to the next bucket. 6) Last bucket reached -&gt; go to 0) Maintenance thread startint start_assoc_maintenance_thread() &#123; int ret; char *env = getenv(\"MEMCACHED_HASH_BULK_MOVE\"); if (env != NULL) &#123; hash_bulk_move = atoi(env); if (hash_bulk_move == 0) &#123; hash_bulk_move = DEFAULT_HASH_BULK_MOVE; &#125; &#125; pthread_mutex_init(&amp;maintenance_lock, NULL); if ((ret = pthread_create(&amp;maintenance_tid, NULL, assoc_maintenance_thread, NULL)) != 0) &#123; fprintf(stderr, \"Can't create thread: %s\\n\", strerror(ret)); return -1; &#125; return 0;&#125; start_assoc_maintenance_thread@assoc.c Similar to initialization methods, it is called during system bootstrap. ...if (start_assoc_maintenance_thread() == -1) &#123; exit(EXIT_FAILURE);&#125;...main@memcached.c:5992 Maintenance thread stopThis method is called in system shutdown process, hence it is opposite in logic to start_assoc_maintenance_thread. Nevertheless, the operations of this method are opposite that of assoc_start_expand mechanism wise. ... stop_assoc_maintenance_thread();...main@memcached.c:6098 void stop_assoc_maintenance_thread() &#123; mutex_lock(&amp;maintenance_lock); do_run_maintenance_thread = 0; pthread_cond_signal(&amp;maintenance_cond); mutex_unlock(&amp;maintenance_lock); /* Wait for the maintenance thread to stop */ pthread_join(maintenance_tid, NULL);&#125; stop_assoc_maintenance_thread@assoc.c As said before, the expanding &amp; migration process discussed here has an impact on the logic of all hash map related operations. In the next section, we look at these operations. CRUDN.b., assoc_delete has been discussed in the last post; and in a key-value system update and insert are essentially the same, thus, this section will discuss the operations of C (create) and R (read) only. assoc_insertint assoc_insert(item *it, const uint32_t hv) &#123; unsigned int oldbucket; if (expanding &amp;&amp; (oldbucket = (hv &amp; hashmask(hashpower - 1))) &gt;= expand_bucket) &#123; it-&gt;h_next = old_hashtable[oldbucket]; // scr: -------------------&gt; 1) old_hashtable[oldbucket] = it; &#125; else &#123; it-&gt;h_next = primary_hashtable[hv &amp; hashmask(hashpower)]; // scr: &gt; 2) primary_hashtable[hv &amp; hashmask(hashpower)] = it; &#125; pthread_mutex_lock(&amp;hash_items_counter_lock); hash_items++; // scr: ------------------------------------------------&gt; 3) if (! expanding &amp;&amp; hash_items &gt; (hashsize(hashpower) * 3) / 2) &#123; assoc_start_expand(); &#125; pthread_mutex_unlock(&amp;hash_items_counter_lock); MEMCACHED_ASSOC_INSERT(ITEM_key(it), it-&gt;nkey, hash_items); return 1;&#125; assoc_insert@assoc.c 1) If expanding process is undergoing and the hash key associated bucket has not been migrated, insert the item to old_hashtable. Note that here we use the old bucket number (i.e., hashmask(hashpower - 1))) to calculate the hash index. 2) Otherwise, insert the itemto primary_hashtable directly. 3) Increase the global variable hash_items (number of items). If it exceeds the threshold after the item is added, start expanding &amp; migration process. Note that this is also the preamble of the last section. ...static unsigned int hash_items = 0;...assoc.c:51 assoc_finditem *assoc_find(const char *key, const size_t nkey, const uint32_t hv) &#123; item *it; unsigned int oldbucket; if (expanding &amp;&amp; (oldbucket = (hv &amp; hashmask(hashpower - 1))) &gt;= expand_bucket) &#123; it = old_hashtable[oldbucket]; // scr: ---------------------------&gt; 1) &#125; else &#123; it = primary_hashtable[hv &amp; hashmask(hashpower)]; // scr: --------&gt; 2) &#125; item *ret = NULL; int depth = 0; while (it) &#123; // scr: -------------------------------------------------&gt; 3) if ((nkey == it-&gt;nkey) &amp;&amp; (memcmp(key, ITEM_key(it), nkey) == 0)) &#123; ret = it; break; &#125; it = it-&gt;h_next; ++depth; &#125; MEMCACHED_ASSOC_FIND(key, nkey, depth); return ret;&#125; assoc_find@assoc.c 1) Similar to that of assoc_insert, this step locates the bucket from old_hashtable when the key is yet to be rehashed. 2) Use primary_hashtable directly otherwise. 3) Go through the linked list and compare the key (instead of the hash index) directly to lookup the item in the case of Collision. One thing worth noting is that assoc_find is very similar to _hashitem_before which has been discussed in the last post. The difference here is, _hashitem_before returns the address of the next member of the element before the found one (pos = &amp;(*pos)-&gt;h_next;), which is required when removing entries from a singly linked list; whilst this method returns the element found directly (ret = it;).","categories":[{"name":"Memcached Source Code","slug":"Memcached-Source-Code","permalink":"https://holmeshe.me/categories/Memcached-Source-Code/"}],"tags":[{"name":"memcached","slug":"memcached","permalink":"https://holmeshe.me/tags/memcached/"},{"name":"cache","slug":"cache","permalink":"https://holmeshe.me/tags/cache/"},{"name":"LRU","slug":"LRU","permalink":"https://holmeshe.me/tags/LRU/"},{"name":"hash map","slug":"hash-map","permalink":"https://holmeshe.me/tags/hash-map/"}]},{"title":"Understanding The Memcached Source Code - LRU I","slug":"understanding-memcached-source-code-IV","date":"2018-12-10T23:00:00.000Z","updated":"2020-12-22T10:48:38.121Z","comments":true,"path":"understanding-memcached-source-code-IV/","link":"","permalink":"https://holmeshe.me/understanding-memcached-source-code-IV/","excerpt":"slab allocator (I, II, III) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I - this article , II , III) for entry expiration; and an event driven model (I , II , III) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. More often than not, the LRU algorithm is combined with a hash map, and is referred to as a LRU CacheIn a LRU-cache, the hash map enables fast accessing of cached objects; and LRU avoids the cache to grow infinitely by marking expired, or so called, least recently used objects. Next we look at how LRU works from a high level standpoint.","text":"slab allocator (I, II, III) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I - this article , II , III) for entry expiration; and an event driven model (I , II , III) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. More often than not, the LRU algorithm is combined with a hash map, and is referred to as a LRU CacheIn a LRU-cache, the hash map enables fast accessing of cached objects; and LRU avoids the cache to grow infinitely by marking expired, or so called, least recently used objects. Next we look at how LRU works from a high level standpoint. Linked listTechnically, LRU algorithm works on a linked list, whenever a list entry is used (accessed or updated), it is removed from the list and be attached to the list head. In this way, the closer an element is to the list tail, the less recently used it is. Hence it is easy to remove irrelevant or “expired” elements from the tail based on a certain configuration. Harsh mapLinked list is slow when it comes to element access, hence another data structure is employed. We have seen how linked list “strings” chunks in slabs to make free lists. In an LRU cache, the mechanism is similar, however, it is the hash map entries instead of chunks in slabs got wired up this time, which looks like: We can also flatten the linked list, and make the structure a bit more clear, Core data structure - itemtypedef struct _stritem &#123; /* Protected by LRU locks */ struct _stritem *next; struct _stritem *prev; /* Rest are protected by an item lock */ struct _stritem *h_next; /* hash chain next */ rel_time_t time; /* least recent access */ rel_time_t exptime; /* expire time */ int nbytes; /* size of data */ unsigned short refcount; uint8_t nsuffix; /* length of flags-and-length string */ uint8_t it_flags; /* ITEM_* above */ uint8_t slabs_clsid;/* which slab class we're in */ uint8_t nkey; /* key length, w/terminating null and padding */ /* this odd type prevents type-punning issues when we do * the little shuffle to save space when not using CAS. */ union &#123;... // scr: cas char end; // scr: flexible array member indicating the item header \"end\" &#125; data[]; /* if it_flags &amp; ITEM_CAS we have 8 bytes CAS */ /* then null-terminated key */ /* then \" flags length\\r\\n\" (no terminating null) */ /* then data with terminating \\r\\n (no terminating null; it's binary!) */&#125; item; do_item_unlink@item.c Properties in discussionnext, prev - LRU list pointers, initialized in do_item_alloc (LRU III), used by item_link_q, item_unlink_q h_next - hash collision list pointers, initialized in do_item_alloc (LRU III), used by assoc_insert, assoc_delete, various methods (LRU II) time - last access time, set in do_item_link, used by lru_pull_tail (LRU III) exptime - expire time indicated by request argument, initialized in do_item_alloc (LRU III), used by lru_pull_tail (LRU III) nbytes - data size indicated by request argument, initialized in do_item_alloc (LRU III) refcount - reference cound, initialized in do_slabs_alloc (Slab III), used by do_item_link nsuffix - initialized in do_item_alloc (LRU III) with item_make_header it_flags - initialized in do_item_alloc (LRU III), used by do_item_link, do_item_unlink slabs_clsid - the LRU list the item belongs, initialized in do_item_alloc (LRU III), used by item_link_q, item_unlink_q nkey - key size, calcuated in do_item_alloc (LRU III), used by assoc_delete Memory layout of an item chunkWe have mentioned item chunk in do_slabs_free. With the help of this data structure, we can now examine the chunk more closely. Next we read the relevant code that performs the above discussed LRU operations. do_item_linkint do_item_link(item *it, const uint32_t hv) &#123; // scr: -------------------&gt; 1)... it-&gt;it_flags |= ITEM_LINKED; // scr: -------------------&gt; 2) it-&gt;time = current_time;... // scr: stat /* Allocate a new CAS ID on link. */... // scr: cas assoc_insert(it, hv); // scr: -------------------&gt; 3) item_link_q(it); // scr: -------------------&gt; 4) refcount_incr(&amp;it-&gt;refcount); // scr: -------------------&gt; 5)... // scr: stat return 1;&#125; do_item_link@item.c 1) hv is supposed to be the shortened “hashed value”. 2) Set ITEM_LINKED in it-&gt;it_flags, and set current time to it-&gt;time. The field it_flags is used in do_slabs_free and do_slabs_alloc 3) Insert the item to hash map. 4) Insert the item to linked list. 5) Increase the reference count. This field is initialized as 1 in do_slabs_alloc It is worth noting here that reference count indicates how many sub-modules are using the same resource, so as to determine when to actually deallocate the resource (In this particular case, item is referred by both slab and LRU). I have written this article that explains a similar mechanism of C++. item_link_q - add to linked listitem_link_q is a thread safe wrapper of the workhorse method do_item_link_q.static void item_link_q(item *it) &#123; pthread_mutex_lock(&amp;lru_locks[it-&gt;slabs_clsid]); do_item_link_q(it); pthread_mutex_unlock(&amp;lru_locks[it-&gt;slabs_clsid]);&#125; item_link_q@item.c static void do_item_link_q(item *it) &#123; /* item is the new head */ item **head, **tail; assert((it-&gt;it_flags &amp; ITEM_SLABBED) == 0); head = &amp;heads[it-&gt;slabs_clsid]; // scr: -------------------&gt; 1) tail = &amp;tails[it-&gt;slabs_clsid]; assert(it != *head); assert((*head &amp;&amp; *tail) || (*head == 0 &amp;&amp; *tail == 0)); it-&gt;prev = 0; // scr: -------------------&gt; 2) it-&gt;next = *head; if (it-&gt;next) it-&gt;next-&gt;prev = it; *head = it; if (*tail == 0) *tail = it; sizes[it-&gt;slabs_clsid]++; // scr: -------------------&gt; 3) return;&#125; do_item_link_q@item.c 1) Get the head and tail of the respective LRU linked list indicated by slabs_clsid. Note that the slabs_clsid is salted with the type of the queue, hence each slab group may enlist multiple lists. 2) Standard operations of “adding an element to the front”. 3) Increase the global array sizes. static item *heads[LARGEST_ID];static item *tails[LARGEST_ID];...static unsigned int sizes[LARGEST_ID];item.c:59 assoc_insert - add to hash mapint assoc_insert(item *it, const uint32_t hv) &#123; // scr: again, hv -&gt; hash value unsigned int oldbucket;... // scr: expanding related operations &#125; else &#123; it-&gt;h_next = primary_hashtable[hv &amp; hashmask(hashpower)]; // scr: 1) primary_hashtable[hv &amp; hashmask(hashpower)] = it; // scr: 2) &#125;... // scr: expanding related operations&#125; assoc_insert@assoc.c 1) Deal with potential conflict. If there is no, the h_next is set to null. 2) Set the item to the bucket in primary_hashtable. ...static item** primary_hashtable = 0;...assoc.c:42 The expanding logic omitted here will be covered in the next post. do_item_unlinkvoid do_item_unlink(item *it, const uint32_t hv) &#123; MEMCACHED_ITEM_UNLINK(ITEM_key(it), it-&gt;nkey, it-&gt;nbytes); if ((it-&gt;it_flags &amp; ITEM_LINKED) != 0) &#123; it-&gt;it_flags &amp;= ~ITEM_LINKED; // scr: -------------------&gt; 1)... // scr: stat assoc_delete(ITEM_key(it), it-&gt;nkey, hv); // scr: ---------------&gt; 2) item_unlink_q(it); // scr: -------------------&gt; 3) do_item_remove(it); // scr: -------------------&gt; *) &#125;&#125; do_item_unlink@item.c 1) Clear ITEM_LINKED in it-&gt;it_flags. 2) Remove the item from hash map. 3) Remove the item from linked list. *) The actual releasing of an item will be covered in later posts. item_unlink_q - remove from linked listLikewise, item_unlink_q is a thread safe wrapper of the workhorse method do_item_unlink_q. static void item_link_q(item *it) &#123; pthread_mutex_lock(&amp;lru_locks[it-&gt;slabs_clsid]); do_item_link_q(it); pthread_mutex_unlock(&amp;lru_locks[it-&gt;slabs_clsid]);&#125;item_unlink_q@item.c static void do_item_unlink_q(item *it) &#123; item **head, **tail; head = &amp;heads[it-&gt;slabs_clsid]; // scr: -------------------&gt; 1) tail = &amp;tails[it-&gt;slabs_clsid]; if (*head == it) &#123; // scr: -------------------&gt; 2) assert(it-&gt;prev == 0); *head = it-&gt;next; &#125; if (*tail == it) &#123; assert(it-&gt;next == 0); *tail = it-&gt;prev; &#125; assert(it-&gt;next != it); assert(it-&gt;prev != it); if (it-&gt;next) it-&gt;next-&gt;prev = it-&gt;prev; if (it-&gt;prev) it-&gt;prev-&gt;next = it-&gt;next; sizes[it-&gt;slabs_clsid]--; // scr: -------------------&gt; 3) return;&#125; do_item_unlink_q@item.c 1) Same, get the head and tail of the respective LRU linked list indicated by slabs_clsid. 2) Standard operations of “removing an element from a linked list”. 3) Decrease the global array sizes. static item *heads[LARGEST_ID];static item *tails[LARGEST_ID];...static unsigned int sizes[LARGEST_ID];item.c:59 assoc_delete - remove from hash mapstatic item** _hashitem_before (const char *key, const size_t nkey, const uint32_t hv) &#123; item **pos; unsigned int oldbucket;... // scr: expanding related operations &#125; else &#123; pos = &amp;primary_hashtable[hv &amp; hashmask(hashpower)]; // scr: -----&gt; 1) &#125; while (*pos &amp;&amp; ((nkey != (*pos)-&gt;nkey) || memcmp(key, ITEM_key(*pos), nkey))) &#123; pos = &amp;(*pos)-&gt;h_next; // scr: ----------------------------------&gt; 2) &#125; return pos;&#125;...void assoc_delete(const char *key, const size_t nkey, const uint32_t hv) &#123; item **before = _hashitem_before(key, nkey, hv); if (*before) &#123; item *nxt;... nxt = (*before)-&gt;h_next; // scr: --------------------------------&gt; 3) (*before)-&gt;h_next = 0; /* probably pointless, but whatever. */ *before = nxt; // scr: ------------------------------------------&gt; 4) return; &#125; /* Note: we never actually get here. the callers don't delete things they can't find. */ assert(*before != 0);&#125; assoc_delete@assoc.c 1) Get the hash bucket using hv. 2) Go through the conflict chain and compare the key. Note that the result value is the address of the next member of the element before the found one. When there is no conflict, the address is the bucket itself. 3) Set the next element after the found one to temporary variable nxt. 4) Update the next member of the element before the found one. Take homeTry this.","categories":[{"name":"Memcached Source Code","slug":"Memcached-Source-Code","permalink":"https://holmeshe.me/categories/Memcached-Source-Code/"}],"tags":[{"name":"memcached","slug":"memcached","permalink":"https://holmeshe.me/tags/memcached/"},{"name":"cache","slug":"cache","permalink":"https://holmeshe.me/tags/cache/"},{"name":"LRU","slug":"LRU","permalink":"https://holmeshe.me/tags/LRU/"}]},{"title":"Understanding The Memcached Source Code - Slab III","slug":"understanding-memcached-source-code-III","date":"2018-09-28T09:55:00.000Z","updated":"2020-12-22T10:48:38.121Z","comments":true,"path":"understanding-memcached-source-code-III/","link":"","permalink":"https://holmeshe.me/understanding-memcached-source-code-III/","excerpt":"slab allocator (I, II, III - this article) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I , II , III) for entry expiration; and an event driven model (I , II , III) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. Last time we saw the memory allocating process, which further formulates slabs and the derivative “free lists” (a.k.a., slots). This time we will examine how to take advantage of the established data structures to “slab allocate / release” memory chunks which will be used to store items.","text":"slab allocator (I, II, III - this article) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I , II , III) for entry expiration; and an event driven model (I , II , III) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. Last time we saw the memory allocating process, which further formulates slabs and the derivative “free lists” (a.k.a., slots). This time we will examine how to take advantage of the established data structures to “slab allocate / release” memory chunks which will be used to store items. Slab allocFirstly, we look at do_slabs_allocwhich is opposite to the discussed do_slabs_free. Note that the “public” interface of do_slabs_alloc is slabs_alloc which is basically a thread-safe wrapper that locks the core data structures manipulated by the Memcached instance that is configured as multithreaded. void *slabs_alloc(size_t size, unsigned int id, unsigned int *total_chunks, unsigned int flags) &#123; void *ret; pthread_mutex_lock(&amp;slabs_lock); ret = do_slabs_alloc(size, id, total_chunks, flags); pthread_mutex_unlock(&amp;slabs_lock); return ret;&#125;slabs_alloc@slabs.c ... case 't': settings.num_threads = atoi(optarg); if (settings.num_threads &lt;= 0) &#123; fprintf(stderr, \"Number of threads must be greater than 0\\n\"); return 1; &#125; /* There're other problems when you get above 64 threads. * In the future we should portably detect # of cores for the * default. */ if (settings.num_threads &gt; 64) &#123; fprintf(stderr, \"WARNING: Setting a high number of worker\" \"threads is not recommended.\\n\" \" Set this value to the number of cores in\" \" your machine or less.\\n\"); &#125; break;...main@memcached.c:5572 static void *do_slabs_alloc(const size_t size, unsigned int id, unsigned int *total_chunks, unsigned int flags) &#123; slabclass_t *p; void *ret = NULL; item *it = NULL;... p = &amp;slabclass[id]; // scr: ----------------------------------------&gt; 1)... if (total_chunks != NULL) &#123; *total_chunks = p-&gt;slabs * p-&gt;perslab; // scr: -----------------&gt; 2) &#125; /* fail unless we have space at the end of a recently allocated page, we have something on our freelist, or we could allocate a new page */ if (p-&gt;sl_curr == 0 &amp;&amp; flags != SLABS_ALLOC_NO_NEWPAGE) &#123; // scr: --&gt; *) do_slabs_newslab(id); // scr: ----------------------------------&gt; 3) &#125; if (p-&gt;sl_curr != 0) &#123; /* return off our freelist */ it = (item *)p-&gt;slots; // scr: ---------------------------------&gt; 4) p-&gt;slots = it-&gt;next; if (it-&gt;next) it-&gt;next-&gt;prev = 0; /* Kill flag and initialize refcount here for lock safety in slab * mover's freeness detection. */ it-&gt;it_flags &amp;= ~ITEM_SLABBED; // scr: -------------------------&gt; 5) it-&gt;refcount = 1; p-&gt;sl_curr--; ret = (void *)it; // scr: --------------------------------------&gt; 6) &#125; else &#123; ret = NULL; &#125;... return ret;&#125; do_slabs_alloc@slabs.c 1) For item allocation, id indicates the slab class that suits the requested item size best. In other words, id is selected using the actual item size, the process of which will be discussed very soon. 2) total_chunks is the parameter that outputs the total number of memory chunks (entries in the free list) available for the slab class. if (total_chunks != NULL) suggests that the argument is optional. *) As the name indicates, SLABS_ALLOC_NO_NEWPAGE (flags) prevents this method to allocate new slab when there is no memory chunk available. This option is not used in the normal path of item allocation, hence is ignored for now. 3) When there is no free memory chunk, allocate a new slab. Here p-&gt;sl_curr indicates the number of available chunks, whose value decreases each time this method got called (in step 5 below). Conversely, this field is increased in do_slabs_free. Note that new slab has also been covered from here. 4) Remove the front element (f) from the free list, and set it to it. In do_slabs_free, an element is added to the front of the free list. 5) Clear the ITEM_SLABBED for the chuck (f), set its reference count to 1, and reduce p-&gt;sl_curr by 1. Likewise, this flag is set in do_slabs_free. 6) Return (f). Next, we look at the process of determining the id based on item size, the workhorse method of which is slabs_clsidunsigned int slabs_clsid(const size_t size) &#123; int res = POWER_SMALLEST; if (size == 0) return 0; while (size &gt; slabclass[res].size) if (res++ == power_largest) /* won't fit in the biggest slab */ return 0; return res;&#125; do_slabs_alloc@slabs.c slabs_clsid consists mainly of a while loop that linear search the possible smallest slab class that can contain the requested size. This method is called from do_item_alloc before slabs_alloc. We will discuss do_item_alloc in the following post. item *do_item_alloc(char *key, const size_t nkey, const unsigned int flags, const rel_time_t exptime, const int nbytes, const uint32_t cur_hv) &#123;... unsigned int id = slabs_clsid(ntotal); if (id == 0) return 0;... it = slabs_alloc(ntotal, id, &amp;total_chunks, 0);...do_item_alloc@items.c","categories":[{"name":"Memcached Source Code","slug":"Memcached-Source-Code","permalink":"https://holmeshe.me/categories/Memcached-Source-Code/"}],"tags":[{"name":"memcached","slug":"memcached","permalink":"https://holmeshe.me/tags/memcached/"},{"name":"slab allocator","slug":"slab-allocator","permalink":"https://holmeshe.me/tags/slab-allocator/"},{"name":"cache","slug":"cache","permalink":"https://holmeshe.me/tags/cache/"}]},{"title":"Understanding The Memcached Source Code - Slab II","slug":"understanding-memcached-source-code-II","date":"2018-09-17T08:17:00.000Z","updated":"2020-12-22T10:48:38.121Z","comments":true,"path":"understanding-memcached-source-code-II/","link":"","permalink":"https://holmeshe.me/understanding-memcached-source-code-II/","excerpt":"slab allocator (I, II - this article , III) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I , II , III) for entry expiration; and an event driven model (I , II , III) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. This time we continue examining how slabs memory is allocated. Firstly we look at the two arguments for slabs_init, which were passed over in the previous article. The first one is settings.maxbytes. It limits the overall memory that can be used by the memcached instance. In slabs_init, the value of settings.maxbytes is assigned to the global variable mem_limit which will be used very soon.","text":"slab allocator (I, II - this article , III) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I , II , III) for entry expiration; and an event driven model (I , II , III) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. This time we continue examining how slabs memory is allocated. Firstly we look at the two arguments for slabs_init, which were passed over in the previous article. The first one is settings.maxbytes. It limits the overall memory that can be used by the memcached instance. In slabs_init, the value of settings.maxbytes is assigned to the global variable mem_limit which will be used very soon. void slabs_init(const size_t limit, const double factor, const bool prealloc, const uint32_t *slab_sizes) &#123;... mem_limit = limit; // scr: here...slabs_init@memcached.c ... settings.maxbytes = 64 * 1024 * 1024; /* default is 64MB */... case 'm': settings.maxbytes = ((size_t)atoi(optarg)) * 1024 * 1024; break;...memcached.c:210,5493 static size_t mem_limit = 0;memcached.c:43 The other argument is preallocate. It determines whether to preallocate slab for each slab class. This argument is toggled with L command line argument. ... bool preallocate = false;... case 'L' : if (enable_large_pages() == 0) &#123; preallocate = true; &#125; else &#123; fprintf(stderr, \"Cannot enable large pages on this system\\n\" \"(There is no Linux support as of this version)\\n\"); return 1; &#125; break;...main@memcached.c:5350,5597 Next we look at the method for slabs memory allocation itself. New slabdo_slabs_newslabMore specific, this method allocates one 1M sized slab for the slab class indicated by the parameter id. static int do_slabs_newslab(const unsigned int id) &#123; slabclass_t *p = &amp;slabclass[id]; // scr: ----------------------------&gt; 1) slabclass_t *g = &amp;slabclass[SLAB_GLOBAL_PAGE_POOL]; // scr: ---------&gt; *) int len = settings.slab_reassign ? settings.item_size_max // scr: ---&gt; 2) : p-&gt;size * p-&gt;perslab; char *ptr; if ((mem_limit &amp;&amp; mem_malloced + len &gt; mem_limit &amp;&amp; p-&gt;slabs &gt; 0 // -&gt; 3) &amp;&amp; g-&gt;slabs == 0)) &#123; mem_limit_reached = true; MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id); return 0; &#125; if ((grow_slab_list(id) == 0) || // scr: ----------------------------&gt; 4) (((ptr = get_page_from_global_pool()) == NULL) &amp;&amp; // scr: -------&gt; *) ((ptr = memory_allocate((size_t)len)) == 0))) &#123; // scr: ---------&gt; 5) MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id); return 0; &#125; memset(ptr, 0, (size_t)len); split_slab_page_into_freelist(ptr, id); // scr: ---------------------&gt; 6) p-&gt;slab_list[p-&gt;slabs++] = ptr; // scr: -----------------------------&gt; 7) MEMCACHED_SLABS_SLABCLASS_ALLOCATE(id); return 1;&#125; do_slabs_newslab@slabs.c 1) slabclass[id] is one of the slab class, the initialization of which is discussed in last article. 2) settings.slab_reassign determines whether to enlist a rebalancing mechanism, which recycles the unused slabs and redistributes them across slab classes. This requires that slabs contained in all slab classes be of the same size, hence this setting also decides whether to use unanimous (i.e., settings.item_size_max, or 1M as mentioned before) or heterogeneous (i.e., p-&gt;size * p-&gt;perslab) slabs. Besides its associated command line argument \"slab_reassign\", the value can be controlled by another argument \"modern\". For the positivity the name “modern” implies, 1M will be used throughout the text. ... settings.slab_reassign = false;... case SLAB_REASSIGN: settings.slab_reassign = true; break;...main@memcached.c:238,5694 case MODERN: /* Modernized defaults. Need to add equivalent no_* flags * before making truly default. */ settings.slab_reassign = true; settings.slab_automove = 1;... break;main@memcached.c:5820 N.b. *, rebalancing mechanism will be discussed later when we have a better understanding of the LRU module. 3) Check if the memory usage will exceed the upper limit. 4) grow_slab_list checks if we need to increase slabclass_t.slab_list, if so, grows it. static int grow_slab_list (const unsigned int id) &#123; slabclass_t *p = &amp;slabclass[id]; if (p-&gt;slabs == p-&gt;list_size) &#123; size_t new_size = (p-&gt;list_size != 0) ? p-&gt;list_size * 2 : 16; void *new_list = realloc(p-&gt;slab_list, new_size * sizeof(void *)); if (new_list == 0) return 0; p-&gt;list_size = new_size; p-&gt;slab_list = new_list; &#125; return 1;&#125;grow_slab_list@slabs.c 5) memory_allocate allocates the actual memory for the slab. As discussed, here the value of len is 1M. static void *memory_allocate(size_t size) &#123; void *ret; if (mem_base == NULL) &#123; /* We are not using a preallocated large memory chunk */ ret = malloc(size); &#125; else &#123; // scr: when preallocate is set to true...memory_allocate@slabs.c 6) split_slab_page_into_freelist initializes (frees) the newly allocated slab preparing for objects storing. This method will be discussed in the next section. 7) Add the newly allocated slab to the slabclass_t.slab_list. What has happened so far can be summarized with the following figure, (we assume do_slabs_newslab(n) is called two times) Now we look inside the 1M slab in step 6). split_slab_page_into_freeliststatic void split_slab_page_into_freelist(char *ptr, const unsigned int id) &#123; slabclass_t *p = &amp;slabclass[id]; int x; for (x = 0; x &lt; p-&gt;perslab; x++) &#123; do_slabs_free(ptr, 0, id); ptr += p-&gt;size; &#125;&#125; split_slab_page_into_freelist@slabs.c This method goes through all the item chunks (in the size of slabclass_t.size) within a slab. And for each of them, the method initializes its meta data by calling do_slabs_free. Another way to interpret this process is “split a slab into item free list”. As you might have already figured out, this “free list” will be used by item allocation in the future. do_slabs_freestatic void do_slabs_free(void *ptr, const size_t size, unsigned int id) &#123; slabclass_t *p; item *it;... p = &amp;slabclass[id]; it = (item *)ptr; it-&gt;it_flags = ITEM_SLABBED; // scr: ---------------&gt; 1) it-&gt;slabs_clsid = 0; it-&gt;prev = 0; // scr: ------------------------------&gt; 2) it-&gt;next = p-&gt;slots; if (it-&gt;next) it-&gt;next-&gt;prev = it; p-&gt;slots = it; p-&gt;sl_curr++; // scr: ------------------------------&gt; 3) p-&gt;requested -= size; return;&#125; do_slabs_free@slabs.c This method works on item meta data that is populated at the beginning of an item chunk. typedef struct _stritem &#123; /* Protected by LRU locks */ struct _stritem *next; struct _stritem *prev;... uint8_t it_flags; /* ITEM_* above */ uint8_t slabs_clsid;/* which slab class we're in */...&#125; item;main@memcached.c:5820 1) Initialize some fields. item is another core data structure, we will come back to item data structure later. 2) Add the item to the front of the linked list (a.k.a., free list). And update the list head, slabclass_t.slots. 3) Update the available (free list) slot count, slabclass_t.sl_curr; and updates the slabclass_t.requested for statistic. Note that here we are not actually releasing an item, so the passed size is 0. Slab preallocateNext we look at how do_slabs_newslab is used. One place it gets called is from the discussed slabs_init when preallocate is set to true, void slabs_init(const size_t limit, const double factor, const bool prealloc, const uint32_t *slab_sizes) &#123;... if (prealloc) &#123; slabs_preallocate(power_largest); &#125;&#125;slabs_init@slabs.c static void slabs_preallocate (const unsigned int maxslabs) &#123; int i; unsigned int prealloc = 0; /* pre-allocate a 1MB slab in every size class so people don't get confused by non-intuitive \"SERVER_ERROR out of memory\" messages. this is the most common question on the mailing list. if you really don't want this, you can rebuild without these three lines. */ for (i = POWER_SMALLEST /* scr: 1 */; i &lt; MAX_NUMBER_OF_SLAB_CLASSES; i++) &#123; if (++prealloc &gt; maxslabs) return; if (do_slabs_newslab(i) == 0) &#123; fprintf(stderr, \"Error while preallocating slab memory!\\n\" \"If using -L or other prealloc options, max memory must be \" \"at least %d megabytes.\\n\", power_largest); exit(1); &#125; &#125;&#125; slabs_preallocate@slabs.c This method simply goes through the slabclass starting from the POWER_SMALLEST, i.e., 1st entry, and allocate one slab for each of them. Note that the 0th is a special slab class used by mentioned rebalancing mechanism. #define POWER_SMALLEST 1#define POWER_LARGEST 256 /* actual cap is 255 */#define SLAB_GLOBAL_PAGE_POOL 0 /* magic slab class for storing pages for reassignment */memcached.h:88 ReferencesSame to the last article.","categories":[{"name":"Memcached Source Code","slug":"Memcached-Source-Code","permalink":"https://holmeshe.me/categories/Memcached-Source-Code/"}],"tags":[{"name":"memcached","slug":"memcached","permalink":"https://holmeshe.me/tags/memcached/"},{"name":"slab allocator","slug":"slab-allocator","permalink":"https://holmeshe.me/tags/slab-allocator/"},{"name":"cache","slug":"cache","permalink":"https://holmeshe.me/tags/cache/"},{"name":"source code analysis","slug":"source-code-analysis","permalink":"https://holmeshe.me/tags/source-code-analysis/"}]},{"title":"Understanding The Memcached Source Code - Slab I","slug":"understanding-memcached-source-code-I","date":"2018-09-12T09:17:00.000Z","updated":"2020-12-22T10:48:38.121Z","comments":true,"path":"understanding-memcached-source-code-I/","link":"","permalink":"https://holmeshe.me/understanding-memcached-source-code-I/","excerpt":"slab allocator (I - this article , II , III) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I , II , III) for entry expiration; and an event driven model (I , II , III) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. Variants of slab allocator is implemented in other systems, such as nginx and Linux kernel, to fight a common problem called memory fragmentation. And this article will, of course, focus on Memcached‘s implementation of the algorithm. memcached version: 1.4.28 Firstly, let’s answer some questions.","text":"slab allocator (I - this article , II , III) is the core module of the cache system, which largely determines how efficient the bottleneck resource, memory, can be utilized. The other 3 parts, namely, LRU algorithm (I , II , III) for entry expiration; and an event driven model (I , II , III) based on libevent; and the consistent harsh (not complete) for data distribution, are built around it. Variants of slab allocator is implemented in other systems, such as nginx and Linux kernel, to fight a common problem called memory fragmentation. And this article will, of course, focus on Memcached‘s implementation of the algorithm. memcached version: 1.4.28 Firstly, let’s answer some questions. IntroductionWhat is a slabslabs are pre-allocated 1M memory chunks that can be subdivided for numerous objects. They are grouped into slab classes to serve allocation requests for various sizes. What is memory fragmentation, how it occursIn particular, slab allocator curbs internal memory fragmentation. This kind of fragmentation exits within an allocated memory chunk. In the context of OS kernel, for instance, the fundamental unit allocated by memory management sub-system is called a page. On the other hand, external memory fragmentation exists across chunks, and the solution of which (keyword: buddy) belongs to another story. The most common phenomenon where internal fragmentation causes the problem is as following: 1) malloc of small objects is called a lot of times; and in the meantime; 2) free of those objects is called a lot of times. The above process generates (a lot of) nominal “free” memory that cannot be utilized, as the discrete holes of various sizes, or fragments, can not be reused by subsequent mallocs for any objects that are larger than them. Why memory fragmentation is badThe impact of memory fragmentation is similar to that of memory leak - periodical system reboot is inevitable whenever the fragments accumulate to a certain level, which, increase the complexity in system operation, or even worse, leads to bad user experiences. How the problem is fixedSlab allocator does not eliminate internal fragmentation. Instead, it converges the fragments and locks them in fixated memory locations. This is done by 1) categorizing objects of similar sizes in classes; and 2) allocating objects belonging to the same class only on the same group of “slabs”, or, a slab class. The detail devil is in the code, so we start reading the code. reminder: Memcached version is 1.4.28 The core data structure in usetypedef struct &#123; unsigned int size; /* sizes of items */ unsigned int perslab; /* how many items per slab */ void *slots; /* list of item ptrs */ unsigned int sl_curr; /* total free items in list */ unsigned int slabs; /* how many slabs were allocated for this class */ void **slab_list; /* array of slab pointers */ unsigned int list_size; /* size of prev array */ size_t requested; /* The number of requested bytes */&#125; slabclass_t;static slabclass_t slabclass[MAX_NUMBER_OF_SLAB_CLASSES];slabclass_t@slabs.c Module initializationIn this section we examine slabs_init that initializes slabclass[MAX_NUMBER_OF_SLAB_CLASSES] array. In particular, this process initializes the values of two fields, i.e., slabclass_t.size, the item (object) size of each slab class, and slabclass_t.perslab the item number one slab contains. This method is called from here as one of the init steps before the logic enters the main even loop. ... assoc_init(settings.hashpower_init); conn_init(); slabs_init(settings.maxbytes, settings.factor, preallocate, use_slab_sizes ? slab_sizes : NULL);...main@memcached.c:5977 In this step slab_sizes and settings.factor jointly control the routes in which sizes of each slab class are decided, they are: uint32_t slab_sizes[MAX_NUMBER_OF_SLAB_CLASSES];main@memcached.c:5372 settings.factor = 1.25;settings_init@memcached.c:217 a) if slab_sizes is not NULL, the values within the array are used directly; and b) otherwise, the sizes are calculated as base size × n × settings.factor where n is the index within slabclass. Besides the default values, the two arguments can be set at runtime as well. ... case 'f': settings.factor = atof(optarg); if (settings.factor &lt;= 1.0) &#123; fprintf(stderr, \"Factor must be greater than 1\\n\"); return 1; &#125; break;... case 'o': /* It's sub-opts time! */... case SLAB_SIZES: if (_parse_slab_sizes(subopts_value, slab_sizes)) &#123; use_slab_sizes = true; &#125; else &#123; return 1; &#125; break;...main@memcached.c:5558, 5810 The other two arguments of this method settings.maxbytes and preallocate will be discussed soon. For now we set false to preallocate and ignore the relevant logic flow. Next we look at the slabs_init itself. void slabs_init(const size_t limit, const double factor, const bool prealloc, const uint32_t *slab_sizes) &#123; int i = POWER_SMALLEST /* scr: 1 */ - 1; unsigned int size = sizeof(item) + settings.chunk_size; // scr: ---------&gt; b 1)... memset(slabclass, 0, sizeof(slabclass)); while (++i &lt; MAX_NUMBER_OF_SLAB_CLASSES-1) &#123; if (slab_sizes != NULL) &#123; // scr: -----------------------------------&gt; a 1) if (slab_sizes[i-1] == 0) break; size = slab_sizes[i-1]; &#125; else if (size &gt;= settings.item_size_max / factor) &#123; break; &#125; /* Make sure items are always n-byte aligned */ if (size % CHUNK_ALIGN_BYTES) // scr: ---------------------------------&gt; 2) size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES); slabclass[i].size = size; slabclass[i].perslab = settings.item_size_max / slabclass[i].size; // -&gt; 3) if (slab_sizes == NULL) size *= factor; // scr: -----------------------------------------&gt; b 4) if (settings.verbose &gt; 1) &#123; fprintf(stderr, \"slab class %3d: chunk size %9u perslab %7u\\n\", i, slabclass[i].size, slabclass[i].perslab); &#125; &#125; // scr: -------------------------------------------------------------------&gt; 5) power_largest = i; slabclass[power_largest].size = settings.item_size_max; slabclass[power_largest].perslab = 1;...&#125; slabs_init@slabs.c Route a1) use the values in slab_sizes; 2) align the size to CHUNK_ALIGN_BYTES, and give the result to slabclass[i].size; 3) calculate the slabclass[i].perslab; 5) use the settings.item_size_max to initialize the last slab class. Note that settings.item_size_max is the size of each slab, hence it is also the max size of items that are allocated on slabs. Likewise, the value of settings.item_size_max can be decided in runtime. settings.item_size_max = 1024 * 1024;settings_init@memcached.c:226 case 'I': buf = strdup(optarg); unit = buf[strlen(buf)-1]; if (unit == 'k' || unit == 'm' || unit == 'K' || unit == 'M') &#123; buf[strlen(buf)-1] = '\\0'; size_max = atoi(buf); if (unit == 'k' || unit == 'K') size_max *= 1024; if (unit == 'm' || unit == 'M') size_max *= 1024 * 1024; settings.item_size_max = size_max; &#125; else &#123; settings.item_size_max = atoi(buf); &#125; free(buf); if (settings.item_size_max &lt; 1024) &#123; fprintf(stderr, \"Item max size cannot be less than 1024 bytes.\\n\"); return 1; &#125; if (settings.item_size_max &gt; 1024 * 1024 * 128) &#123; fprintf(stderr, \"Cannot set item size limit higher than 128 mb.\\n\"); return 1; &#125; if (settings.item_size_max &gt; 1024 * 1024) &#123; fprintf(stderr, \"WARNING: Setting item max size above 1MB is not\" \" recommended!\\n\" \" Raising this limit increases the minimum memory requirements\\n\" \" and will decrease your memory efficiency.\\n\" ); &#125; break;main@memcached.c:5626 Route b1) calculate the base size with settings.chunk_size plus the extra bytes for metadata (item will be discussed in following articles); 2) align the size to CHUNK_ALIGN_BYTES, and give the result to slabclass[i].size; (same to route a) 3) calculate the slabclass[i].perslab; (same to route a) 4) calculate the size for the next slab class using factor (settings.factor); 5) use the settings.item_size_max to initialize the last slab class. (same to route a) Referencesmemcached wiki 第2回 memcachedのメモリストレージを理解する Memcached源码分析之存储机制Slabs（7） Understanding Malloc Ch8 - Slab Allocator The Slab Allocator:An Object-Caching Kernel Memory Allocator","categories":[{"name":"Memcached Source Code","slug":"Memcached-Source-Code","permalink":"https://holmeshe.me/categories/Memcached-Source-Code/"}],"tags":[{"name":"memcached","slug":"memcached","permalink":"https://holmeshe.me/tags/memcached/"},{"name":"slab allocator","slug":"slab-allocator","permalink":"https://holmeshe.me/tags/slab-allocator/"},{"name":"cache","slug":"cache","permalink":"https://holmeshe.me/tags/cache/"}]},{"title":"setsockopt, TCP_NODELAY and Packet Aggregation I","slug":"network-essentials-setsockopt-TCP_NODELAY","date":"2018-06-08T23:47:00.000Z","updated":"2020-12-22T10:48:38.121Z","comments":true,"path":"network-essentials-setsockopt-TCP_NODELAY/","link":"","permalink":"https://holmeshe.me/network-essentials-setsockopt-TCP_NODELAY/","excerpt":"Latency, instead of throughput, is found as the system bottleneck more often than not. However, the TCP socket enables a so-called nagle algorithm by default, which delays an egress packet in order to coalesces it with one that could be sent in the future, into a single TCP segment. This effectively reduces the number of TCP segments and the bandwidth overhead used by the TCP headers, whilst potentially imposes latency for every network request (response) being sent. Lock, and his temperamental brother, Block, are the two notorious villains in the world of programming. In the beginning, they always show up to assist. But sooner or later, they will kick your back-end like really hard. When I consider about nagle algorithem, it seems to me another scenario involving block operations which are meant to be helpful. So I decide to put hands on a keyboard to test if I am wrong. Software setupClient OS: Debian 4.9.88Server OS (LAN &amp; WAN): Unbutu 16.04gcc: 6.3.0 Hardware (or VM) setupServer (LAN): Intel® Core™2 Duo CPU E8400 @ 3.00GHz × 2, 4GBServer (WAN): t2.micro, 1GB","text":"Latency, instead of throughput, is found as the system bottleneck more often than not. However, the TCP socket enables a so-called nagle algorithm by default, which delays an egress packet in order to coalesces it with one that could be sent in the future, into a single TCP segment. This effectively reduces the number of TCP segments and the bandwidth overhead used by the TCP headers, whilst potentially imposes latency for every network request (response) being sent. Lock, and his temperamental brother, Block, are the two notorious villains in the world of programming. In the beginning, they always show up to assist. But sooner or later, they will kick your back-end like really hard. When I consider about nagle algorithem, it seems to me another scenario involving block operations which are meant to be helpful. So I decide to put hands on a keyboard to test if I am wrong. Software setupClient OS: Debian 4.9.88Server OS (LAN &amp; WAN): Unbutu 16.04gcc: 6.3.0 Hardware (or VM) setupServer (LAN): Intel® Core™2 Duo CPU E8400 @ 3.00GHz × 2, 4GBServer (WAN): t2.micro, 1GB The impact nagle algorithm has on latencyFirst thing first, the code of client: and server: The client code given above sends 1000 packets 4 bytes long in an interval indicated by the last command line argument. And as discussed, it adopts the default TCP behavior by default. The server is not different than a discard server, so the code is irrelevant here. In this test, I will record the number of packets that are aggregated in different intervals, by adjusting the mentioned argument. This way, we can grasp the extent of latency the nagle algorithm can impose. The same test is conducted in both LAN (RTT &lt; 0.6ms) and WAN (RTT ≈ 200ms). As given in the figures, the number aggregated packets approaches to 0 when the interval is greater than the RTT. This conforms to what described in &lt;&lt;TCP/IP Illustrated&gt;&gt; This algorithm says that a TCP connection can have only one outstanding small segment that has not yet been acknowledged. No additional small segments can be sent until the acknowledgment is received. If looking at the tcpdump output, we can also see that this algorithm effectively changes the sending interval to the RTT regardless of the actual write(2) frequency of the program. And the packets between two sends are those being aggregated. ...18:34:52.986972 IP debian.53700 &gt; ******.compute.amazonaws.com.6666: Flags [P.], seq 4:12, ack 1, win 229, options [nop,nop,TS val 7541746 ecr 2617170332], length 818:34:53.178277 IP debian.53700 &gt; ******.amazonaws.com.6666: Flags [P.], seq 12:20, ack 1, win 229, options [nop,nop,TS val 7541794 ecr 2617170379], length 818:34:53.369431 IP debian.53700 &gt; ******.amazonaws.com.6666: Flags [P.], seq 20:32, ack 1, win 229, options [nop,nop,TS val 7541842 ecr 2617170427], length 1218:34:53.560351 IP debian.53700 &gt; ******.amazonaws.com.6666: Flags [P.], seq 32:40, ack 1, win 229, options [nop,nop,TS val 7541890 ecr 2617170475], length 818:34:54.325242 IP debian.53700 &gt; ******.amazonaws.com.6666: Flags [P.], seq 68:80, ack 1, win 229, options [nop,nop,TS val 7542081 ecr 2617170666], length 12... As a result, the delay imposed on every packet by the algorithm is RTT on average and 2 * RTT in worst case. Combined with delayed ACKDelayed ACK is another similar algorithm, here I will just use the lines from &lt;&lt;TCP/IP Illustrated&gt;&gt; to brief the mechanism TCP will delay an ACK up to 200 ms to see if there is data to send with the ACK. Apperantly nagle algorithm is not happy with delayed ACK. In some cases when the back-end do not reply instantly to a request, delayed ACK will have to wait for another request which is potentially delayed by nagle algorithm waiting for ACK. This senario where two resources waiting for each other, in another word, is called a dead-lock. Remember the two brothers mentioned in the beginning? Unfortunately, in my environments, seems like the delayed ACK is disabled by default and I failed to enable it by flags = 0;flglen = sizeof(flags);getsockopt(sfd, SOL_TCP, TCP_QUICKACK, &amp;flags, &amp;flglen) So I could not hand test the compounded impact. DiscussionAt the moment when I am writing, except for telnet, most of the other applications, including those of front-end(Firefox, Chromium), back-end(nginx, memcached), and the telnet‘s substitute, ssh, disable nagle algorithm with some code like bellow, int flags =1;setsockopt(sfd, SOL_TCP, TCP_NODELAY, (void *)&amp;flags, sizeof(flags)); which indicates that the packets should be emitted as it is. ...18:22:38.983278 IP debian.43808 &gt; 192.168.1.71.6666: Flags [P.], seq 1:5, ack 1, win 229, options [nop,nop,TS val 7358245 ecr 6906652], length 418:22:38.984149 IP debian.43808 &gt; 192.168.1.71.6666: Flags [P.], seq 5:9, ack 1, win 229, options [nop,nop,TS val 7358246 ecr 6906652], length 418:22:38.985028 IP debian.43808 &gt; 192.168.1.71.6666: Flags [P.], seq 9:13, ack 1, win 229, options [nop,nop,TS val 7358246 ecr 6906653], length 418:22:38.985897 IP debian.43808 &gt; 192.168.1.71.6666: Flags [P.], seq 13:17, ack 1, win 229, options [nop,nop,TS val 7358246 ecr 6906653], length 418:22:38.986765 IP debian.43808 &gt; 192.168.1.71.6666: Flags [P.], seq 17:21, ack 1, win 229, options [nop,nop,TS val 7358246 ecr 6906653], length 4... I think the reasons behind the prevalence of TCP_NODELAY are as follows,1) the increasing bandwidth makes the benefits of nagle algorithm more and more negligible - it requires hundreds of thousands of tinygrams to saturate an edge node with mediocre bandwidth nowadays; and2) app that generate a lot of tinygram tend to demand low latency. To conclude, technically, it’s probably not a good idea to turn a modern real-time on-line battle arena into some (200 ms) turn based 80s RPG. ReferencesTCP/IP IllustratedRFC 896Hacker news","categories":[{"name":"Network Essentials","slug":"Network-Essentials","permalink":"https://holmeshe.me/categories/Network-Essentials/"}],"tags":[{"name":"network","slug":"network","permalink":"https://holmeshe.me/tags/network/"},{"name":"socket","slug":"socket","permalink":"https://holmeshe.me/tags/socket/"},{"name":"setsockopt","slug":"setsockopt","permalink":"https://holmeshe.me/tags/setsockopt/"},{"name":"SOL_TCP","slug":"SOL-TCP","permalink":"https://holmeshe.me/tags/SOL-TCP/"},{"name":"TCP_NODELAY","slug":"TCP-NODELAY","permalink":"https://holmeshe.me/tags/TCP-NODELAY/"},{"name":"packet aggregation","slug":"packet-aggregation","permalink":"https://holmeshe.me/tags/packet-aggregation/"},{"name":"nagle algorithm","slug":"nagle-algorithm","permalink":"https://holmeshe.me/tags/nagle-algorithm/"}]},{"title":"setsockopt, SO_KEEPALIVE and Heartbeats","slug":"network-essentials-setsockopt-SO_KEEPALIVE","date":"2018-05-27T08:45:00.000Z","updated":"2020-12-22T10:48:38.121Z","comments":true,"path":"network-essentials-setsockopt-SO_KEEPALIVE/","link":"","permalink":"https://holmeshe.me/network-essentials-setsockopt-SO_KEEPALIVE/","excerpt":"There are two end purposes for sending heartbeats through a persistent connection. For a back-end application, heartbeats are generally used to detect an absent client, so as to drop a connection and release the associated resources; for a client, on the contrary, it is to prevent connection resources stored within intermediate nodes being released (such as a NAT router), SO as to KEEP the connection ALIVE. This article will examine how to configure the four socket options, SO_KEEPALIVE, TCP_KEEPIDLE, TCP_KEEPINTVL and TCP_KEEPCNT with setsockopt() to send heartbeats; and discuss the practice of keep-alive heartbeats in general. Experiment setting:OS: Unbutu 16.04gcc: 5.4.0","text":"There are two end purposes for sending heartbeats through a persistent connection. For a back-end application, heartbeats are generally used to detect an absent client, so as to drop a connection and release the associated resources; for a client, on the contrary, it is to prevent connection resources stored within intermediate nodes being released (such as a NAT router), SO as to KEEP the connection ALIVE. This article will examine how to configure the four socket options, SO_KEEPALIVE, TCP_KEEPIDLE, TCP_KEEPINTVL and TCP_KEEPCNT with setsockopt() to send heartbeats; and discuss the practice of keep-alive heartbeats in general. Experiment setting:OS: Unbutu 16.04gcc: 5.4.0 To keep the connection aliveOne cause of silent connection drop is NAT entry timeout. A NAT entry consisting of the 4-tuple (source address, source port, destination address and destination port) is recorded by a network router internally for address translation. Due to limited memory available to the hardware, the router has to remove the entry belonging to an inactive session after a timeout. As a result, the connection is effectively closed even though neither ends have explicitly issued a FIN nor RST. Reconnecting is expensive. An end user has to wait for at least 3xRTT spent by handshakes; and additional logic is required to smoothly restore the UX with the previously interrupted state after the user is back on-line. In order to avoid the unnecessary handshakes and the RTTs imposed, HTTP adopts KEEP-ALIVE so that the short-lived HTTP sessions can reuse the same established, persistent TCP connection, which is another story. Next, I will use two programs to illustrate how it works exactly. We look at the code of a server first, For simplicity, I do not apply IO multiplexing so the server can accept connect from 1 client one time. the code of client, After setting the socket options mentioned before, the client initiates the TCP handshakes by connect(), and yield the CPU by sleep(). If you are not familiar with network programming (socket), please read this first. Next, let’s see the network interaction in action. sudo tcpdump -i wlp3s0 dst net 192.168.1.71 or src net 192.168.1.71 and not dst port 22 and not src port 22 // ========================&gt; start handshakes12:21:42.437163 IP 192.168.1.66.43066 &gt; 192.168.1.71.6666: Flags [S], seq 3002564942, win 29200, options [mss 1460,sackOK,TS val 7961984 ecr 0,nop,wscale 7], length 012:21:42.439960 IP 192.168.1.71.6666 &gt; 192.168.1.66.43066: Flags [S.], seq 3450454053, ack 3002564943, win 28960, options [mss 1460,sackOK,TS val 2221927 ecr 7961984,nop,wscale 7], length 012:21:42.440088 IP 192.168.1.66.43066 &gt; 192.168.1.71.6666: Flags [.], ack 1, win 229, options [nop,nop,TS val 7961985 ecr 2221927], length 0// ========================&gt; end handshakes12:21:52.452057 IP 192.168.1.66.43066 &gt; 192.168.1.71.6666: Flags [.], ack 1, win 229, options [nop,nop,TS val 7964488 ecr 2221927], length 012:21:52.454443 IP 192.168.1.71.6666 &gt; 192.168.1.66.43066: Flags [.], ack 1, win 227, options [nop,nop,TS val 2224431 ecr 7961985], length 012:22:02.468056 IP 192.168.1.66.43066 &gt; 192.168.1.71.6666: Flags [.], ack 1, win 229, options [nop,nop,TS val 7966992 ecr 2224431], length 012:22:02.470458 IP 192.168.1.71.6666 &gt; 192.168.1.66.43066: Flags [.], ack 1, win 227, options [nop,nop,TS val 2226935 ecr 7961985], length 012:22:12.484119 IP 192.168.1.66.43066 &gt; 192.168.1.71.6666: Flags [.], ack 1, win 229, options [nop,nop,TS val 7969496 ecr 2226935], length 012:22:12.489786 IP 192.168.1.71.6666 &gt; 192.168.1.66.43066: Flags [.], ack 1, win 227, options [nop,nop,TS val 2229440 ecr 7961985], length 0 Here I removed the irrelevant output of ARPs. If you are not familiar with tcpdump, please read this first. With the feet gotten wet, now it’s a good time to explain the heartbeat mechanism, 1) SO_KEEPALIVE enables (or disables) heartbeat;int flags =1;if (setsockopt(sfd, SOL_SOCKET, SO_KEEPALIVE, (void *)&amp;flags, sizeof(flags))) &#123; perror(\"ERROR: setsocketopt(), SO_KEEPALIVE\"); exit(0); &#125;; and 2) the side with heartbeat enabled (in this example, client) sends empty packets (&#x1f441; length 0); and3) after received the packets, the other side (server) reply with ACK (&#x1f441; Flags [.]); and4) TCP_KEEPIDLE defines the heartbeat frequency (&#x1f441; timestamps).flags = 10;if (setsockopt(sfd, SOL_TCP, TCP_KEEPIDLE, (void *)&amp;flags, sizeof(flags))) &#123; perror(\"ERROR: setsocketopt(), SO_KEEPIDLE\"); exit(0); &#125;; Note that throughout the process, the read() is blocked in the server side, which means the heartbeat packets are transparent to the recipient (server). To detect an absent peerBesides NAT entry expiration, a connection can be dropped silently in one way or another (e.g., a loosen cable). It is crucial for a server application to identify such exception in time, so it can release the associated resources, invoke clean-up routines and/or notify other peer clients. This is why sending heartbeats from server-side makes more sense. Since our feet is already wet.5) TCP_KEEPINTVL defines the heartbeat frequency when there is no answer from the other side; and6) TCP_KEEPCNT dictates how many unanswered heartbeat will indicate a dropped connection; Next we modify the server and client code to test this feature in server, we added all the mentioned socket options, and client is reduced to and the tcpdump output (that is executed on server machine, because we are going to unplug the connection from client) // ========================&gt; handshakes are omitted here20:04:12.535386 IP 192.168.1.66.49232 &gt; 192.168.1.71.6666: Flags [.], ack 1, win 229, options [nop,nop,TS val 12312604 ecr 9154395], length 020:04:22.538591 IP 192.168.1.71.6666 &gt; 192.168.1.66.49232: Flags [.], ack 1, win 227, options [nop,nop,TS val 9161936 ecr 12312604], length 020:04:22.570817 IP 192.168.1.66.49232 &gt; 192.168.1.71.6666: Flags [.], ack 1, win 229, options [nop,nop,TS val 12315113 ecr 9154395], length 0// ========================&gt; we unplug the network connection here20:04:32.586590 IP 192.168.1.71.6666 &gt; 192.168.1.66.49232: Flags [.], ack 1, win 227, options [nop,nop,TS val 9164448 ecr 12315113], length 020:04:37.594590 IP 192.168.1.71.6666 &gt; 192.168.1.66.49232: Flags [.], ack 1, win 227, options [nop,nop,TS val 9165700 ecr 12315113], length 020:04:42.602590 IP 192.168.1.71.6666 &gt; 192.168.1.66.49232: Flags [.], ack 1, win 227, options [nop,nop,TS val 9166952 ecr 12315113], length 020:04:47.610591 IP 192.168.1.71.6666 &gt; 192.168.1.66.49232: Flags [.], ack 1, win 227, options [nop,nop,TS val 9168204 ecr 12315113], length 020:04:52.618596 IP 192.168.1.71.6666 &gt; 192.168.1.66.49232: Flags [.], ack 1, win 227, options [nop,nop,TS val 9169456 ecr 12315113], length 0 Because we set 5 as the threshold number of unacknowledged packets, and each is 5 seconds apart, (&#x1f441; timestamps) flags = 5;if (setsockopt(sfd, SOL_TCP, TCP_KEEPCNT, (void *)&amp;flags, sizeof(flags))) &#123; perror(\"ERROR: setsocketopt(), SO_KEEPCNT\"); exit(0); &#125;;flags = 5;if (setsockopt(sfd, SOL_TCP, TCP_KEEPINTVL, (void *)&amp;flags, sizeof(flags))) &#123; perror(\"ERROR: setsocketopt(), SO_KEEPINTVL\"); exit(0); &#125;; after 5 heartbeats that are unanswered from the client, the n = read(rfd, buffer, BUF_SIZE); is unblocked with an n indicating a closed connection. So that the process of breaking a connection, unlike heartbeat itself, notifies the monitor (server in this case) which in turn can trigger the mentioned actions to finalize a broken connection. ConsiderationWhen heartbeat should not be usedIn mobile network, periodic data transfer will unnecessarily keep the radio active. When this happens in background, the application drains the battery fast and surprises users. So I would go for the extra miles preparing to reconnecting in such case. When heartbeat could not be usedFor a back-end with heavy traffic, the packets generated by business logic alone can be indicators of connectivity. In such case, I would make the server drop a connection after a client has not been sending packets for a long period of time. Alternatively, if I need to further reduce false-positive, I could activate the heartbeat mechanism (through setsockopt()) only for a prolonged silence of a client. It is worth noting that when modifying socket option midway, setsockopt() should work on the file descriptor returned by the accept(), i.e., rdf which represents a established connection. (and other settings will be “inherited” from sdf) System wide settingSome of the discussed socket options can also be set using procfs and sysctl. TCP_KEEPIDLE -&gt; /net/ipv4/tcp_keepalive_timeTCP_KEEPCNT -&gt; /net/ipv4/tcp_keepalive_probesTCP_KEEPINTVL -&gt; /net/ipv4/tcp_keepalive_intvl Referenceshttps://hpbn.co/https://www.tldp.org/HOWTO/html_single/TCP-Keepalive-HOWTO/https://gist.github.com/physacco/5792698https://notes.shichao.io/unp/ch7/#so_keepalive-socket-option","categories":[{"name":"Network Essentials","slug":"Network-Essentials","permalink":"https://holmeshe.me/categories/Network-Essentials/"}],"tags":[{"name":"network","slug":"network","permalink":"https://holmeshe.me/tags/network/"},{"name":"socket","slug":"socket","permalink":"https://holmeshe.me/tags/socket/"},{"name":"setsockopt","slug":"setsockopt","permalink":"https://holmeshe.me/tags/setsockopt/"},{"name":"SOL_SOCKET","slug":"SOL-SOCKET","permalink":"https://holmeshe.me/tags/SOL-SOCKET/"},{"name":"SO_KEEPALIVE","slug":"SO-KEEPALIVE","permalink":"https://holmeshe.me/tags/SO-KEEPALIVE/"},{"name":"heartbeat","slug":"heartbeat","permalink":"https://holmeshe.me/tags/heartbeat/"}]},{"title":"Smart Pointer, shared_ptr, Automatic pointer, and unique_ptr","slug":"cpp-pointers","date":"2017-10-05T11:00:00.000Z","updated":"2020-12-22T10:48:38.109Z","comments":true,"path":"cpp-pointers/","link":"","permalink":"https://holmeshe.me/cpp-pointers/","excerpt":"Objects life-cycle is crucial. A mistake in determining an object’s lifecycle can lead to resource (e.g., memory, fd) leaks as the resource owned cannot be properly released and recycled for future use. When the leak accumulates to a certain level, it crashes the whole system. Objects life-cycle is also complicated since the ownership of one object might be relinquished by, transferred to, or shared with different entities which include but are not limited to variables, function arguments, modules, data structures, containers, and threads. Again, the resource has to be released and recycled by one of the owners at some undetermined point. There is no de-facto standard to determine objects life-cycle. Utilities like GC (garbage collection) that is used in Java, ARC used in Objective-C and all those pointers (ptrs) in C++, all have their pros and cons. However, this article is not about pros and cons but is focused on C++ resource management helper classes, Smart Pointer, shared_ptr, auto_ptr and unique_ptr.","text":"Objects life-cycle is crucial. A mistake in determining an object’s lifecycle can lead to resource (e.g., memory, fd) leaks as the resource owned cannot be properly released and recycled for future use. When the leak accumulates to a certain level, it crashes the whole system. Objects life-cycle is also complicated since the ownership of one object might be relinquished by, transferred to, or shared with different entities which include but are not limited to variables, function arguments, modules, data structures, containers, and threads. Again, the resource has to be released and recycled by one of the owners at some undetermined point. There is no de-facto standard to determine objects life-cycle. Utilities like GC (garbage collection) that is used in Java, ARC used in Objective-C and all those pointers (ptrs) in C++, all have their pros and cons. However, this article is not about pros and cons but is focused on C++ resource management helper classes, Smart Pointer, shared_ptr, auto_ptr and unique_ptr. Smart pointerA smart pointer is a wrapper class of a normal pointer. Smart point defines life-cycle with a reference count that reflects how many time the smart pointer object is referenced.Next, I will show a simple implementation of a smart pointer. The code is for demonstration purposes only, thus, there is no sanity check, no exception handling and no thread-safety guarantee. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778#include &lt;stdio.h&gt;template &lt; typename T &gt; class SmartPointer &#123;private: T* _pRes; int* _refCount;void _release() &#123; if(--(*_refCount) == 0) &#123; printf(\"---Valar Morghulis:%d\\n\",*_refCount); delete _pRes; delete _refCount; &#125; else &#123; printf(\"---not today:%d\\n\",*_refCount); &#125; &#125;public: SmartPointer() : _pRes(NULL), _refCount(NULL) &#123; _refCount = new int(0); printf(\"SP default cons:%d\\n\",*_refCount); &#125;SmartPointer(T* pRes) : _pRes(pRes), _refCount(NULL) &#123; _refCount = new int(1); printf(\"SP cons:%d\\n\",*_refCount); &#125;SmartPointer(const SmartPointer&lt;T&gt;&amp; sp) : _pRes(sp._pRes), _refCount(sp._refCount) &#123; (*_refCount)++; printf(\"SP copy cons:%d\\n\",*_refCount); &#125;SmartPointer&lt;T&gt;&amp; operator = (const SmartPointer&lt;T&gt;&amp; sp) &#123; this-&gt;_release(); // release the last resource it points to _pRes = sp._pRes; _refCount = sp._refCount; (*_refCount)++; printf(\"SP assign:%d\\n\",*_refCount); return *this; &#125;~SmartPointer() &#123; this-&gt;_release(); &#125;// to mimic a real pointer T&amp; operator* () &#123; return *_pRes; &#125;// to mimic a real pointer T* operator-&gt; () &#123; return _pRes; &#125;&#125;;class AClass &#123;public: AClass() &#123; printf(\"aclass cons\\n\"); &#125; ~AClass() &#123; printf(\"aclass des\\n\"); &#125;&#125;;void l2(SmartPointer&lt;AClass&gt;&amp; p) &#123; SmartPointer&lt;AClass&gt; use3 = p; // &gt;&gt; SP copy cons:3&#125; // &gt;&gt; ---not today:2void l1(SmartPointer&lt;AClass&gt;&amp; p) &#123; SmartPointer&lt;AClass&gt; use2 = p; // &gt;&gt; SP copy cons:2 l2(p);&#125; // &gt;&gt; ---not today:1int main() &#123; AClass *res = new AClass(); // &gt;&gt; aclass cons SmartPointer&lt;AClass&gt; aSmartP(res); // &gt;&gt; SP cons:1 l1(aSmartP);&#125; // &gt;&gt; ---Valar Morghulis:0 // &gt;&gt; aclass des Result: 12345678aclass consSP cons:1SP copy cons:2SP copy cons:3---not today:2---not today:1---Valar Morghulis:0aclass des To briefly explain the code above: SmartPointer‘s life-cycle is no more than that of an ordinary class. Thus, logic flow going out of a (function) scope destructs it; SmartPointer has two properties, _pRes and _refCount, both are allocated from heap. Thus, logic flow going out of a (function) scope DOES NOT destruct them; each time a SmartPointer is constructed with a valid _pRes (of type T), the _refCount plus 1; each time a SmartPointer is destructed, in our case, by a logic flow going out of a scope, the _refCount minus 1; however, the destruction of SmartPointer does not necessarily lead to a destruction of _pRes: a) when _refCount is still larger than 0, SmartPointer simply reduce the _refCount and print b) only when _refCount is set to 0 by the minus, SmartPointer destructs the resource referred by _pRes and and print So smart pointers work as handles that are used by different parts of a program to keep track and to control the resource instance. When all handles are destroyed, the resource is considered “not used”, and is deleted as well. In the end of this article, I will show some real handles that embody smart pointer in real world. The sample showcases the usage of smart pointer in program that is linear, which is rarely the case in real scenario. Rather, as mentioned before, the resource (i.e., the instance of AClass) can be shared, by multiple data structure and variables in parallel. shared_ptr (C++11)shared_ptr is the std’s implementation of smart pointer that is more robust than the demo code listed above. And it does not generate dodgy log. Automatic pointerAn automatic pointer, though looks similar to smart pointer, is totally different. It is a convenient helper class that destructs the resource whenever the logic flow going out of the scope, just in case a programmer forgets. To some extent, it makes a pointer (that refers to a memory chunk dynamically allocated in runtime) works similar to a stack variable (statically allocated in compiling time). Example, AutoPointer v1.0: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include &lt;stdio.h&gt;template &lt; typename T &gt; class AutoPointer &#123;private: T* _pRes; public: AutoPointer() : _pRes(NULL) &#123;&#125; AutoPointer(T* pRes) : _pRes(pRes) &#123;&#125; AutoPointer(const AutoPointer&lt;T&gt;&amp; ap) : _pRes(ap._pRes) &#123;&#125; AutoPointer&lt;T&gt;&amp; operator = (const AutoPointer&lt;T&gt;&amp; ap) &#123; delete _pRes; _pRes = ap._pRes; return *this; &#125; ~AutoPointer() &#123; delete _pRes; &#125; // to mimic a real pointer T&amp; operator* () &#123; return *_pRes; &#125; // to mimic a real pointer T* operator-&gt; () &#123; return _pRes; &#125;&#125;; class AClass &#123;public: AClass() &#123; printf(\"cons\\n\"); &#125; ~AClass() &#123; printf(\"des\\n\"); &#125; int i;&#125;; void l1(AutoPointer&lt;AClass&gt;&amp; p) &#123; AutoPointer&lt;AClass&gt; use2 = p;&#125;//the resource has already been deallocated here int main() &#123; AClass *res = new AClass(); res-&gt;i = 5; AutoPointer&lt;AClass&gt; use1(res); l1(use1);&#125;// abort, repeat deallocating pointer Result: 123456consdesdesautop(1148,0x7fff74eff000) malloc: *** error for object 0x7f9940c03240: pointer being freed was not allocated*** set a breakpoint in malloc_error_break to debug[1] 1148 abort ./a.out As given by the code snippet above, automatic pointer works internally like a simplified smart pointer that deallocates the resource regardless of the reference count (in fact, there is no reference count at all). The coredump shows a major drawback of the automatic pointer: the ownership can not be transferred (to l1() ). As a result, even though the resource has been deallocate in l1(), main()still consider itself as the owner of automatic pointer and deallocates the pointer one time more. How about implementing the copy constructor as well as the assignment operator so the ownership can be properly transferred? Example, AutoPointer v2.0: 12345678910111213...... AutoPointer(AutoPointer&lt;T&gt;&amp; ap) : _pRes(ap._pRes) &#123; ap._pRes = NULL; &#125; AutoPointer&lt;T&gt;&amp; operator = (AutoPointer&lt;T&gt;&amp; ap) &#123; delete _pRes; _pRes = ap._pRes; ap._pRes = NULL; return *this; &#125;...... Result: 12consdes All seems good. Yet it is another example of “fixing one bug leads to another”. The new problem is that the two semantics, ownership-transferring and copy, are coupled. So it is not compatible to some of the library functions such as std::sort that takes one extra copy (as pivot in quick sort) as it destroys the previous one that is still in use. The detailed explanation of the problem can be found here, and thanks patatahooligan for pointing out the mistake in the original implementation. std::auto_ptr is the std implementation of the automatic pointer. As discussed above, it is either not very interesting or problematic, so it is now deprecated. And we should use std::unique_ptr instead. std::unique_ptr (C++11)std::unique_ptr is the std’s replacement of std::auto_ptr in C++11. With the newly added rvalue and move semantics, the ownership of a unique_ptr can be safely transferred to another entity. Moreover, the copy semantic is disabled for unique_ptrs to avoid ambiguity we saw in AutoPointer v2.0. Like automatic pointer, the last owner of the pointer is responsible for deallocation. 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;memory&gt;#include &lt;cstdio&gt;#include &lt;fstream&gt;#include &lt;cassert&gt; class AClass &#123;public: AClass() &#123;printf(\"cons\\n\");&#125; ~AClass() &#123;printf(\"des\\n\");&#125; int i;&#125;; std::vector&lt; std::unique_ptr&lt;AClass&gt; &gt; v; void l1() &#123; std::unique_ptr&lt;AClass&gt; p1(new AClass()); // &gt;&gt; cons p1-&gt;i = 1; v.push_back(std::move(p1)); std::unique_ptr&lt;AClass&gt; p2(new AClass()); // &gt;&gt; cons p2-&gt;i = 2; v.push_back(std::move(p2));&#125; // p1 and p2 are not destructed here int main() &#123; l1(); for(auto&amp; p: v) printf(\"%d\\n\", p-&gt;i);&#125; // &gt;&gt; des // &gt;&gt; des Result: 123456conscons12desdes As shown in the code snippet above, the unique pointer is preserved across different owners. When the ownership has been moved to vector v, l1() does not deallocates the resource anymore. This gains unique pointer a much wider usage. N.b., I would rather believe unique pointer is the major reason of the introduction of the new move semantic. Because compared to the improvement gained here, the optimization enabled by move and rvalue is less significant. Take home“I can understand the stuffs, but I’m not sure if I still remember them exactly next morning.” Sure. I will find some real world counterparts to enhance your memory. 1) a std::shared_ptr is like a handle of a video game console. The console (resource) is “shared” by multiple players with handles, and the game should continue even if there is only one player left. Thus, “Game over” only when all players stop playing. 2) a std::unique_ptr is like a portable game console. One player at a time, and one should “move” it to let another to play. “Game over” when the LAST player stops playing. 3) a std::auto_ptr is a as it can not be easily moved.","categories":[{"name":"Rvisit C++","slug":"Rvisit-C","permalink":"https://holmeshe.me/categories/Rvisit-C/"}],"tags":[{"name":"Smart Pointer","slug":"Smart-Pointer","permalink":"https://holmeshe.me/tags/Smart-Pointer/"},{"name":"shared ptr","slug":"shared-ptr","permalink":"https://holmeshe.me/tags/shared-ptr/"},{"name":"automatic ptr","slug":"automatic-ptr","permalink":"https://holmeshe.me/tags/automatic-ptr/"},{"name":"unique ptr","slug":"unique-ptr","permalink":"https://holmeshe.me/tags/unique-ptr/"}]},{"title":"C++ rvalue, && and Move","slug":"cpp-rvalue-and-move","date":"2017-09-19T12:00:00.000Z","updated":"2020-12-22T10:48:38.109Z","comments":true,"path":"cpp-rvalue-and-move/","link":"","permalink":"https://holmeshe.me/cpp-rvalue-and-move/","excerpt":"C++ is hard, the newer versions become even harder. This article will deal with some of the hard parts in C++, rvalue, rvalue reference (&amp;&amp;) and move semantics. And I am going to reverse engineer (not a metaphor) these complex and correlated topics, so you can understand them completely in one shot.","text":"C++ is hard, the newer versions become even harder. This article will deal with some of the hard parts in C++, rvalue, rvalue reference (&amp;&amp;) and move semantics. And I am going to reverse engineer (not a metaphor) these complex and correlated topics, so you can understand them completely in one shot. Firstly, let’s examine What is a rvalue?A rvalue is one that should be on the right side of an equals sign. Example: 12345int var; // too much JavaScript recently:)var = 8; // OK! lvalue (yes, there is a lvalue) on the left8 = var; // ERROR! rvalue on the left(var + 1) = 8; // ERROR! rvalue on the left Simple enough. Then let’s look at some more subtle rvalues, ones that are returned by functions: 12345678910111213141516#include &lt;string&gt;#include &lt;stdio.h&gt;int g_var = 8;int&amp; returnALvalue() &#123; return g_var; //here we return a lvalue&#125;int returnARvalue() &#123; return g_var; //here we return a rvalue&#125;int main() &#123; printf(\"%d\", returnALvalue()++); // g_var += 1; printf(\"%d\", returnARvalue());&#125; Result: 1289 It is worth noting that the way of returning a l-value (in the example) is considered a bad practice. So do not do that in real world programming. Beyond theoretical levelWhether a variable is a rvalue can make differences in real programming even before &amp;&amp; is invented. For example, this line 1const int&amp; var = 8; can be compiled fine while this: 1int&amp; var = 8; // use a lvalue reference for a rvalue generates following error: 12rvalue.cc:24:6: error: non-const lvalue reference to type &apos;int&apos; cannot bind to a temporary of type &apos;int&apos; The error message means that the compiler enforces a const reference for rvalue. A more interesting example: 123456789101112131415161718#include &lt;stdio.h&gt;#include &lt;string&gt;void print(const std::string&amp; name) &#123; printf(\"rvalue detected:%s\\n\", name.c_str());&#125;void print(std::string&amp; name) &#123; printf(\"lvalue detected:%s\\n\", name.c_str());&#125;int main() &#123; std::string name = \"lvalue\"; std::string rvalu = \"rvalu\"; print(name); //compiler can detect the right function for lvalue print(rvalu + \"e\"); // likewise for rvalue&#125; Result: 12lvalue detected:lvaluervalue detected:rvalue The difference is actually significant enough and compiler can determine overloaded functions. So rvalue is constant value?Not exactly. And this where &amp;&amp; (rvalue reference)comes in. Example: 123456789101112131415161718192021222324#include &lt;stdio.h&gt;#include &lt;string&gt;void print(const std::string&amp; name) &#123; printf(“const value detected:%s\\n”, name.c_str());&#125;void print(std::string&amp; name) &#123; printf(“lvalue detected%s\\n”, name.c_str());&#125;void print(std::string&amp;&amp; name) &#123; printf(“rvalue detected:%s\\n”, name.c_str());&#125;int main() &#123; std::string name = “lvalue”; const std::string cname = “cvalue”; std::string rvalu = \"rvalu\"; print(name); print(cname); print(rvalu + \"e\");&#125; result: 123lvalue detected:lvalueconst value detected:cvaluervalue detected:rvalue If the functions are overloaded for rvalue, a rvalue variable choose the more specified version over the version takes a const reference parameter that is compatible for both. Thus, &amp;&amp; can further diversify rvalue from const value. In bellow I summarize the compatibility of overloaded function versions to different types in default setting. You can verify the result by selectively commenting out lines in the example above. It sounds cool to further differentiate rvalue and constant value as they are not exactly the same indeed. But what is the practical value? What problem does &amp;&amp; solve exactly?The problem is the unnecessary deep copy when the argument is a rvalue. To be more specific. &amp;&amp; notation is provided to specify a rvalue, which can be used to avoid the deep copy when the rvalue, 1) is passed as an argument of either a constructor or an assignment operator, and 2) the class of which contains a pointer (or pointers) referring to dynamically allocated resource (memory). It can be more specific with examples: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include &lt;stdio.h&gt;#include &lt;string&gt;#include &lt;algorithm&gt;using namespace std;class ResourceOwner &#123;public: ResourceOwner(const char res[]) &#123; theResource = new string(res); &#125; ResourceOwner(const ResourceOwner&amp; other) &#123; printf(\"copy %s\\n\", other.theResource-&gt;c_str()); theResource = new string(other.theResource-&gt;c_str()); &#125; ResourceOwner&amp; operator=(const ResourceOwner&amp; other) &#123; ResourceOwner tmp(other); swap(theResource, tmp.theResource); printf(\"assign %s\\n\", other.theResource-&gt;c_str()); &#125; ~ResourceOwner() &#123; if (theResource) &#123; printf(\"destructor %s\\n\", theResource-&gt;c_str()); delete theResource; &#125; &#125;private: string* theResource;&#125;;void testCopy() &#123; // case 1 printf(\"=====start testCopy()=====\\n\"); ResourceOwner res1(\"res1\"); ResourceOwner res2 = res1; //copy res1 printf(\"=====destructors for stack vars, ignore=====\\n\");&#125;void testAssign() &#123; // case 2 printf(\"=====start testAssign()=====\\n\"); ResourceOwner res1(\"res1\"); ResourceOwner res2(\"res2\"); res2 = res1; //copy res1, assign res1, destrctor res2 printf(\"=====destructors for stack vars, ignore=====\\n\");&#125;void testRValue() &#123; // case 3 printf(\"=====start testRValue()=====\\n\"); ResourceOwner res2(\"res2\"); res2 = ResourceOwner(\"res1\"); //copy res1, assign res1, destructor res2, destructor res1 printf(\"=====destructors for stack vars, ignore=====\\n\");&#125;int main() &#123; testCopy(); testAssign(); testRValue();&#125; result: 12345678910111213141516171819=====start testCopy()=====copy res1=====destructors for stack vars, ignore=====destructor res1destructor res1=====start testAssign()=====copy res1assign res1destructor res2=====destructors for stack vars, ignore=====destructor res1destructor res1=====start testRValue()=====copy res1assign res1destructor res2destructor res1=====destructors for stack vars, ignore=====destructor res1 The result are all good for the first two test cases, i.e., testCopy() and testAssign(), in which resource in res1 is copied for the res2. It is reasonable to copy the resource because they are two entities both need their unshared resource (a string). However, in the third case, the (deep) copying of the resource in res1 is superfluous because the anonymous rvalue (returned by ResourceOwner(“res1”)) will be destructed right after the assignment thus it does not need the resource anymore: 1res2 = ResourceOwner(&quot;res1&quot;); // Please note that the destructor res1 is called right after this line before the point where stack variables are destructed. I think it is a good chance to repeat the problem statement: &amp;&amp; notation is provided to specify a rvalue, which can be used to avoid the deep copy when the rvalue, 1) is passed as an argument of either a constructor or an assignment operator, and 2) the class of which contains a pointer (or pointers) referring to dynamically allocated resource (memory). If copying of a resource that is about to disappear is not optimal, what is the right operation then? The answer is MoveThe idea is pretty straightforward, if the argument is a rvalue, we do not need to copy. Rather, we can simply “move” the resource (that is the memory the rvalue points to). Now let’s overload the assignment operator using the new technique: 1234ResourceOwner&amp; operator=(ResourceOwner&amp;&amp; other) &#123; theResource = other.theResource; other.theResource = NULL;&#125; This new assignment operator is called a move assignment operator. And a move constructor can be programmed in a similar way. A good way of understanding this is: when you sell your old property and move to a new house, you do not have to toss all the furniture as we did in case 3 right? Rather, you can simply move the furniture to the new home. All good. What is std::move?Besides the move assignment operator and move constructor discussed above, there is one last missing piece in this puzzle, std::move. Again, we look at the problem first: when 1) we know a variable is in fact a rvalue, while 2) the compiler does not. The right version of the overloaded functions can not be called. A common case is when we add another layer of resource owner, ResourceHolder and the relation of the three entities is given as bellow: 12345holder | |-----&gt;owner | |-----&gt;resource (N.b., in the following example, I complete the implementation of ResourceOwner’s move constructor as well) Example:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include &lt;string&gt;#include &lt;algorithm&gt;using namespace std;class ResourceOwner &#123;public: ResourceOwner(const char res[]) &#123; theResource = new string(res); &#125; ResourceOwner(const ResourceOwner&amp; other) &#123; printf(“copy %s\\n”, other.theResource-&gt;c_str()); theResource = new string(other.theResource-&gt;c_str()); &#125;++ResourceOwner(ResourceOwner&amp;&amp; other) &#123;++ printf(“move cons %s\\n”, other.theResource-&gt;c_str());++ theResource = other.theResource;++ other.theResource = NULL;++&#125; ResourceOwner&amp; operator=(const ResourceOwner&amp; other) &#123; ResourceOwner tmp(other); swap(theResource, tmp.theResource); printf(“assign %s\\n”, other.theResource-&gt;c_str()); &#125;++ResourceOwner&amp; operator=(ResourceOwner&amp;&amp; other) &#123;++ printf(“move assign %s\\n”, other.theResource-&gt;c_str());++ theResource = other.theResource;++ other.theResource = NULL;++&#125; ~ResourceOwner() &#123; if (theResource) &#123; printf(“destructor %s\\n”, theResource-&gt;c_str()); delete theResource; &#125; &#125;private: string* theResource;&#125;;class ResourceHolder &#123;……ResourceHolder&amp; operator=(ResourceHolder&amp;&amp; other) &#123; printf(“move assign %s\\n”, other.theResource-&gt;c_str()); resOwner = other.resOwner;&#125;……private: ResourceOwner resOwner;&#125; In ResourceHolder’s move assignment operator, we want to call ResourceOwner’s move assignment operator since “a no-pointer member of a rvalue should be a rvalue too”. However, when we simply code resOwner = other.resOwner, what gets invoked is actually the ResourceOwner’s normal assignment operator that, again, incurs the extra copy. It’s a good chance to repeat the problem statement again: when 1) we know a variable is in fact a rvalue, while 2) the compiler does not. The right version of the overloaded functions can not be called. As a solution we use to std::move to cast the variable to rvalue, so the right version of ResourceOwner’s assignment operator can be called. 1234ResourceHolder&amp; operator=(ResourceHolder&amp;&amp; other) &#123; printf(“move assign %s\\n”, other.theResource-&gt;c_str()); resOwner = std::move(other.resOwner);&#125; What is std::move exactly?We know that type cast is not simply a compiler placebo telling a compiler that “I know what I am doing”. It effectively generate instructions of mov a value to bigger or smaller registers (e.g.,%eax-&gt;%cl) to conduct the “cast”. So what std::move does exactly behind scene. I do not know myself when I am writing this paragraph, so let’s find out together. First we modify the main a bit (I tried to make the style consistent) Example: 123456int main() &#123; ResourceOwner res(“res1”); asm(“nop”); // remeber me ResourceOwner &amp;&amp; rvalue = std::move(res); asm(“nop”); // remeber me&#125; Compile it, and dissemble the obj using 12clang++ -g -c -std=c++11 -stdlib=libc++ -Weverything move.ccgobjdump -d -D move.o Result: 12345678910111213141516171819200000000000000000 &lt;_main&gt;: 0: 55 push %rbp 1: 48 89 e5 mov %rsp,%rbp 4: 48 83 ec 20 sub $0x20,%rsp 8: 48 8d 7d f0 lea -0x10(%rbp),%rdi c: 48 8d 35 41 03 00 00 lea 0x341(%rip),%rsi # 354 &lt;GCC_except_table5+0x18&gt; 13: e8 00 00 00 00 callq 18 &lt;_main+0x18&gt; 18: 90 nop // remember me 19: 48 8d 75 f0 lea -0x10(%rbp),%rsi 1d: 48 89 75 f8 mov %rsi,-0x8(%rbp) 21: 48 8b 75 f8 mov -0x8(%rbp),%rsi 25: 48 89 75 e8 mov %rsi,-0x18(%rbp) 29: 90 nop // remember me 2a: 48 8d 7d f0 lea -0x10(%rbp),%rdi 2e: e8 00 00 00 00 callq 33 &lt;_main+0x33&gt; 33: 31 c0 xor %eax,%eax 35: 48 83 c4 20 add $0x20,%rsp 39: 5d pop %rbp 3a: c3 retq 3b: 0f 1f 44 00 00 nopl 0x0(%rax,%rax,1) between the two nop, we can notice some dummy instructions generated for the move (if looking closely, you can know that they do basically nothing) However, if we turn on O (-O1) for the compiler, all the instructions will be gone. 12clang++ -g -c -O1 -std=c++11 -stdlib=libc++ -Weverything move.ccgobjdump -d -D move.o Moreover, if changing the critical line to: 1ResourceOwner &amp; rvalue = res; The assembly generated is identical. That means the move semantics is pure syntax candy and a machine does not care at all. To conclude, The MACHINE thinks it irrelevant, we don’t.-Harold Finch","categories":[{"name":"Rvisit C++","slug":"Rvisit-C","permalink":"https://holmeshe.me/categories/Rvisit-C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://holmeshe.me/tags/C/"},{"name":"rvalue","slug":"rvalue","permalink":"https://holmeshe.me/tags/rvalue/"},{"name":"move","slug":"move","permalink":"https://holmeshe.me/tags/move/"}]},{"title":"JavaScript Tutorial for Programmers - Async","slug":"javascript-tutorial-for-experienced-programmer-async","date":"2017-09-11T10:00:00.000Z","updated":"2020-12-22T10:48:38.109Z","comments":true,"path":"javascript-tutorial-for-experienced-programmer-async/","link":"","permalink":"https://holmeshe.me/javascript-tutorial-for-experienced-programmer-async/","excerpt":"I still remember the days of debugging CORS problem when I put together some projects using JavaScript (&amp; ajax), “a very particular programming language” in my first impression. Recently I got a great opportunity. The new role uses JS, the browser-side script that is now winning in all sides, as the major language. So I tooke it as a good chance to learn JS more systematically, and this series will be part of the outcome of my study. As the name implies, I will not cover primary level such as “if, else” (condition), “for” (or any kinds of loops), or basic OOP concepts. Instead, I will focus only on differences so you can learn this versatile language like reviewing a pull request, and use it the next day in your next awesome project.","text":"I still remember the days of debugging CORS problem when I put together some projects using JavaScript (&amp; ajax), “a very particular programming language” in my first impression. Recently I got a great opportunity. The new role uses JS, the browser-side script that is now winning in all sides, as the major language. So I tooke it as a good chance to learn JS more systematically, and this series will be part of the outcome of my study. As the name implies, I will not cover primary level such as “if, else” (condition), “for” (or any kinds of loops), or basic OOP concepts. Instead, I will focus only on differences so you can learn this versatile language like reviewing a pull request, and use it the next day in your next awesome project. Nothing new under the sunBasically, asynchronization has two layers of meaning 1) unblocking of slow operations; and 2) triggering events non-linearly. In OS terms, the event is also called an interruption that can represent a coming network packet, a clock tick, or simply a mouse click. Technically, the event interrupts the current process, puts the next CPU instruction on hold, and calls a predefined code block (a.k.a., an event handler), “asynchronously”. The concept is essentially the same in application level. The problem of blocking operationsIn a narrow sense, asynchronization solves a fundamental difficulty in application development: blocking operation (mostly I/O). Why blocking is difficult? Well, no matter what kinds of App (with UI) you are working on (an embedded system, an mobile App, a game, or a web page), there is a underlying “loop” that refreshes the screen in a very high frequency. If the “loop” is blocked by a slow operation, say, a network interaction, your UI will be frozen, and the users might just let the App go. In particular, JavaScript runs as part of the “loop”, so we need to wield this black magic wisely. Before we start experimenting on JS, let’s do some preparations. Firstly, we download Moesif Origin &amp; CORS Changer because we are going to make (a lot of) cross-origin HTTP requests. (as briefly mentioned in my first post) Secondly, we use python (Flask) to simulate a slow API which sleeps ten seconds for each request to impose noticeable latency: 123456789101112from flask import Flaskimport timeapp = Flask(__name__)@app.route(\"/lazysvr\")def recv(): time.sleep(10) return \"ok\"if __name__ == \"__main__\": app.run(host='***.***.***.***', threaded=True) Now we toggle the CORS plug-in to “on” (otherwise the network request will fail instantly) and run the example: 12345678910111213&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt; &lt;button type=\"button\"&gt;Click Me!&lt;/button&gt; &lt;script&gt; var xmlHttp = new XMLHttpRequest(); xmlHttp.open( \"GET\", \"http://***.***.***.***:5000/lazysvr\", false ); // false for synchronous request xmlHttp.send( null ); // the thread is suspended here alert(xmlHttp.responseText); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; By debugging the code, we can observe that that after the network request, the code is suspended at the following line: 1xmlHttp.send( null ); // it is the aforementioned blocking operation for &gt;10 seconds and the button is not clickable at all before it displays the result: 1ok Moreover, the runtime (I’m using Chrome) complaints: 1[Deprecation] Synchronous XMLHttpRequest on the main thread is deprecated because of its detrimental effects to the end user’s experience. For more help, check https://xhr.spec.whatwg.org/. which can be an official statement of the problem. Asynchronization in actionBroadly speaking, asychronization can be 1) (slow) operations that are performed from another thread; or 2) events that are triggered from external; or the composite of both. I am introducing three examples to demonstrate asychronization in code: The first one, a packet arrivalThe code used by this example also can solve the problem discussed in the previous section: 123456789101112131415161718192021&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt; &lt;button type=\"button\"&gt;Click Me!&lt;/button&gt; &lt;script&gt; var xmlHttp = new XMLHttpRequest();-- xmlHttp.open( \"GET\", \"http://192.241.212.230:5000/lazysvr\", false );++ xmlHttp.open( \"GET\", \"http://192.241.212.230:5000/lazysvr\", true ); // 1) change the param to \"true\" for asynchronous request++ xmlHttp.onreadystatechange = function() &#123; // 2) add the callback++ if(xmlHttp.readyState == 4 &amp;&amp; xmlHttp.status == 200) &#123;++ alert(xmlHttp.responseText);++ &#125; &#125; xmlHttp.send(); -- alert(xmlHttp.responseText); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; In this example, we 1) change the second parameter to “true” so as to offload the slow network interaction to another thread, and 2) register a callback as an event handler for the response packet. The callback will be effectively triggered from the other thread when the network interaction completes. This time, the button can respond to a user’s click throughout the process and 1ok is displayed after the send as expected. The second, a clock tick1234setTimeout(callback, 3000);function callback() &#123; alert('event triggered');&#125; N.b., 1, JavaScript does not allow synchronous sleep() from beginning.N.b., 2, unlike OS kernel, the clock tick here will never trigger a process (thread) scheduling. As mentioned before, all the JavaScript code is running in one thread. And the third, a mouse click123456789101112&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt; &lt;button type=\"button\" onclick=\"callback()\"&gt;Click Me!&lt;/button&gt; &lt;script&gt; function callback() &#123; alert('event triggered'); &#125; &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; In all the three examples, we register handlers (a.k.a., callbacks) for certain events occurred outside of the main thread. In the first example, we also offload a slow operation to an external thread to fight the blocking problem. As mentioned, all operations can be abstracted in one word, asynchronization! The new fetch() APIIn the first example, packet arrival, I use a callback to make the operation more obvious as asynchronized. A better practice of sending network request is to use the newer API — fetch(). The function returns a Promise that can call then() in turn, so that the asynchronized operation can be coded in a synchronized manner (thus less obvious), and the so dubbed “callback hell” can be effectively avoided, and the best part all the potential exceptions involved in multiple asynchronized calls can be handled in one place: 1234567891011121314151617&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt; &lt;button type=\"button\" onclick=\"callback()\"&gt;Click Me!&lt;/button&gt; &lt;script&gt; fetch(\"http://192.241.212.230:5000/lazysvr\") .then((response) =&gt; &#123; return response.text(); &#125;).then((text) =&gt; &#123; alert(text); &#125;).catch(function(error) &#123; console.log('error: ' + error.message); &#125;); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; The result is the same as the example one, and I leave the button there for you to click. Under the hood, multi-threaded + event loopIsn’t JavaScript single threadedThe answer is yes and no. I’ll explain: 123456789101112 var i; for (i = 0; i &lt; 1000; i++) &#123; var xmlHttp = new XMLHttpRequest(); xmlHttp.open( \"GET\", \"http://192.241.212.230:5000/lazysvr\", true ); xmlHttp.onreadystatechange = function() &#123; if (xmlHttp.readyState == 4 &amp;&amp; xmlHttp.status == 200) &#123; alert(xmlHttp.responseText); &#125; &#125; // end of the callback xmlHttp.send( null );&#125; Assuming the browser’s pid is 666, we can use a simple script (I’m using Mac) to monitor the status of threads belonging to the browser : 12#!/bin/bashwhile true; do ps -M 666; sleep 1; done initial values (I beautified the output a bit by removing the irrelevant columns and rows): 1234USER PID ... STIME UTIMEholmes 666 ... 0:00.42 0:01.47 ...... 666 0:00.20 0:00.64 values when I stop: 1234USER PID ... STIME UTIMEholmes 666 ... 0:00.50 0:01.88 ...... 666 0:00.37 0:01.28 Besides the main thread, there is another thread that is pretty active during the process, which indicate that one more thread is involved, most likely, by sending the network request and listening to the multiplex socket. So JavaScript runs in a single thread indeed. But you take the perspective from the application, it is multi-threaded. Feel free to conduct the same experiment on other JavaScript platforms like Node.js. Event loop, the coarse-grained asynchronizationI hope you still remember that asynchronized exception is triggered in a granularity of CPU instruction in OS level as I mentioned it in the beginning of this article. What about that in JavaScript? We look at a frequent example first: 123456789var i;for (i = 0; i &lt; 3; i++) &#123; alert(i);&#125;setTimeout(callback, 0);function callback() &#123; alert('event triggered');&#125; We know that the result is: 1234123event triggered To recap, though we register a time event and indicate the callback should be invoked immediately, the runtime still waits for the current “loop” iteration to finish before it executes the callback from an “event queue”. ConclusionIn this series, I have covered the diversified “equals to” operation and “null” value; as well as the simplified string, array, object and dictionary, in JavaScript. And I further discussed the object from a low level point of view, prototype in this and this post. And I highlighted “this” pitfall throughout the series in three different posts, 1st time 2nd time 3rd time which signals its importance. Then we come to this one that demystifies the asynchronization operation.","categories":[{"name":"JavaScript Tutorial for Programmers","slug":"JavaScript-Tutorial-for-Programmers","permalink":"https://holmeshe.me/categories/JavaScript-Tutorial-for-Programmers/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://holmeshe.me/tags/JavaScript/"},{"name":"async","slug":"async","permalink":"https://holmeshe.me/tags/async/"},{"name":"callback","slug":"callback","permalink":"https://holmeshe.me/tags/callback/"},{"name":"promise","slug":"promise","permalink":"https://holmeshe.me/tags/promise/"}]},{"title":"JavaScript Tutorial for Programmers - Prototype(2)","slug":"javascript-tutorial-for-experienced-programmer-prototype-2","date":"2017-08-25T10:00:00.000Z","updated":"2020-12-22T10:48:38.109Z","comments":true,"path":"javascript-tutorial-for-experienced-programmer-prototype-2/","link":"","permalink":"https://holmeshe.me/javascript-tutorial-for-experienced-programmer-prototype-2/","excerpt":"I still remember the days of debugging CORS problem when I put together some projects using JavaScript (&amp; ajax), “a very particular programming language” in my first impression. Recently I got a great opportunity. The new role uses JS, the browser-side script that is now winning in all sides, as the major language. So I tooke it as a good chance to learn JS more systematically, and this series will be part of the outcome of my study. As the name implies, I will not cover primary level such as “if, else” (condition), “for” (or any kinds of loops), or basic OOP concepts. Instead, I will focus only on differences so you can learn this versatile language like reviewing a pull request, and use it the next day in your next awesome project.","text":"I still remember the days of debugging CORS problem when I put together some projects using JavaScript (&amp; ajax), “a very particular programming language” in my first impression. Recently I got a great opportunity. The new role uses JS, the browser-side script that is now winning in all sides, as the major language. So I tooke it as a good chance to learn JS more systematically, and this series will be part of the outcome of my study. As the name implies, I will not cover primary level such as “if, else” (condition), “for” (or any kinds of loops), or basic OOP concepts. Instead, I will focus only on differences so you can learn this versatile language like reviewing a pull request, and use it the next day in your next awesome project. Prototype chain based inheritanceClass inheritance can be implemented using a technique called prototype chain. So an object can be traced back to its ancestor(s) using a prototype instance that is embedded (i.e., __proto__) in it, like a gene. For the end purpose of inheritance, i.e., code reusing, prototype chain also enables the accessing of members that only exist in ancestors, hence whenever such a member (method or property) is accessed, the runtime will check upwards the prototype chain that contains members information of the whole inheritance (sub-)tree. Simple, but not easyPhew, it’s hard to explain coding concept without code. So next I am going to implement this prototype chain step by step. Moreover, I make it a trial and error process in hope that the read can simulate a real developing experience, and which I hope can be remembered (by you) easier. Example: 123456789101112131415var ASuperClass = function() &#123; this.aproperty = 'a'; this.amethod = function () &#123;return 'b';&#125;;&#125;;var ASubClass = function() &#123;&#125;;ASubClass.prototype = ASuperClass.prototype; //let's make the chainvar subobj = new ASubClass();alert(subobj instanceof ASuperClass);alert(subobj.aproperty);alert(subobj.amethod()); Result: 12trueundefined Uncaught TypeError: subobj.amethod is not a function Though the type can be recognized by the runtime, the properties can not be accessed as expected. So I think we need to call the super class constructor to initialize the missing properties. Let’s give it a go. Example:12345678910111213141516 var ASuperClass = function() &#123; this.aproperty = 'a'; this.amethod = function () &#123;return 'b';&#125;; &#125;; var ASubClass = function() &#123;++ ASuperClass(); &#125;; ASubClass.prototype = ASuperClass.prototype;++SubClass.prototype.constructor = ASubClass; var subobj = new ASubClass(); alert(subobj instanceof ASuperClass); alert(subobj.aproperty); alert(subobj.amethod()); Unfortunately, the result is the same. Hmmm, this time is the corrupted this in the superclass constructor. If this is not your first reaction, please ⬅ to my previous post that is dedicated to this topic. In order to fix this, we can either add: 12var tmpf = ASuperClass.bind(this);tmpf(); or: 1ASuperClass.apply(this); , as per discussed in the post. I will use the second method because, well, it’s one line less: 12345678910111213141516 var ASuperClass = function() &#123; this.aproperty = 'a'; this.amethod = function () &#123;return 'b';&#125;; &#125;; var ASubClass = function() &#123;++ ASuperClass.apply(this); &#125;; ASubClass.prototype = ASuperClass.prototype; ASubClass.prototype.constructor = ASubClass; var subobj = new ASubClass(); alert(subobj instanceof ASuperClass); alert(subobj.aproperty); alert(subobj.amethod()); Result: 123trueab Finally we nailed it! Are we? No: 123456789101112131415161718 var ASuperClass = function() &#123; this.aproperty = 'a'; this.amethod = function () &#123;return 'b';&#125;; &#125;; var ASubClass = function() &#123; ASuperClass.apply(this); &#125;; ASubClass.prototype = ASuperClass.prototype; ASubClass.prototype.constructor = ASubClass; ++ASubClass.prototype.another_property = 'c'; var subobj = new ASubClass();++var superobj = new ASuperClass();++alert(subobj.another_property);++alert(superobj.another_property); Result: 12cc In fact, we just made something else but inheritance, which does not exist in real world. I tentatively call it coupling. Though we made a wrong decision from the very first step, still the other effort we made can be largely reused. I’ve heard versions of stories about one guy, who failed a kickstart project, one day realized the pain eventually paid off in some other way. This is just like that. So let’s keep up. Prototype DEcouplingIn common (and correct) practice, prototype chain is established using two entities, an intermediate class (which is also an object, first class) and an intermediate object (,normal) in order to decouple the “thing” we made just now, and the designated object relation is given below: 1234Subclass.prototype |---intermediate object |---.__proto__ |---IntermediateClass.prototype === Superclass.prototype in which, the intermediate object is an instance of the intermediate class. Let’s get back to the snippet I showed in the beginning of my last post: 1234567function inherits(ChildClass, ParentClass) &#123; function IntermediateClass() &#123;&#125; IntermediateClass.prototype = ParentClass.prototype; ChildClass.prototype = new IntermediateClass; ChildClass.prototype.constructor = ChildClass; return ChildClass;&#125;; I hope the above text has made the above function clear now. Then let’s verify the function in code: 12345678910111213141516171819202122232425262728 var ASuperClass = function() &#123; this.aproperty = 'a'; this.amethod = function () &#123;return 'b';&#125;; &#125;; var ASubClass = function() &#123; ASuperClass.apply(this); &#125;; --ASubClass.prototype = ASuperClass.prototype;++inherits(ASubClass, ASuperClass); ASubClass.prototype.another_property = 'c'; var subobj = new ASubClass(); var superobj = new ASuperClass(); alert(subobj instanceof ASuperClass); alert(subobj instanceof ASubClass); alert(subobj.aproperty); alert(subobj.amethod()); alert(subobj.another_property); alert(superobj instanceof ASuperClass); alert(superobj instanceof ASubClass); alert(superobj.aproperty); alert(superobj.amethod()); alert(superobj.another_property); A better version of the inherits() can be implemented using Object.create() that can simplify the creation of the intermediate entities. 12345678 function inherits(ChildClass, ParentClass) &#123;-- function IntermediateClass() &#123;&#125;-- IntermediateClass.prototype = ParentClass.prototype;-- ChildClass.prototype = new IntermediateClass;++ ChildClass.prototype = Object.create(ParentClass.prototype); ChildClass.prototype.constructor = ChildClass; return ChildClass; &#125;; To be honest, I am feeling a bit exciting when checking the result:) 1234567891011truetrueabc// this line is artificialtruefalseabundefined Allllll good! Wait, there is one last piece, what is a prototype.constructorYou may have noticed, I stealthily added the constructor related code from here and omit the explanation. I did it on purpose in order to make the logic flow smooth. Now let’s scrutinize it: 123class &lt;----------------| |------.prototype.constructor |------......(I have drawn this) prototype.constructor is a special method referring to the class itself. Normally the link is made by the runtime, Example: 123456var ASuperClass = function() &#123; this.aproperty = 'a'; this.amethod = function () &#123;return 'b';&#125;;&#125;;alert(ASuperClass.prototype.constructor === ASuperClass); Result:1true However, the original ChildClass.prototype.constructor is broken when we chained up the prototypes in 1function inherits(ChildClass, ParentClass) So we need to assign it back to ChildClass: 1ChildClass.prototype.constructor = ChildClass;","categories":[{"name":"JavaScript Tutorial for Programmers","slug":"JavaScript-Tutorial-for-Programmers","permalink":"https://holmeshe.me/categories/JavaScript-Tutorial-for-Programmers/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://holmeshe.me/tags/JavaScript/"},{"name":"prototype","slug":"prototype","permalink":"https://holmeshe.me/tags/prototype/"},{"name":"inheritance","slug":"inheritance","permalink":"https://holmeshe.me/tags/inheritance/"}]},{"title":"JavaScript Tutorial for Programmers - Prototype(1)","slug":"javascript-tutorial-for-experienced-programmer-prototype-1","date":"2017-08-21T09:30:00.000Z","updated":"2020-12-22T10:48:38.109Z","comments":true,"path":"javascript-tutorial-for-experienced-programmer-prototype-1/","link":"","permalink":"https://holmeshe.me/javascript-tutorial-for-experienced-programmer-prototype-1/","excerpt":"I still remember the days of debugging CORS problem when I put together some projects using JavaScript (&amp; ajax), “a very particular programming language” in my first impression. Recently I got a great opportunity. The new role uses JS, the browser-side script that is now winning in all sides, as the major language. So I tooke it as a good chance to learn JS more systematically, and this series will be part of the outcome of my study. As the name implies, I will not cover primary level such as “if, else” (condition), “for” (or any kinds of loops), or basic OOP concepts. Instead, I will focus only on differences so you can learn this versatile language like reviewing a pull request, and use it the next day in your next awesome project.","text":"I still remember the days of debugging CORS problem when I put together some projects using JavaScript (&amp; ajax), “a very particular programming language” in my first impression. Recently I got a great opportunity. The new role uses JS, the browser-side script that is now winning in all sides, as the major language. So I tooke it as a good chance to learn JS more systematically, and this series will be part of the outcome of my study. As the name implies, I will not cover primary level such as “if, else” (condition), “for” (or any kinds of loops), or basic OOP concepts. Instead, I will focus only on differences so you can learn this versatile language like reviewing a pull request, and use it the next day in your next awesome project. prototype, a semi-OOP semanticA prototype is a singleton that is created for each class (by JS runtime), so that instances of the class can be constructed based on the in memory template. I call prototype semi-OOP because it is not normal. For instance, this is how a prototype based inheritance is programmed: 12345678function inherits(ChildClass, ParentClass) &#123; function IntermediateClass() &#123;&#125; IntermediateClass.prototype = ParentClass.prototype; ChildClass.prototype = new IntermediateClass; ChildClass.prototype.constructor = ChildClass; return ChildClass;&#125;; I will cover prototype based inheritance in the next post. For now, we just grasp the idea of how things are different here. Practical implicationAs per discussed in the last post, ES6 has already defined a more standard OOP paradigm in JavaScript. And ES6 code can be “transpiled” (compile into another language or syntax) into ES5 so the two standards are equally compatible with various browsers in practice. Does that mean prototype is not useful anymore? My answer is No, because it still largely exists in a large amount of existing code. For concrete numbers, if grep -R prototype *|wc -l, at the point of time when I am writing, in Angular, the output shows there are 286 lines; in React, there are 405; and Vue, 756). As for me, I need to know prototype to understand these 1447 lines of backbones. Last but not least, the frameworks are ordered alphabetically 😅. Playaround with prototypeIn the following example, we first define a constructor (If you do not know what is a constructor in JS context, please ⬅ to my last post). Then we use it to create an object and examine the prototype(s). Example: 123456789101112131415var MyClass = function ()&#123;&#125;;var obj = new MyClass;alert(obj.__proto__ == MyClass.prototype); //=truealert(obj.prototype == MyClass.prototype); //=&gt;false// if the object's prototype does not equal to the class' prototype, what is it thenalert(obj.prototype); //=&gt;undefined// you can change the definition of a class on the runMyClass.prototype.a_member = \"abc\";// and the object's structure is changed as wellalert(obj.a_member); //=&gt;abcalert(obj.__proto__.a_member); //=&gt;abc Result: 12345truefalseundefinedabcabc An initial observation: prototype only exists in a class (a.k.a., a constructor). If you need to access prototype from a class instance (a.k.a., an object), __proto__ is the way to go. prototype and __proto__ are just two sides of one coin. In technical words, they refer to the same instance, i.e., the singleton mentioned in the beginning of this text. the prototype of a class can be changed at runtime, and all the instances of this class are affected accordingly. Moreover, a modification of __proto__ of an instance can change the prototype (definition) of a class, and all the sibling instances are affected in a cascade manner. Example: 1234567891011var MyClass = function ()&#123;&#125;;var obj1 = new MyClass;var obj2 = new MyClass;obj1.__proto__.a_member = 'abc';// an instance can change the definition of the classalert(MyClass.prototype.a_member); //Ooooops...// so all other instances are affectedalert(obj2.a_member); Result: 12abcabc This operation is dangerous. It will cause side-effect and confusion in a project with reasonable complexity as the definition of a class can not be easily traceable. Hence I highly recommend considering __proto__ final (in Java) or constant (in C++) and should be never touched. If you really want to modify the behavior of an object, say, add a property, at least modify the object itself rather than __proto__. 1234567891011var MyClass = function ()&#123;&#125;;var obj1 = new MyClass;var obj2 = new MyClass;obj1.a_member = 'abc'; // change the object behavior at run time// no effectalert(MyClass.prototype.a_member);// no effectalert(obj2.a_member); Result:12abcabc On the other hand, adding a property to a class effectively creates a static member (in other language term). 123456789101112131415var MyClass = function ()&#123;&#125;;var obj1 = new MyClass;var obj2 = new MyClass;MyClass.a_member = 'abc'; // change the class behavior// no effectalert(MyClass.prototype.a_member);// no effectalert(obj1.a_member);alert(obj2.a_member);// the member has been effectively addedalert(MyClass.a_member); Result: 1234undefinedundefinedundefinedabc I would recommend this usage. So the bright side of this feature is flexibility, empowered by that Everything is objectEverything in JavaScript is object. That includes, functions, constructors (class)es, instances, prototypes and __proto__s, etc. Let’s prove it in code. Example:1234567891011121314151617181920212223var MyClass = function ()&#123; this.a = \"a\"; this.b = \"b\";&#125;;var obj = new MyClass;var arry = [];function f1() &#123; alert(\"something\");&#125;;alert(MyClass instanceof Object);alert(MyClass.prototype instanceof Object);alert(MyClass.__proto__ instanceof Object);alert(obj instanceof Object);alert(obj.__proto__ instanceof Object);alert(obj instanceof Object);alert(arry instanceof Object);alert(f1 instanceof Object);alert(f1.prototype instanceof Object);alert(f1.__proto__ instanceof Object); Result:1true(chorus) Furthermore, Object is subdivided into two categories, first class objects and all other normal objects. Normal objects are just variables, which can be created, modified, assigned to other variables, and passed as arguments (or return value), all in runtime. Whilst first class object is the “platinum version” of object that unlocks extra privileges. Besides the operations listed above, first class object can be invoked (as a normal function); and it can be invoked to create normal objects (as a constructor). I think those extra privileges can explain from another perspective why first class object is attached one extra member property prototype alongside __proto__. More examples:12345678910111213141516171819202122var MyClass = function ()&#123; this.a = \"a\"; this.b = \"b\";&#125;;var obj = new MyClass;function f1() &#123; alert(\"something\");&#125;;// MyClass is a first class object so...alert(MyClass.prototype); // it has prototype andalert(MyClass.__proto__); // __proto__// obj is an object so...alert(obj.prototype); // it does not have prototype butalert(obj.__proto__); // it has __proto__ // f1 is a first class object so...alert(f1.prototype); // it has prototype andalert(f1.__proto__); // __proto__ Result: 123456[object Object]function () &#123;&#125;undefined[object Object][object Object]function () &#123;&#125; Except for…Exception 1, primitive typesDid I just said everything is object? 123var str = \"whatever\";alert(str instanceof Object);alert(str.__proto__ instanceof Object); Result: 12falsetrue Even though a string literal is not an object, it contains __proto__ that is an object?! So either string literal is an object, which is obviously not, or what I said in the last section is just bullshit. It is not. What happens here is that a mechanism called automatic wrapping kicks in, which wraps the no-object (primitive) variables to its object counterpart , in this case, a String(), as required. Automatic wrapping can enable calling methods directly from primitives as well: 12var str = \"whatever\";alert(str.indexOf(\"tever\")); Result: 13 Same phenomenon happens: the runtime wrap the literal to String() automatically and calls the indexOf() method. And the rule applies to other primitive types (int, float) as well. Exception 2, ???All sounds perfect until I tried this Example: 12345678var obj = &#123; a:\"abc\", b:\"bcd\"&#125;;alert(obj instanceof Object);alert(obj.__proto__);alert(obj.__proto__ instanceof Object); Result: 123true[object Object]false Apparently var obj and obj.__proto__ are objects according to the first two outputs. However, when I want to confirm if obj.__proto__ is really an object, runtime gives me a false. I hope I am not the first bloger who writes about things he is not sure himself, but if you have an answer, please let me know in the comments. It does not matter if you don’t, as __proto__ is no-standard usage anyway. In the next post, I will discuss one of the hardest parts of JavaScript, inheritance based on prototype chain.","categories":[{"name":"JavaScript Tutorial for Programmers","slug":"JavaScript-Tutorial-for-Programmers","permalink":"https://holmeshe.me/categories/JavaScript-Tutorial-for-Programmers/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://holmeshe.me/tags/JavaScript/"},{"name":"js","slug":"js","permalink":"https://holmeshe.me/tags/js/"},{"name":"prototype","slug":"prototype","permalink":"https://holmeshe.me/tags/prototype/"}]},{"title":"Don't Worry if You Overslept this class - Byte Order","slug":"network-essentials-byte-order","date":"2017-08-18T18:00:00.000Z","updated":"2020-12-22T10:48:38.109Z","comments":true,"path":"network-essentials-byte-order/","link":"","permalink":"https://holmeshe.me/network-essentials-byte-order/","excerpt":"When I was reading the source code of a network system, htons() and ntohs() are the very first two functions that confused me. To understand them, I decided to refresh the fading knowledge learned from college in order to squeeze out the last bit of conceptual doubt in byte order.","text":"When I was reading the source code of a network system, htons() and ntohs() are the very first two functions that confused me. To understand them, I decided to refresh the fading knowledge learned from college in order to squeeze out the last bit of conceptual doubt in byte order. We are lucky as computer science practitioners since we already understand the smallest unit in cyber space, that is, bit. Thus, we only need to advance towards one direction, up. A bit is simply a 0 or an 1. Physically it is sometimes represented as a tiny magnetic region on a metal plate. Moving up, we have byte that is 8 bits. We generally use byte to measure the sizes of data, like we use meter to measure sizes of objects. So instead of saying “something is 32 bits”, we normally say “it is 4 bytes”. Moving upward, we have word that is 4 bytes (or 32 bits)in 32 bit architecture. Note that I use 32 bit architecture throughout this text to keep it concise, and the concept can be easily ported to 64 bit architectures. Network and host byte orderByte order (also known as, endianness) controls how a word is stored in memory, and how it is transmitted over the network. In big-endian, the most significant byte is set at the lowest address; whilst in little India, some dishes are really hot…no…in little-endian, the most significant byte is set at the highest address. Laying bytes on physical media (that is, memory &amp; network) is like laying floors, in both cases it is OK to lay a wooden tile, or a 4-bytes word in both directions. However, a system designer has to make a decision so as to keep the style consistent. Defined by RFC 1700, network protocols designers chose big-endian. However, some designers of host systems disagree. In X86, it is little-endian; In ARM, it could be either. That means the actual data of the same value varies in different physical media. For instance, the value A1B2C3D4 (2712847316 in decimal) can have two forms, as given below: In the above figure, each box — for example, the box containing A1 — represents a byte. Please note that the “endianness shuffling” is based on the unit of byte, not bit. Machines can process the two formats almost equally well, but humans complaint that the numbers in little-endian are reversed. Why? Isn’t it more logical to set the less significant byte to lower address, and more to higher? The reason is: when we write digital on papers (that is, well, another physical media), we use big endian unconsciously. Taking the above number (A1B2C3D4) as the example, our sub-conscious draws the low and high addresses from left to right even though they are not there: But if we mandate our subconscious to draw them from right to left, maybe we can reconcile the conflicts between the rationale and intuition. In my opinion, it is perfectly acceptable because we use all kinds of coordinate systems when locating UI components on a screen (e.g., app, game development), for instance: What do you think? Next we look at how the theories are used, and why they matter, in practice. htons() ntohs()These two functions fill the format gap between the network and hosts. Technically, when hosts communicate over network, they are used to convert the packet’s byte order. If a host and network byte order are the same (that means they both uses big-endian), the two functions simply do nothing. When a host byte order is different from network (that means the host uses little-endian), htons() converts the data from little-endian to big-endian, and ntohs() converts from big-endian back to little-endian. There are another pair of functions of this kind, htonl() and ntohl() that operate on larger numbers than htons() and ntohs(). They are based on the same principle so I will not further discuss them. Fact-checkI always need concrete code to be sure. 123456789101112131415161718192021222324252627#include &lt;stdio.h&gt;#define BITS_PER_BYTE 8int main() &#123; unsigned int anint = 0xa1b2c3d4; unsigned char *truth = &amp;anint; printf(\"value in decimal: %u\\n\", anint); printf(\"0x\"); for (int i = 0; i &lt; sizeof(anint); i++) &#123; printf(\"%2X\", truth[i]); &#125; printf(\"\\n\"); unsigned int anint_net = htons(anint); truth = &amp;anint_net; printf(\"value in decimal after hton: %u\\n\", anint_net); printf(\"0x\"); for (int i = 0; i &lt; sizeof(anint_net); i++) &#123; printf(\"%02X\", truth[i]); &#125; printf(\"\\n\");&#125; The result on my machine:1234value in decimal: 27128473160xD4C3B2A1value in decimal after hton: 544670xC3D40000 As given in the example, htons() changes the original value of anint using big-endian to prepare for network transmission and the original value will be recovered in the receiving host by ntohs(). Though the real network operations are not demonstrated in the example, I think you can get the idea. How to determineWe can reuse some code from the above example to impelment a function that can show a machine’s byte order: 12345int isLittle() &#123; unsigned int anint = 0xa1b2c3d4; unsigned char *truth = &amp;anint; return truth[0] == 0xd4;&#125; In fact, there is a much simpler way, reading directly the CPU information. If asking Ubuntu, you’ll know the code free methothe code-free method:1cpu | grep &quot;Byte Order&quot;","categories":[{"name":"Network Essentials","slug":"Network-Essentials","permalink":"https://holmeshe.me/categories/Network-Essentials/"}],"tags":[{"name":"network","slug":"network","permalink":"https://holmeshe.me/tags/network/"},{"name":"byte order","slug":"byte-order","permalink":"https://holmeshe.me/tags/byte-order/"},{"name":"host byte order","slug":"host-byte-order","permalink":"https://holmeshe.me/tags/host-byte-order/"},{"name":"network byte order","slug":"network-byte-order","permalink":"https://holmeshe.me/tags/network-byte-order/"},{"name":"big endian","slug":"big-endian","permalink":"https://holmeshe.me/tags/big-endian/"},{"name":"little endian","slug":"little-endian","permalink":"https://holmeshe.me/tags/little-endian/"},{"name":"endianness","slug":"endianness","permalink":"https://holmeshe.me/tags/endianness/"}]},{"title":"JavaScript Tutorial for Programmers - Object (Dictionary)","slug":"javascript-tutorial-for-experienced-programmer-object-dictionary","date":"2017-08-18T09:09:00.000Z","updated":"2020-12-22T10:48:38.109Z","comments":true,"path":"javascript-tutorial-for-experienced-programmer-object-dictionary/","link":"","permalink":"https://holmeshe.me/javascript-tutorial-for-experienced-programmer-object-dictionary/","excerpt":"I still remember the days of debugging CORS problem when I put together some projects using JavaScript (&amp; ajax), “a very particular programming language” in my first impression. Recently I got a great opportunity. The new role uses JS, the browser-side script that is now winning in all sides, as the major language. So I tooke it as a good chance to learn JS more systematically, and this series will be part of the outcome of my study. As the name implies, I will not cover primary level such as “if, else” (condition), “for” (or any kinds of loops), or basic OOP concepts. Instead, I will focus only on differences so you can learn this versatile language like reviewing a pull request, and use it the next day in your next awesome project. In JavaScript, it is hard to differentiate an object and a dictionary (a.k.a., map in some other languages):","text":"I still remember the days of debugging CORS problem when I put together some projects using JavaScript (&amp; ajax), “a very particular programming language” in my first impression. Recently I got a great opportunity. The new role uses JS, the browser-side script that is now winning in all sides, as the major language. So I tooke it as a good chance to learn JS more systematically, and this series will be part of the outcome of my study. As the name implies, I will not cover primary level such as “if, else” (condition), “for” (or any kinds of loops), or basic OOP concepts. Instead, I will focus only on differences so you can learn this versatile language like reviewing a pull request, and use it the next day in your next awesome project. In JavaScript, it is hard to differentiate an object and a dictionary (a.k.a., map in some other languages): 1234567891011121314var hero = &#123; name: 'iron man', birth: 1971,&#125;;alert(hero[\"name\"]);// use map key to fetch propertyalert(hero.name); var herodict = &#123; \"name\": 'iron man', \"birth\": 1971,&#125;; alert(herodict[\"name\"]);alert(herodict.name);// use property name to fetch a value Result: 1234iron maniron maniron maniron man According to the test result, they are the same, and it is officially called an object. Hense in the rest of text, I will use . when referring to properties to make it clear. In the above example, the class of an object is defined in a one-off manner. To declare types that can be reusable: we use constructor and class (es6). ConstructorA constructor is a special function used for creating an object: Example: 123456789function StarkIndustries(name, birth) &#123; this.name = name; this.birth = birth;&#125;var iron1 = new StarkIndustries('iron01', 2017);var iron2 = new StarkIndustries('iron02', 2017);alert(iron1.name);alert(iron2.name); Result: 12iron01iron02 By adding new before a function, we turn the function into a constructor that constructs an object. Revisit this issueMissing new falls into the this pitfall: 1234567891011var name = \"stark\";var birth = 1971;function StarkIndustries(name, birth) &#123; this.name = name; this.birth = birth;&#125; var iron1 = StarkIndustries('iron01', 2017);alert(iron1);alert(name);alert(birth); Result:123undefinediron012017 From the experiment above, we can observe that without new: this inside StarkIndustries() points to window, thus, the global variables(name ,birth) have been modified within the misused constructor; the misused StarkIndustries() returns a undefine. So it is dangerous to call a function designed as a constructor without new. And a common practice is doing sanity check inside the constructor: 123if (!(this instanceof StarkIndustries)) &#123; warn(\"StarkIndustries is a constructor and should be called with `new`\");&#125; Or you can use real constructor inside a real class. ClassExample:1234567891011class Hero &#123; constructor(name, birth) &#123; this.name = name; this.birth = birth; &#125;&#125;var iron1 = new Hero('iron01', 2017);var iron2 = new Hero('iron02', 2017);alert(iron1.name);alert(iron2.name); Result: 12iron01iron02 What if new is missed here: 1234567891011class Hero &#123; constructor(name, birth) &#123; this.name = name;// note, here we can still declare properties within constructor this.birth = birth; &#125;&#125;var iron1 = Hero('iron01', 2017);// error: Uncaught TypeError: Class constructor Hero cannot be invoked without 'new'var iron2 = Hero('iron02', 2017);alert(iron1.name);alert(iron2.name); The code triggers an error when we are about to create iron1. So class is safer than the pure constructor. DictionaryIn the following example we examine more dictionary facets of an object. In particular, we will see how easily properties of an object can be added and removed like key-value entries. 123456789101112131415function StarkIndustries(name, birth) &#123; this.name = name; this.birth = birth;&#125; var iron1 = new StarkIndustries('iron01', 2017);alert(iron1);alert(iron1.name);alert(iron1.birth);iron1.age = 0;alert(iron1.age);delete iron1.age;alert(iron1.age); Result:1234iron0120170undefined Besides the techniques described above, JavaScript provides prototype as a more flexible way to modify class definitions. prototype will be discussed in the next post.","categories":[{"name":"JavaScript Tutorial for Programmers","slug":"JavaScript-Tutorial-for-Programmers","permalink":"https://holmeshe.me/categories/JavaScript-Tutorial-for-Programmers/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://holmeshe.me/tags/JavaScript/"},{"name":"js","slug":"js","permalink":"https://holmeshe.me/tags/js/"},{"name":"object","slug":"object","permalink":"https://holmeshe.me/tags/object/"},{"name":"dictionary","slug":"dictionary","permalink":"https://holmeshe.me/tags/dictionary/"},{"name":"constructor","slug":"constructor","permalink":"https://holmeshe.me/tags/constructor/"},{"name":"this","slug":"this","permalink":"https://holmeshe.me/tags/this/"},{"name":"class","slug":"class","permalink":"https://holmeshe.me/tags/class/"},{"name":"new","slug":"new","permalink":"https://holmeshe.me/tags/new/"}]},{"title":"JavaScript Tutorial for Programmers - This","slug":"javascript-tutorial-for-experienced-programmer-this","date":"2017-08-16T20:30:00.000Z","updated":"2020-12-22T10:48:38.109Z","comments":true,"path":"javascript-tutorial-for-experienced-programmer-this/","link":"","permalink":"https://holmeshe.me/javascript-tutorial-for-experienced-programmer-this/","excerpt":"I still remember the days of debugging CORS problem when I put together some projects using JavaScript (&amp; ajax), “a very particular programming language” in my first impression. Recently I got a great opportunity. The new role uses JS, the browser-side script that is now winning in all sides, as the major language. So I tooke it as a good chance to learn JS more systematically, and this series will be part of the outcome of my study. As the name implies, I will not cover primary level such as “if, else” (condition), “for” (or any kinds of loops), or basic OOP concepts. Instead, I will focus only on differences so you can learn this versatile language like reviewing a pull request, and use it the next day in your next awesome project. In this post, I will discuss this, the legendary JavaScript pitfall countless programmers trod upon.","text":"I still remember the days of debugging CORS problem when I put together some projects using JavaScript (&amp; ajax), “a very particular programming language” in my first impression. Recently I got a great opportunity. The new role uses JS, the browser-side script that is now winning in all sides, as the major language. So I tooke it as a good chance to learn JS more systematically, and this series will be part of the outcome of my study. As the name implies, I will not cover primary level such as “if, else” (condition), “for” (or any kinds of loops), or basic OOP concepts. Instead, I will focus only on differences so you can learn this versatile language like reviewing a pull request, and use it the next day in your next awesome project. In this post, I will discuss this, the legendary JavaScript pitfall countless programmers trod upon. What is thisExample: 12345678910var hero = &#123; name :\"Hulk\", motto :\"Rahhhhh!!\", speak:function () &#123; alert(this.name + \":\" + this.motto); &#125;&#125;hero.speak(); Result: 1Hulk:Rahhhhh!! this‘ runtime valueWhat showed above is a classic OOP example, in which we use this to refer to the object that calls the function. Yet JavaScript is not strictly object-oriented and the behavior of this is different from all the rest. To be more specific, this is assigned with the reference of the execution context by the runtime. What does that mean, we look at another example: 123456789101112131415var name = \"Iron man\", motto = \"Right in the feels!\";function speak() &#123; alert(this.name + \":\" + this.motto);&#125;var hero = &#123; name :\"Hulk\", motto :\"Rahhhhh!!\", speak:speak //here we assign the function to the member method&#125;hero.speak();speak(); // implicitly reads the two variables in the global context(window) window.speak(); // we call speak from windows to verify Results:123Hulk:Rahhhhh!!Iron man:Right in the feels!Iron man:Right in the feels! In the code above, the first speak() is called in the context of hero, and the value of this is set accordingly. Albeit not obvious, we can know that the caller of second speak() is in fact window, by comparing the output of the second function call and that of the third. In both cases, the members of window (global variable name = &quot;Iron man&quot;) can be dereferenced using this.***, inside the implementation of speak(). The confusion shown above can not be counted as a real issue (rather, it is more like a puzzle game) since no one will program in this way. Next, we look at some problems that are relevant in practice. Problem 1 - callbackThe most common way to incur the “context mess up” (I made this one up as I really do not want to use the term “context switch”) is through a callback assigned to a variable, in most cases, a function parameter. For example: 123456789101112131415var name = \"Iron man\", motto = \"Right in the feels!\";function thinkBeforeSpeak(callback) &#123; callback();&#125;var hero = &#123; name :\"Hulk\", motto :\"Rahhhhh!!\", speak :function speak() &#123; alert(this.name + \":\" + this.motto); &#125;&#125;setTimeout(function()&#123; thinkBeforeSpeak(hero.speak); &#125;, 3000); Result: 1Iron man:Right in the feels! In the example, the function in use is assigned to other variables and the context of the function is changed to window …… and again, Iron man feels not well. Solution 1, bind()123456789101112131415var name = \"Iron man\", motto = \"Right in the feels!\";function thinkBeforeSpeak(callback) &#123; callback();&#125;var hero = &#123; name :\"Hulk\", motto :\"Rahhhhh!!\", speak :function speak() &#123; alert(this.name + \":\" + this.motto); &#125;&#125;setTimeout(function()&#123; thinkBeforeSpeak(hero.speak.bind(hero)); &#125;, 3000); Result: 1Hulk:Rahhhhh!! As you may have noticed, the magic is the following line: 1hero.speak.bind(hero); which fixate the this value to the designated object, the Hulk. Solution 2, apply()123456789101112131415var name = \"Iron man\", motto = \"Right in the feels!\";function thinkBeforeSpeak(callback) &#123; callback.apply(hero, []);&#125;var hero = &#123; name :\"Hulk\", motto :\"Rahhhhh!!\", speak :function speak() &#123; alert(this.name + \":\" + this.motto); &#125; &#125;setTimeout(function()&#123; thinkBeforeSpeak(hero.speak); &#125;, 3000); Result: 1Hulk:Rahhhhh!! apply() works similar to bind() while it takes an extra array argument that represents the arguments required for the real, workhorse function. I will not further discuss another similar method, call() that takes variable arguments instead of an array. Problem 2 - inner functionAnother commonly encountered pitfall is using this inside an inner function (function defined within another function, yes, you can do that in JavaScript), in which case this IS NOT the object that the outer method belongs to. Example: 123456789101112131415var name = \"Iron man\", motto = \"Right in the feels!\";var hero = &#123; name :\"Hulk\", motto :\"Rahhhhh!!\", speak :function speak() &#123; function innervoice() &#123; alert(this.name + \":\" + this.motto); &#125; innervoice(); &#125;&#125;hero.speak(); Result: 1Iron man:Right in the feels! So do I. The this becomes window again. Like whenever the JavaScript runtime is not sure, it set this to window. Solution, =&gt;We can use =&gt; (ES6) to define a inner function, in which the this value is fixated automatically. 12345678910111213var name = \"Iron man\", motto = \"Right in the feels!\";var hero = &#123; name :\"Hulk\", motto :\"Rahhhhh!!\", speak :function speak() &#123; var fn = () =&gt; &#123; alert(this.name + \":\" + this.motto); &#125; fn(); &#125;&#125;hero.speak(); Result: 1Hulk:Rahhhhh!! This time, runtime got it right. BTW, =&gt; is called an arrow method and ()=&gt;{...;} means an arrow method with no argument. What? The problem can be solved using that as well?! NaN. Problem 3 - constructorIn practice, (a misused) constructor is the last common place you can encounter the unpredictable this. However, it requires further understanding how JavaScript handles class &amp; object, so I will cover this problem and the solutions in later posts of this series. Today we have learned some techniques to save Iron man. And I hope the next time when you encounter an undefined caused by the messed up this, you can smash the problem like the Hulk.","categories":[{"name":"JavaScript Tutorial for Programmers","slug":"JavaScript-Tutorial-for-Programmers","permalink":"https://holmeshe.me/categories/JavaScript-Tutorial-for-Programmers/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://holmeshe.me/tags/JavaScript/"},{"name":"js","slug":"js","permalink":"https://holmeshe.me/tags/js/"},{"name":"this","slug":"this","permalink":"https://holmeshe.me/tags/this/"},{"name":"arrow method","slug":"arrow-method","permalink":"https://holmeshe.me/tags/arrow-method/"},{"name":"=>","slug":"","permalink":"https://holmeshe.me/tags//"}]},{"title":"JavaScript Tutorial for Programmers - String and Array","slug":"javascript-tutorial-for-experienced-programmer-string-and-array","date":"2017-08-15T20:00:00.000Z","updated":"2020-12-22T10:48:38.109Z","comments":true,"path":"javascript-tutorial-for-experienced-programmer-string-and-array/","link":"","permalink":"https://holmeshe.me/javascript-tutorial-for-experienced-programmer-string-and-array/","excerpt":"I still remember the days of debugging CORS problem when I put together some projects using JavaScript (&amp; ajax), “a very particular programming language” in my first impression. Recently I got a great opportunity. The new role uses JS, the browser-side script that is now winning in all sides, as the major language. So I took it as a good chance to learn JS more systematically, and this series will be part of the outcome of my study. As the name implies, I will not cover primary level such as “if, else” (condition), “for” (or any kinds of loops), or basic OOP concepts. Instead, I will focus only on differences so you can learn this versatile language like reviewing a pull request, and use it the next day in your next awesome project. First thing first, in JavaScript, strings are constant while arrays are mutable.","text":"I still remember the days of debugging CORS problem when I put together some projects using JavaScript (&amp; ajax), “a very particular programming language” in my first impression. Recently I got a great opportunity. The new role uses JS, the browser-side script that is now winning in all sides, as the major language. So I took it as a good chance to learn JS more systematically, and this series will be part of the outcome of my study. As the name implies, I will not cover primary level such as “if, else” (condition), “for” (or any kinds of loops), or basic OOP concepts. Instead, I will focus only on differences so you can learn this versatile language like reviewing a pull request, and use it the next day in your next awesome project. First thing first, in JavaScript, strings are constant while arrays are mutable. Example: 1234567var str = \"123\";str[0] = \"0\";alert(str); var arr = [1, 2, 3];arr[0] = \"0\";alert(arr); Result: 121230,1,3 By the way, strings are marked with &quot;&quot; , &#39;&#39; or (multiline code definition), and arrays are []. More examples: 12345678var str = \"123\";str.length = 10;alert(str); var arr = [1, 2, 3];arr.length = 10;alert(arr);alert(arr[3]); Results:1231231,2,3,,,,,,,undefined Interestingly, if you change the length of an array by force, the JavaScript runtime adds undefined into the array. But the string can not be modified this way because, as mentioned in the beginning, they are immutable. Escape character \\JavaScript too uses \\ to escape special characters (e.g., a &quot; that is inside a string marked with &quot;). Example: 123alert(\"\\\"\");alert(\"a\\nb\");alert(\"a\\tb\"); Result: 1234&quot;aba b so \\&quot; represents a &quot; literal; \\n means a new line and `\\t, a tab. Search for “JavaScript special characters” in Google for the full list. We can directly use ASCII and Unicode in strings with \\ just like we did in C. But I will not elaborate further as I can not see an obvious use case of this feature in a high level language. ConcatenationFor strings, we use +, or string template: 12345var js = \"JavaScript\";var message1 = \"I like \" + js; //using +alert(message1);var message2 = `and $&#123;js&#125;'s particularities`; //using string templatealert(message2); Result: 12I like JavaScriptand JavaScript&apos;s particularities Please note that grave accent (`) should be used for string template, not apostrophe (‘). For array, use concat() 123var arr = ['a', 'b', 'c'];var added = arr.concat(['d', 'e', 'f']);alert(added); Result: 1a,b,c,d,e,f Access elementsUse [] for both: 1234var arr = ['a', 'b', 'c'];var str = \"abc\";alert(arr[0]);alert(str[0]); Result: 12aa SearchUse indexOf() for both. 12345678var arr = ['abc', 'xyz', 123, 789];alert(arr.indexOf(123));alert(arr.indexOf('xyz'));alert(arr.indexOf(789));alert(arr.indexOf('abc')); var str = \"abcxyz\";//a=&gt;0 b=&gt;1 c=&gt;2 x=&gt;3alert(str.indexOf('xyz')); Result: 1234521303 No explanation… Substring and subarrayUse substring() for string and slice() for array. Example: 1234567var str = \"abcxyz\";//a=&gt;0 b=&gt;1 c=&gt;2 x=&gt;3 y=&gt;4 z=&gt;5alert(str.substring(0, 5));// does not include 5(z)alert(str.substring(3));//start from 3(x) to the end var arr = [\"a\", \"b\", \"c\", \"x\", \"y\", \"z\"];alert(arr.slice(0, 5));// does not include 5(z)alert(arr.slice(3));//start from 3(x) to the end Result: 1234abcxyxyza,b,c,x,yx,y,z I hope this post gives you enough confidence to use these two everyday life data structures (types).","categories":[{"name":"JavaScript Tutorial for Programmers","slug":"JavaScript-Tutorial-for-Programmers","permalink":"https://holmeshe.me/categories/JavaScript-Tutorial-for-Programmers/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://holmeshe.me/tags/JavaScript/"},{"name":"js","slug":"js","permalink":"https://holmeshe.me/tags/js/"},{"name":"array","slug":"array","permalink":"https://holmeshe.me/tags/array/"},{"name":"concatenation","slug":"concatenation","permalink":"https://holmeshe.me/tags/concatenation/"},{"name":"escape","slug":"escape","permalink":"https://holmeshe.me/tags/escape/"},{"name":"slice","slug":"slice","permalink":"https://holmeshe.me/tags/slice/"},{"name":"special character","slug":"special-character","permalink":"https://holmeshe.me/tags/special-character/"},{"name":"string","slug":"string","permalink":"https://holmeshe.me/tags/string/"},{"name":"substring","slug":"substring","permalink":"https://holmeshe.me/tags/substring/"}]},{"title":"JavaScript Tutorial for Programmers - Some Basics","slug":"javascript-tutorial-for-experienced-programmer-some-basics","date":"2017-08-09T12:00:00.000Z","updated":"2020-12-22T10:48:38.109Z","comments":true,"path":"javascript-tutorial-for-experienced-programmer-some-basics/","link":"","permalink":"https://holmeshe.me/javascript-tutorial-for-experienced-programmer-some-basics/","excerpt":"I still remember the days of debugging CORS problem when I put together some projects using JavaScript (&amp; ajax), “a very particular programming language” in my first impression. Recently I got a great opportunity. The new role uses JS, the browser-side script that is now winning in all sides, as the major language. So I took it as a good chance to learn JS more systematically, and this series will be part of the outcome of my study. As the name implies, I will not cover primary level such as “if, else” (condition), “for” (or any kinds of loops), or basic OOP concepts. Instead, I will focus only on differences so you can learn this versatile language like reviewing a pull request, and use it the next day in your next awesome project. By the way, CORS was solved using Moesif Origin &amp; CORS Changer.","text":"I still remember the days of debugging CORS problem when I put together some projects using JavaScript (&amp; ajax), “a very particular programming language” in my first impression. Recently I got a great opportunity. The new role uses JS, the browser-side script that is now winning in all sides, as the major language. So I took it as a good chance to learn JS more systematically, and this series will be part of the outcome of my study. As the name implies, I will not cover primary level such as “if, else” (condition), “for” (or any kinds of loops), or basic OOP concepts. Instead, I will focus only on differences so you can learn this versatile language like reviewing a pull request, and use it the next day in your next awesome project. By the way, CORS was solved using Moesif Origin &amp; CORS Changer. Comparison, == V.S. ===== does automatic type conversion and compare the “value”. So a string &quot;1&quot; equals to an integer 1 in ==. === does not do type conversion. So when comparing a string to an integer, === returns false regardless of the value. Example: 123456789101112131415161718192021//comparing string with number var res = \"1\" == 1; alert(res); //=&gt;true res = \"1\" === 1; alert(res); //=&gt;false//comparing two special type with the same value res = null == undefined; alert(res); //=&gt;true res = null === undefined; alert(res); //=&gt;false//comparing number with object var obj = new Number(123); res = 123 == obj alert(res) //=&gt;true res = 123 === obj alert(res) //=&gt;false Result: 123456truefalsetruefalsetruefalse It is worth noting that the result of NaN === NaN is always false. Use isNaN() in such case. The complete behavior of these two “equal-to” operations is given by the equality tables below, use this cheat sheet when something counter-intuitive happens: Source: dorey.github.io NaN, undefined and nullSimply put, undefined is a special init value assigned to a variable (implicitly) by the runtime; null is a value assigned by a programmer to mark the variable as “I am done with the it”; and NaN is set to indicate that something goes wrong (again, by the runtime). Practically, look at the “declaration” portion of your code when encountering undefined, the uninitialized; search for = null and evaluate the occurrences in your project when encountering null, the released; and look inside a function implementation and check if the arguments are passed correctly when encountering NaN, an error. I think subdividing of these exceptional “nil”s can narrow down the code review scope for bug analysis. Do you think so too? Let me know in the comment bellow. Example: 1234567891011var res;alert(res);res = null;alert(res);res = parseFloat(\"geoff\");alert(res);res = 1/0;alert(res); Result: 1234undefinednullNaNinfinity It is worth noting that n/0 does not make a NaN, as some claims. The result should be infinity. The flexibility of JavaScript introduces cases of uninitialized variables (undefined) that are not common in other languages. Following are the examples: 1234567var res = [1,2,3];alert(res[10]);function doSomething(first, second, optional) &#123; alert(optional);&#125;doSomething(1,2); result: 12undefinedundefined As demonstrated by the code, whenever a variable is not explicitly assigned with a value, which could be the case of an out-of-range array accessing or an ignored function argument, the variable is set as undefined by the runtime.","categories":[{"name":"JavaScript Tutorial for Programmers","slug":"JavaScript-Tutorial-for-Programmers","permalink":"https://holmeshe.me/categories/JavaScript-Tutorial-for-Programmers/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://holmeshe.me/tags/JavaScript/"},{"name":"js","slug":"js","permalink":"https://holmeshe.me/tags/js/"},{"name":"undefined","slug":"undefined","permalink":"https://holmeshe.me/tags/undefined/"},{"name":"==","slug":"","permalink":"https://holmeshe.me/tags//"},{"name":"===","slug":"","permalink":"https://holmeshe.me/tags//"},{"name":"comparison","slug":"comparison","permalink":"https://holmeshe.me/tags/comparison/"},{"name":"equal","slug":"equal","permalink":"https://holmeshe.me/tags/equal/"}]}]}